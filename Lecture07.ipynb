{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bash\n",
    "## grep, sed, awk, and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `grep`\n",
    "- `grep` is a standard \\*NIX tool to search files for patterns and print matching lines\n",
    "- There are numerous options for both how matching is done, and what the output of `grep` is\n",
    "- Options that effect matching\n",
    "    - -P: Use a Perl style regular expression\n",
    "    - -e PATTERN: Specify a pattern after each -e (allows multiple patterns)\n",
    "    - -f FILE: Read patterns in from FILE, one per line\n",
    "    - -i: Ignore case\n",
    "    - -v: Invert matches, only print lines that do not match the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/* initial implementaton using a linked list - \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K hashtab */\n"
     ]
    }
   ],
   "source": [
    "grep  \"todo\"  /usr/src/linux-headers-3.19.0-15-generic/include/drm/drmP.h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/irda/irttp.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\tstruct timer_list \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K_timer; \n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/drm/drm_sarea.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    /** \\\u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K Use readers/writer lock for drm_sarea::drawable_lock */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/netdevice.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K *\t@\u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K_list:\t\tDelayed register/unregister\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/netdevice.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\tstruct list_head\t\u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K_list;\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/netdevice.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t       NETREG_UNREGISTERED,\t/* completed unregister \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/netdevice.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kvoid netdev_run_\u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K(void);\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/drm/drmP.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K/* initial implementaton using a linked list - \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K hashtab */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/video/mmp_disp.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K: add query */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/media/saa7146.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\tu32\t\t\t\tint_\u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K;\n"
     ]
    }
   ],
   "source": [
    "grep -R \"todo\"  /usr/src/linux-headers-3.19.0-15-generic/include/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/dst.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K : stats should be SMP safe */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/9p/client.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: This needs lots of explanation.\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/af_vsock.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: stream_bind() */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/irda/irttp.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\tstruct timer_list \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K_timer; \n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/nl802154.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* for backwards compatibility \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/cfg80211.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/nl80211.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: need more info for other interface types\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/dvb/osd.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K// \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: remove \"test\" in final version\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/aio_abi.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t__u64\taio_reserved2;\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: use this for a (struct sigevent *) */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/atm.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t\t\t    /* set CLP bit value - \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/drm/omap_drm.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K maybe here we pass down info about what regions are touched\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/drm/drm_sarea.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    /** \\\u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K Use readers/writer lock for drm_sarea::drawable_lock */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/sound/compress_offload.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/ata.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\tATA_MAX_SECTORS_LBA48\t= 65535,/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: 65536? */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/byteorder/generic.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K:\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/netdevice.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K *\t@\u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K_list:\t\tDelayed register/unregister\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/netdevice.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\tstruct list_head\t\u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K_list;\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/netdevice.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t       NETREG_UNREGISTERED,\t/* completed unregister \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/netdevice.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kvoid netdev_run_\u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K(void);\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/pwm_backlight.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K remove once all users are switched to gpiod_* API */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/pagemap.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * (\u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: hugepage should have ->index in PAGE_SIZE)\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/platform_data/davinci_asp.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: Fix arch/arm/mach-davinci/ users and remove this define */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/dma-buf.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: Add try_map_dma_buf version, to return immed with -EBUSY\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/iio/iio.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K/* IIO \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K LIST */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/mtd/bbm.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K Add more NAND specific fileds */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/ti_wilink_st.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K#define FM_MAX_FRAME_SIZE 0xFF\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/ssb/ssb_regs.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: Make it deprecated */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/ssb/ssb_regs.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: Make it deprecated */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/ssb/ssb.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K store board flags in a single u64 */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/mfd/twl6040.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: All platform data struct can be removed */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/mfd/max14577-private.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: TABLE registers\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/mfd/max14577-private.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: CMD register\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/mfd/da9063/registers.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K#define\t\tDA9063_NONKEY_PIN_AU\u001b[97;45m\u001b[KTODO\u001b[m\u001b[KWN\t0x02\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/mfd/cros_ec_commands.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K(crosbug.com/p/11223): This is effectively useless; protocol is\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/mfd/cros_ec_commands.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K(crosbug.com/p/23570): These commands are deprecated, and will be\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/mfd/cros_ec_commands.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K(crosbug.com/p/23747): This is a confusing name, since it doesn't\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/linux/mfd/dbx500-prcmu.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: These should be prefixed.\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/drm/drmP.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K/* initial implementaton using a linked list - \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K hashtab */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/video/udlfb.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: Propose standard fb.h ioctl for reporting damage,\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/video/mmp_disp.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K: add query */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/media/davinci/vpfe_capture.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: This is to be split into multiple ioctls and also explore the\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/media/saa7146.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\tu32\t\t\t\tint_\u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K;\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/scsi/osd_initiator.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K   * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: In ver2 if at finalize time only one attr was set and no gets,\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/scsi/libfc.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * @stats:                 FC local port stats (\u001b[97;45m\u001b[KTODO\u001b[m\u001b[K separate libfc LLD stats)\n"
     ]
    }
   ],
   "source": [
    "grep -Ri \"todo\"  /usr/src/linux-headers-3.19.0-15-generic/include/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[K/home/bryan/MyMethod/analyze_mturk_membershipy.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/analyze_mturk_membershipy.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K re\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dill as pickle\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shelve\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pathos.multiprocessing as mp\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numexpr as ne\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K copy\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K networkx as nx\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K csv\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K matplotlib\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K matplotlib.pyplot as plt\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K gzip\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K lucene\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_iqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K utdeftvs\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K re\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dill as pickle\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shelve\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pathos.multiprocessing as mp\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numexpr as ne\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K copy\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K networkx as nx\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K csv\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K matplotlib\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K matplotlib.pyplot as plt\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K gzip\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K lucene\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_miqap_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K utdeftvs\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K re\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dill as pickle\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shelve\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pathos.multiprocessing as mp\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numexpr as ne\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K copy\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K networkx as nx\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K csv\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K matplotlib\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K matplotlib.pyplot as plt\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K gzip\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K lucene\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_vm_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K utdeftvs\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K re\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dill as pickle\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shelve\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pathos.multiprocessing as mp\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numexpr as ne\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K copy\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K networkx as nx\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K csv\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K matplotlib\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K matplotlib.pyplot as plt\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K gzip\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/build_ordered_wn_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K lucene\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca2.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca2.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca2.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca2.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K random\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca2.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca2.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K rpy2.robjects.packages as rpackages\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca2.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K rpy2.robjects.numpy2ri as np2ri\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca2.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K rpy2.robjects.pandas2ri as pd2ri\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca2.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K rpy2.robjects as ro\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca2.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K re\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K random\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K rpy2.robjects.packages as rpackages\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K rpy2.robjects.numpy2ri as np2ri\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K rpy2.robjects.pandas2ri as pd2ri\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K rpy2.robjects as ro\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/cca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K re\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/collect_wordnet_antonyms.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K utdeftvs\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/create_mturk_data.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/create_mturk_data_vm.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K re\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dill as pickle\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shelve\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pathos.multiprocessing as mp\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K codecs \n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K gzip\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K lucene\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K unicodedata\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_splitter.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/data_splitter.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shutil\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/extract_scale_og.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/extract_scale_og.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/extract_scales_and_eigens.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/extract_scales_and_eigens.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/extract_scales.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/extract_scales.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/findexample.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/findexample.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/generate_all_pairs_list.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/generate_all_pairs_size.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/generate_all_test_pairs_augmented.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/generate_all_test_pairs_augmented.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K random\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/generate_all_test_pairs_augmented.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/generate_all_test_pairs.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/generate_percentage_answers.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/generate_percentage_answers.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/generate_percentage_answers.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K csv\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/generate_shorter_answers.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K re\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dill as pickle\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shelve\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pathos.multiprocessing as mp\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K codecs\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K gzip\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K lucene\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K codecs\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/hartung.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas   \n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os.path\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K argparse\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numexpr as ne\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K random\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K depextract\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K utdeftvs\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K ctxpredict.models\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy.ma as ma\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K copy\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dill as pickle\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pathos.multiprocessing as mp\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shelve\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/lexsub.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K codecs\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/process_mturk_membership.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/process_mturk_membership.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/profile.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K lucene\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/profile.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pathos.multiprocessing as mp\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/profile.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dask\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/profile.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dask.multiprocessing\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/profile.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dask.threaded\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/profile.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K time\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shutil\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate_test_augmented.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate_test_augmented.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shutil\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate_test_augmented.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate_test_augmented.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate_test.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate_test.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shutil\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate_test.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/remove_partner_candidate_test.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/sigtest.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K numpy as np\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/sigtest.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/test_data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K os\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/test_data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K re\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/test_data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K dill as pickle\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/test_data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shelve\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/test_data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/test_data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pathos.multiprocessing as mp\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/test_data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K gzip\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/test_data_creation.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K lucene\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vanMiltenburg.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K lucene\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vanMiltenburg.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K shelve \n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vanMiltenburg.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K spacy\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vanMiltenburg.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pathos.multiprocessing as mp\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vanMiltenburg.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K codecs\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vanMiltenburg.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K re\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vanMiltenburg.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vanMiltenburg.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K itertools\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vanMiltenburg.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K rpy2\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vanMiltenburg.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K rpy2.robjects as ro\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vec_benchmark_alt.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K itertools\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vec_benchmark_alt.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vec_benchmark_alt.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K csv\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vec_benchmark_alt.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vec_benchmark.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K itertools\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vec_benchmark.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vec_benchmark.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K csv\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/vec_benchmark.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K sys\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/w2v_benchmark_alt.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K itertools\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/w2v_benchmark_alt.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/w2v_benchmark.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K itertools\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/w2v_benchmark.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/w2v_benchmark_test.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K itertools\n",
      "\u001b[35m\u001b[K/home/bryan/MyMethod/w2v_benchmark_test.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[97;45m\u001b[Kimport\u001b[m\u001b[K pandas as pd\n"
     ]
    }
   ],
   "source": [
    "grep -P \"^import\" ~/MyMethod/*.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#Must be -sp, -ps means \"s\" is the argument of -p\n",
      "read -sp \"Enter the secret word: \" secret\n",
      "\n",
      "#Not printing characters means that we need to \n",
      "#explicitly move to the next line\n",
      "echo\n",
      "echo \"Was I supposed to keep $secret a secret?\"\n"
     ]
    }
   ],
   "source": [
    "more read_ps_example.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "read -sp \"Enter the secret word: \" secret\n",
      "\n",
      "echo\n",
      "echo \"Was I supposed to keep $secret a secret?\"\n"
     ]
    }
   ],
   "source": [
    "grep -vP \"^#[^\\!]\" read_ps_example.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `grep` Output Options\n",
    "- -o: Only output the part of the line that matches\n",
    "- -color=COLOR_OPTION: suppress or enforce highlighting of matches in line\n",
    "- -l: Print only the file names where a match has been found\n",
    "- -L: Print only the file names where no match has been found\n",
    "- -h: Don't print the file name for each match\n",
    "- -n: Print the line number the match was found on\n",
    "- -c: Print the number of matches found in each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/dst.h:\t/* TODO : stats should be SMP safe */\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/9p/client.h: * TODO: This needs lots of explanation.\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/af_vsock.h:\t/* TODO: stream_bind() */\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/irda/irttp.h:\tstruct timer_list todo_timer; \n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/nl802154.h:\t/* for backwards compatibility TODO */\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/cfg80211.h: * TODO\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/nl80211.h: * TODO: need more info for other interface types\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/dvb/osd.h:// TODO: remove \"test\" in final version\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/aio_abi.h:\t__u64\taio_reserved2;\t/* TODO: use this for a (struct sigevent *) */\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/atm.h:\t\t\t    /* set CLP bit value - TODO */\n",
      "grep: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "grep -Ri \"todo\"  /usr/src/linux-headers-3.19.0-15-generic/include/ | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/dst.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K : stats should be SMP safe */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/9p/client.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: This needs lots of explanation.\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/af_vsock.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: stream_bind() */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/irda/irttp.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\tstruct timer_list \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K_timer; \n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/nl802154.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* for backwards compatibility \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/cfg80211.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/nl80211.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: need more info for other interface types\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/dvb/osd.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K// \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: remove \"test\" in final version\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/aio_abi.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t__u64\taio_reserved2;\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: use this for a (struct sigevent *) */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/atm.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t\t\t    /* set CLP bit value - \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K */\n",
      "grep: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "grep -Ri --color=always \"todo\"  /usr/src/linux-headers-3.19.0-15-generic/include/ | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/dst.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K352\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K : stats should be SMP safe */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/9p/client.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K182\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: This needs lots of explanation.\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/af_vsock.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K110\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: stream_bind() */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/irda/irttp.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K130\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\tstruct timer_list \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K_timer; \n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/nl802154.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K110\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t/* for backwards compatibility \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/net/cfg80211.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K3780\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/nl80211.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K55\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K * \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: need more info for other interface types\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/dvb/osd.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K93\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K// \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: remove \"test\" in final version\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/aio_abi.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K95\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t__u64\taio_reserved2;\t/* \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K: use this for a (struct sigevent *) */\n",
      "\u001b[35m\u001b[K/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/atm.h\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K68\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\t\t\t    /* set CLP bit value - \u001b[97;45m\u001b[KTODO\u001b[m\u001b[K */\n",
      "grep: write error\n"
     ]
    }
   ],
   "source": [
    "grep -Rin --color=always \"todo\"  /usr/src/linux-headers-3.19.0-15-generic/include/ | head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/dst.h\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/9p/client.h\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/af_vsock.h\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/irda/irttp.h\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/nl802154.h\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/net/cfg80211.h\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/nl80211.h\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/dvb/osd.h\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/aio_abi.h\n",
      "/usr/src/linux-headers-3.19.0-15-generic/include/uapi/linux/atm.h\n"
     ]
    }
   ],
   "source": [
    "grep -Ril \"todo\"  /usr/src/linux-headers-3.19.0-15-generic/include/ | head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bryan/MyMethod/analyze_mturk_membershipy.py:2\n",
      "/home/bryan/MyMethod/build_ordered_iqap_pairs.py:16\n",
      "/home/bryan/MyMethod/build_ordered_miqap_pairs.py:16\n",
      "/home/bryan/MyMethod/build_ordered_vm_pairs.py:16\n",
      "/home/bryan/MyMethod/build_ordered_wn_pairs.py:15\n",
      "/home/bryan/MyMethod/cca2.py:10\n",
      "/home/bryan/MyMethod/cca.py:10\n",
      "/home/bryan/MyMethod/collect_wordnet_antonyms.py:1\n",
      "/home/bryan/MyMethod/create_mturk_data.py:1\n",
      "/home/bryan/MyMethod/create_mturk_data_vm.py:1\n"
     ]
    }
   ],
   "source": [
    "grep -Pc \"^import\" ~/MyMethod/*.py | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import sys\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n"
     ]
    }
   ],
   "source": [
    "grep -Ph \"^import\" ~/MyMethod/*.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Printing More Context per Match\n",
    "- It is often useful, especially in debugging, to print some lines around each match\n",
    "- `grep` has three flags that control this, each taking a numerical argument\n",
    "    - -A NUM: Print the NUM lines after each match\n",
    "    - -B NUM: Print the NUM lines before each match\n",
    "    - -C NUM: Print the NUM lines before and after each match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/* initial implementaton using a linked list - \u001b[97;45m\u001b[Ktodo\u001b[m\u001b[K hashtab */\n",
      "struct drm_prime_file_private {\n",
      "\tstruct list_head head;\n",
      "\tstruct mutex lock;\n",
      "};\n"
     ]
    }
   ],
   "source": [
    "grep -A4 \"todo\"  /usr/src/linux-headers-3.19.0-15-generic/include/drm/drmP.h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tint switch_power_state;\n",
      "};\n",
      "\n",
      "#define DRM_SWITCH_POWER_ON 0\n",
      "#define DRM_SWITCH_POWER_OFF 1\n",
      "#define DRM_SWITCH_POWER_CHANGING 2\n",
      "#define DRM_SWITCH_POWER_DYNAMIC_OFF 3\n",
      "\n",
      "\t\t\t\t\t     int feature)\n",
      "{\n",
      "\t\u001b[97;45m\u001b[Kreturn\u001b[m\u001b[K ((dev->driver->driver_features & feature) ? 1 : 0);\n",
      "}\n",
      "\n",
      "{\n",
      "\tsmp_wmb();\n",
      "\tatomic_set(&dev->unplugged, 1);\n",
      "}\n",
      "\n",
      "{\n",
      "\tint ret = atomic_read(&dev->unplugged);\n",
      "\tsmp_rmb();\n",
      "\t\u001b[97;45m\u001b[Kreturn\u001b[m\u001b[K ret;\n",
      "}\n",
      "\n",
      "{\n",
      "\t\u001b[97;45m\u001b[Kreturn\u001b[m\u001b[K file_priv->minor->type == DRM_MINOR_RENDER;\n"
     ]
    }
   ],
   "source": [
    "grep -v \"^/[\\*/]\" /usr/src/linux-headers-3.19.0-15-generic/include/drm/drmP.h \\\n",
    "| grep -v \"\\s*\\*\"  | grep -B10 -m3 -P 'return' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\thuge\t_\tJJ\t_\t12\tamod\n",
      "2\thuge\t_\tJJ\t_\t7\tamod\n"
     ]
    }
   ],
   "source": [
    "grep -m2 -P --color=never \"huge\\t_\\tJJ\\t\" ~/wackyAll.deps.sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\tby\t_\tIN\t_\t7\tcase\n",
      "7\teveryone\t_\tNN\t_\t5\tnmod\n",
      "8\tand\t_\tCC\t_\t7\tcc\n",
      "9\tit\t_\tPRP\t_\t10\tnsubj\n",
      "10\tattracted\t_\tVBD\t_\t7\tconj\n",
      "11\thuge\t_\tJJ\t_\t12\tamod\n",
      "12\taudiences\t_\tNNS\t_\t10\tdobj\n",
      "13\twhich\t_\tWDT\t_\t15\tnsubj\n",
      "14\tcompletely\t_\tRB\t_\t15\tadvmod\n",
      "15\tfilled\t_\tVBD\t_\t12\tacl:relcl\n",
      "16\tthe\t_\tDT\t_\t17\tdet\n",
      "--\n",
      "19\ttrans-Pennine\t_\tNN\t_\t20\tcompound\n",
      "20\troute\t_\tNN\t_\t10\tnmod\n",
      "21\t.\t_\t.\t_\t7\tpunct\n",
      "\n",
      "1\tThe\t_\tDT\t_\t7\tdet\n",
      "2\thuge\t_\tJJ\t_\t7\tamod\n",
      "3\t,\t_\t,\t_\t7\tpunct\n",
      "4\ttwin\t_\tJJ\t_\t7\tamod\n",
      "5\t76\t_\tCD\t_\t7\tnummod\n",
      "6\tmeter\t_\tNN\t_\t7\tcompound\n",
      "7\ttowers\t_\tNNS\t_\t13\tnsubj\n"
     ]
    }
   ],
   "source": [
    "grep -m2 -P -C5 --color=never \"huge\\t_\\tJJ\\t\" ~/wackyAll.deps.sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `sed` and `awk`\n",
    "- `sed` and `awk` are both extremely popular tools for text manipulation from the command line\n",
    "    - `sed` is short for Stream EDitor\n",
    "    - `awk` is named after the last names of the 3 creators\n",
    "- Both `sed` and `awk` are full-fledges scripting languges in their own right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `awk` Scripts\n",
    "- An `awk` script is a series of rules of the format\n",
    "```awk\n",
    "pattern { action }\n",
    "```\n",
    "- The `awk` script is running either directly as an argument to the `awk` command, or from a file specified to `awk`\n",
    "```awk\n",
    "awk 'SCRIPT' INPUT_FILE\n",
    "awk -f SCRIPT_FILE INPUT_FILE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `awk` Scripts Continued\n",
    "- Two special patterns exist in `awk`\n",
    "    - BEGIN will match before any input is read \n",
    "    - END will match after all input is read\n",
    "- The most common action is print, often seen as\n",
    "```bash\n",
    "{ print $0; }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import sys\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import gzip\n",
      "import lucene\n",
      "import utdeftvs\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import gzip\n",
      "import lucene\n",
      "import utdeftvs\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import gzip\n",
      "import lucene\n",
      "import utdeftvs\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import gzip\n",
      "import lucene\n",
      "import sys\n",
      "import pandas\n",
      "import numpy as np\n",
      "import random\n",
      "import numpy\n",
      "import rpy2.robjects.packages as rpackages\n",
      "import rpy2.robjects.numpy2ri as np2ri\n",
      "import rpy2.robjects.pandas2ri as pd2ri\n",
      "import rpy2.robjects as ro\n",
      "import re\n",
      "import sys\n",
      "import pandas\n",
      "import numpy as np\n",
      "import random\n",
      "import numpy\n",
      "import rpy2.robjects.packages as rpackages\n",
      "import rpy2.robjects.numpy2ri as np2ri\n",
      "import rpy2.robjects.pandas2ri as pd2ri\n",
      "import rpy2.robjects as ro\n",
      "import re\n",
      "import utdeftvs\n",
      "import pandas as pd\n",
      "import pandas as pd\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import codecs \n",
      "import gzip\n",
      "import lucene\n",
      "import sys\n",
      "import unicodedata\n",
      "import os\n",
      "import shutil\n",
      "import sys\n",
      "import pandas as pd\n",
      "import sys\n",
      "import pandas as pd\n",
      "import sys\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import numpy as np\n",
      "import numpy as np\n",
      "import random\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import sys\n",
      "import csv\n",
      "import sys\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import codecs\n",
      "import gzip\n",
      "import lucene\n",
      "import sys\n",
      "import codecs\n",
      "import pandas   \n",
      "import sys\n",
      "import os.path\n",
      "import argparse\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import random\n",
      "import pandas as pd\n",
      "import depextract\n",
      "import utdeftvs\n",
      "import ctxpredict.models\n",
      "import numpy.ma as ma\n",
      "import copy\n",
      "import dill as pickle\n",
      "import pathos.multiprocessing as mp\n",
      "import shelve\n",
      "import codecs\n",
      "import pandas as pd\n",
      "import sys\n",
      "import lucene\n",
      "import pathos.multiprocessing as mp\n",
      "import dask\n",
      "import dask.multiprocessing\n",
      "import dask.threaded\n",
      "import time\n",
      "import sys\n",
      "import shutil\n",
      "import os\n",
      "import pandas as pd\n",
      "import sys\n",
      "import shutil\n",
      "import os\n",
      "import pandas as pd\n",
      "import sys\n",
      "import shutil\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import gzip\n",
      "import lucene\n",
      "import lucene\n",
      "import shelve \n",
      "import spacy\n",
      "import pathos.multiprocessing as mp\n",
      "import codecs\n",
      "import re\n",
      "import pandas as pd\n",
      "import itertools\n",
      "import rpy2\n",
      "import rpy2.robjects as ro\n",
      "import itertools\n",
      "import pandas as pd\n",
      "import csv\n",
      "import sys\n",
      "import itertools\n",
      "import pandas as pd\n",
      "import csv\n",
      "import sys\n",
      "import itertools\n",
      "import pandas as pd\n",
      "import itertools\n",
      "import pandas as pd\n",
      "import itertools\n",
      "import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "awk '/^import/ { print $0; }' ~/MyMethod/*.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import sys\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import gzip\n",
      "import lucene\n",
      "import utdeftvs\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import gzip\n",
      "import lucene\n",
      "import utdeftvs\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import gzip\n",
      "import lucene\n",
      "import utdeftvs\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import gzip\n",
      "import lucene\n",
      "import sys\n",
      "import pandas\n",
      "import numpy as np\n",
      "import random\n",
      "import numpy\n",
      "import rpy2.robjects.packages as rpackages\n",
      "import rpy2.robjects.numpy2ri as np2ri\n",
      "import rpy2.robjects.pandas2ri as pd2ri\n",
      "import rpy2.robjects as ro\n",
      "import re\n",
      "import sys\n",
      "import pandas\n",
      "import numpy as np\n",
      "import random\n",
      "import numpy\n",
      "import rpy2.robjects.packages as rpackages\n",
      "import rpy2.robjects.numpy2ri as np2ri\n",
      "import rpy2.robjects.pandas2ri as pd2ri\n",
      "import rpy2.robjects as ro\n",
      "import re\n",
      "import utdeftvs\n",
      "import pandas as pd\n",
      "import pandas as pd\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import codecs \n",
      "import gzip\n",
      "import lucene\n",
      "import sys\n",
      "import unicodedata\n",
      "import os\n",
      "import shutil\n",
      "import sys\n",
      "import pandas as pd\n",
      "import sys\n",
      "import pandas as pd\n",
      "import sys\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import numpy as np\n",
      "import numpy as np\n",
      "import random\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import sys\n",
      "import csv\n",
      "import sys\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import codecs\n",
      "import gzip\n",
      "import lucene\n",
      "import sys\n",
      "import codecs\n",
      "import pandas   \n",
      "import sys\n",
      "import os.path\n",
      "import argparse\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import random\n",
      "import pandas as pd\n",
      "import depextract\n",
      "import utdeftvs\n",
      "import ctxpredict.models\n",
      "import numpy.ma as ma\n",
      "import copy\n",
      "import dill as pickle\n",
      "import pathos.multiprocessing as mp\n",
      "import shelve\n",
      "import codecs\n",
      "import pandas as pd\n",
      "import sys\n",
      "import lucene\n",
      "import pathos.multiprocessing as mp\n",
      "import dask\n",
      "import dask.multiprocessing\n",
      "import dask.threaded\n",
      "import time\n",
      "import sys\n",
      "import shutil\n",
      "import os\n",
      "import pandas as pd\n",
      "import sys\n",
      "import shutil\n",
      "import os\n",
      "import pandas as pd\n",
      "import sys\n",
      "import shutil\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import os\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import gzip\n",
      "import lucene\n",
      "import lucene\n",
      "import shelve \n",
      "import spacy\n",
      "import pathos.multiprocessing as mp\n",
      "import codecs\n",
      "import re\n",
      "import pandas as pd\n",
      "import itertools\n",
      "import rpy2\n",
      "import rpy2.robjects as ro\n",
      "import itertools\n",
      "import pandas as pd\n",
      "import csv\n",
      "import sys\n",
      "import itertools\n",
      "import pandas as pd\n",
      "import csv\n",
      "import sys\n",
      "import itertools\n",
      "import pandas as pd\n",
      "import itertools\n",
      "import pandas as pd\n",
      "import itertools\n",
      "import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "awk '/^import/' ~/MyMethod/*.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import sys\n",
      "from itertools import combinations,product\n",
      "from nltk import wordnet, ngrams\n",
      "from nltk.corpus import framenet as fn\n",
      "\n",
      "data = pd.read_csv(sys.argv[1])\n",
      "data['unfiltered'] = data.apply(lambda x: x['0_x'].split(',') + [x['word1']] + [x['word2']],axis=1).tolist()\n",
      "\n",
      "\n",
      "to_remove = []\n",
      "\n",
      "for i, row in data.iterrows():\n",
      "        for j, row2 in data.iterrows():\n",
      "                if set(row['unfiltered']) < set(row2['unfiltered']):\n",
      "                        to_remove.append(i)\n",
      "     \n",
      "\n",
      "\n",
      "for i, row in data.iterrows():\n",
      "        \n",
      "        for j, row2 in data.iterrows():\n",
      "\n",
      "                if i != j and set(row['unfiltered']) == set(row2['unfiltered']) and j not in to_remove:\n",
      "                        to_remove.append(i)\n",
      "          \n",
      "unfiltered = data.drop(to_remove)\n",
      "#Weird Edge Case\n",
      "unfiltered = unfiltered[unfiltered['0_x'] !=  'Series([],']\n",
      "to_remove = []\n",
      "for i, row in data.iterrows():\n",
      "        for j, row2 in data.iterrows():\n",
      "                if set(eval(row['final_output'])) < set(eval(row2['final_output'])):\n",
      "                        to_remove.append(i)\n",
      "        \n",
      "\n",
      "\n",
      "for i, row in data.iterrows():\n",
      "        \n",
      "        for j, row2 in data.iterrows():\n",
      "\n",
      "                if i != j and set(eval(row['final_output'])) == set(eval(row2['final_output'])) and j not in to_remove:\n",
      "                        to_remove.append(i)\n",
      "                \n",
      "\n",
      "filtered = data.drop(to_remove)\n",
      "\n",
      "print len(data), len(unfiltered), len(filtered)\n",
      "\n",
      "pairs = []\n",
      "with open(sys.argv[2]) as f:                             \n",
      "\tfor line in f:                                                     \n",
      "\t\tpair = line.strip().split(',')                                 \n",
      "\t\tpairs.append(sorted(pair))  \n",
      "\n",
      "\n",
      "def count_existing(row):                                               \n",
      "\tmembers = row['0_x'].split(',')                                    \n",
      "\ti = 0                                                       \n",
      "\tfor p in list(combinations(members,2)):                            \n",
      "\t\tif sorted(p) in pairs:                                         \n",
      "\t\t\ti += 1                                         \n",
      "\tfor pair in product(members,[row['word1'],row['word2']]):   \n",
      "\t\tif sorted(pair) in pairs:                                      \n",
      "\t\t\ti += 1                                              \n",
      "\treturn i                                              \n",
      "\n",
      "def count_in_wn(row):\n",
      "\tmembers = row['0_x'].split(',')\n",
      "\ti = 0\n",
      "\tfor word in members:\n",
      "\t\tif wordnet.wordnet.synsets(word,wordnet.wordnet.ADJ):\n",
      "\t\t\ti +=1\n",
      "\treturn i\n",
      "\n",
      "def count_in_fn(row):\n",
      "   \tmembers = row['0_x'].split(',')\n",
      "        i = 0\n",
      "        for word in members:\n",
      "                if filter(lambda x: x['name'] == (word + '.a'), fn.lus(r'%s.a' % word)):\n",
      "                        i +=1\n",
      "        return i  \n",
      "\n",
      "def count_possible(row):\n",
      "\tmembers = row['0_x'].split(',')\n",
      "        \n",
      "        return len(list(combinations(members,2))) + len(list(product(members,[row['word1'],row['word2']])))\n",
      "\n",
      "\n",
      "def get_attributes(words):                                \n",
      "\tsynsets_for_words = []\n",
      "    \n",
      "\tattributes = []\n",
      "\ti = 0\n",
      "\tcovered = 0\n",
      "\tfound = False\n",
      "\n",
      "\tfor word in words:                                                             \n",
      "\t\tpotential_synsets = wordnet.wordnet.synsets(word,wordnet.wordnet.ADJ)\n",
      "\t\tnum_atts = {}\n",
      "\t\t\n",
      "\t\tfor synset in potential_synsets:\n",
      "\t\t\tnum_atts[synset] = sum(map(lambda y: len(y.attributes()) ,synset.similar_tos())) + len(synset.attributes())\n",
      "\t\t\n",
      "\t\tfiltered_synsets = filter(lambda x: num_atts[x] > 0, potential_synsets)\n",
      "\t\tif filtered_synsets:\n",
      "\t\t\tsynsets_for_words.append(filtered_synsets)\n",
      "\n",
      "\tstart_length = len(synsets_for_words)\n",
      "\n",
      "\twhile len(synsets_for_words) and i < start_length:\n",
      "\t\tfor combo in combinations(synsets_for_words,start_length-i):\n",
      "\t\t\tfor group in product(*combo):\n",
      "\t\t\t\tif len(reduce(set.intersection,map(lambda x:set(x.attributes()),group)))> 0:\n",
      "\t\t\t\t\tattributes.append(reduce(set.intersection,map(lambda x:set(x.attributes()),group)))\n",
      "\t\t\t\t\tsynsets_for_words = filter(lambda x: not any(map(lambda y: y in x, list(group))),synsets_for_words)\n",
      "\t\t\t\t\tcovered += len(group)\n",
      "\t\t\t\t\tbreak\n",
      "\t\ti += 1\n",
      "\n",
      "\treturn pd.Series([attributes,covered],index=['attributes','covered'])\n",
      "   \n",
      "def get_frames(words):                                \n",
      "\tlus_for_words = []\n",
      "\tframes = []\n",
      "\tcovered = 0\n",
      " \ti = 0\n",
      "     \n",
      "\tfor word in words:\n",
      "\t#\tlus_for_words.append(fn.lus(r'%s.a' % word))\n",
      "\t\tlus_for_words.append(filter(lambda x: x['name'] == '%s.a'%word, fn.lus(r'%s.a' % word)))\n",
      "     \n",
      "\tstart_length = len(lus_for_words)\n",
      "\twhile len(lus_for_words) and i < start_length:\n",
      "\t\tfor combo in combinations(lus_for_words,start_length-i):\n",
      "\t\t\tfor group in product(*combo):\n",
      "\t\t\t\tif len(reduce(set.intersection,map(lambda x:set([x['frame']['name']]),group))):\n",
      "\t\t\t\t\tframes.append(reduce(set.intersection,map(lambda x: set([x['frame']['name']]), group)))\n",
      "\t\t\t\t\tlus_for_words = filter(lambda x: not any(map(lambda y: y in x, list(group))),lus_for_words)\n",
      "\t\t\t\t\tcovered += len(group)\n",
      "\t\t\t\t\tbreak\n",
      "\t\ti += 1\n",
      "\treturn pd.Series([frames,covered],index=['frames','covered'])\n",
      "\n",
      "          \n",
      "unfiltered['prexisting'] = unfiltered.apply(count_existing,axis=1)       \n",
      "filtered['prexisting'] = filtered.apply(count_existing,axis=1)\n",
      "\n",
      "unfiltered['wn_prexisting'] = unfiltered.apply(count_in_wn,axis=1)\n",
      "filtered['wn_prexisting'] = filtered.apply(count_in_wn,axis=1)\n",
      "\n",
      "unfiltered['fn_prexisting'] = unfiltered.apply(count_in_fn,axis=1)\n",
      "filtered['fn_prexisting'] = filtered.apply(count_in_fn,axis=1)\n",
      "\n",
      "unfiltered['possible'] = unfiltered.apply(count_possible,axis=1)\n",
      "filtered['possible'] = filtered.apply(count_possible,axis=1)\n",
      "\n",
      "unfiltered['existing_percent'] =  unfiltered['prexisting']/unfiltered['possible']\n",
      "filtered['existing_percent'] =  filtered['prexisting']/filtered['possible']\n",
      "\n",
      "unfiltered['wn_percent'] =  unfiltered['wn_prexisting']/unfiltered['0_x'].apply(lambda x: len(x.split(',')))\n",
      "filtered['wn_percent'] =  filtered['wn_prexisting']/filtered['0_x'].apply(lambda x: len(x.split(',')))\n",
      "\n",
      "unfiltered['fn_percent'] =  unfiltered['fn_prexisting']/unfiltered['0_x'].apply(lambda x: len(x.split(',')))\n",
      "filtered['fn_percent'] =  filtered['fn_prexisting']/filtered['0_x'].apply(lambda x: len(x.split(',')))\n",
      "\n",
      "print \"UNFILTERED\"\n",
      "print unfiltered['existing_percent'].describe()\n",
      "print \"FILTERED\"\n",
      "print filtered['existing_percent'].describe()\n",
      "\n",
      "\n",
      "print \"UNFILTERED - WN EXIST\"\n",
      "print unfiltered['wn_percent'].describe()\n",
      "print \"FILTERED - WN EXIST\"\n",
      "print filtered['wn_percent'].describe()\n",
      "\n",
      "print \"UNFILTERED - FN EXIST\"\n",
      "print unfiltered['fn_percent'].describe()\n",
      "print \"FILTERED - FN EXIST\"\n",
      "print filtered['fn_percent'].describe()\n",
      "\n",
      "print \"UNFILTERED - WORDNET COVERED\"\n",
      "unfiltered[['attributes','covered']] = unfiltered['unfiltered'].apply(get_attributes)\n",
      "print (unfiltered['covered']/unfiltered['unfiltered'].apply(len)).describe()\n",
      "\n",
      "filtered[['attributes','covered']] = filtered['final_output'].apply(eval).apply(get_attributes)\n",
      "print \"FILTERED - WORDNET COVERED\"\n",
      "print (filtered['covered']/filtered['final_output'].apply(eval).apply(len)).describe()\n",
      "\n",
      "unfiltered['attribute_length'] = unfiltered['attributes'].apply(len)\n",
      "print \"UNFILTERED - WORDNET ATTRIBUTE LENGTH\"\n",
      "print unfiltered['attribute_length'].describe()\n",
      "\n",
      "filtered['attribute_length'] = filtered['attributes'].apply(len)\n",
      "print \"FILTERED - WORDNET ATTRIBUTE LENGTH\"\n",
      "print filtered['attribute_length'].describe()\n",
      "\n",
      "unfiltered[['frames','covered_frames']] = unfiltered['unfiltered'].apply(get_frames)\n",
      "print \"UNFILTERED - FRAMENET COVERED\"\n",
      "print (unfiltered['covered_frames']/unfiltered['unfiltered'].apply(len)).describe()\n",
      "\n",
      "\n",
      "filtered[['frames','covered_frames']] = filtered['final_output'].apply(eval).apply(get_frames)\n",
      "print \"FILTERED - FRAMENET COVERED\"\n",
      "print (filtered['covered_frames']/filtered['final_output'].apply(eval).apply(len)).describe()\n",
      "\n",
      "unfiltered['frame_length'] = unfiltered['frames'].apply(len)\n",
      "print \"UNFILTERED - FRAMENET FRAME LENGTH\"\n",
      "print unfiltered['frame_length'].describe()\n",
      "\n",
      "filtered['frame_length'] = filtered['frames'].apply(len)\n",
      "print \"FILTERED - FRAMENET FRAME LENGTH\"\n",
      "print filtered['frame_length'].describe()\n",
      "\n",
      "\n",
      "\n",
      "import os\n",
      "from docopt import docopt\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "from collections import defaultdict\n",
      "from sklearn.preprocessing import normalize\n",
      "from sklearn.cluster import KMeans\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from graph_tool.all import *\n",
      "import networkx as nx\n",
      "import csv\n",
      "\n",
      "import matplotlib\n",
      "matplotlib.use('Agg')\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import gzip\n",
      "# import StringIO\n",
      "import lucene\n",
      "from random import random, sample\n",
      "lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n",
      "from lucene import VERSION\n",
      "from org.apache.lucene import analysis, util\n",
      "from lupyne import engine\n",
      "from cStringIO import StringIO\n",
      "from itertools import izip\n",
      "from collections import defaultdict\n",
      "from string import Template\n",
      "\n",
      "def tokenizer(reader):\n",
      "    return analysis.standard.StandardTokenizer(util.Version.LATEST, reader)\n",
      "myAnalyzer = engine.indexers.Analyzer(tokenizer,\n",
      "     analysis.standard.StandardFilter, analysis.LowerCaseFilter)\n",
      "\n",
      "indexer = engine.IndexSearcher(\n",
      "      '/home/bryan/ukwakSentenceIndex', analyzer=myAnalyzer)\n",
      "\n",
      "import utdeftvs\n",
      "\n",
      "space = utdeftvs.load_numpy(\"data/lexsub_embeddings_withcontexts.npz\", True)\n",
      "\n",
      "patterns = [Template('text:\"$x but not $y\"'),\n",
      "                     Template('text:\"$x if not $y\"'),\n",
      "                     Template('text:\"$x although not $y\"'),\n",
      "                     Template('text:\"$x though not $y\"'),\n",
      "                     Template('text:\"$x and even $y\"'),\n",
      "                     Template('text:\"$x or even $y\"'),\n",
      "                     Template('text:\"$x and almost $y\"'),\n",
      "                     Template('text:\"$x or almost $y\"'),\n",
      "                     Template('text:\"not only $x but $y\"'),\n",
      "                     Template('text:\"not just $x but $y\"'),\n",
      "                Template('text:\"not $x just $y\"'),\n",
      "                  Template('text:\"not $x but just $y\"'),\n",
      "                  Template('text:\"not $x still $y\"'),\n",
      "                  Template('text:\"not $x but still $y\"'),\n",
      "                  Template('text:\"not $x although still $y\"'),\n",
      "                  Template('text:\"not $x though still $y\"'),\n",
      "                  Template('text:\"$x or very $y\"'),\n",
      "  Template('text:\"$y very $x\"'),\n",
      "                  Template('text:\"not $y but $x enough\"'),\n",
      "                  Template('text:\"$y unbelievably $x\"'),\n",
      "                  Template('text:\"$y not even $x\"'),\n",
      "  Template('text:\"$x even $y\"'),\n",
      "                     Template('text:\"$x no $y\"'),\n",
      "                     Template('text:\"$x perhaps $y\"'),\n",
      "                     Template('text:\"$x sometimes $y\"'),\n",
      "                    Template('text:\"extremely $x $y\"'),\n",
      "                    Template('text:\"are very $x $y\"'),\n",
      "                    Template('text:\"is very $x $y\"')]\n",
      "\n",
      "pair_counts = defaultdict(int)\n",
      " \n",
      "with open('iqap_seeds.csv') as f:\n",
      "    \n",
      "    for line in f:\n",
      "        pairs = sorted(line.strip().split(','))\n",
      "\tif pairs[0] in space.vocab and pairs[1] in space.vocab:\n",
      "\t\tfor pat in patterns:\n",
      "\t\t\tpat1 = pat.substitute({'x': pairs[0], 'y': pairs[1]})\n",
      "        \t        pat2 = pat.substitute({'x': pairs[1], 'y': pairs[0]})\n",
      "                \tpair_counts[','.join(pairs)] +=  indexer.search(pat1).count\n",
      "\t                pair_counts[','.join(pairs)] +=  indexer.search(pat2).count\n",
      "\n",
      "pc = pd.Series(pair_counts)\n",
      "pc = pc.sort_values(ascending=False)\n",
      "print len(pc)\n",
      "batch= 20\n",
      "group1 = pc.ix[0:batch]\n",
      "group2 = pc.ix[batch:batch*2]\n",
      "group3 = pc.ix[batch*2:batch*3]\n",
      "group4 = pc.ix[batch*3:batch*4]\n",
      "group5= pc.ix[batch*4:batch*5]\n",
      "group6 = pc.ix[batch*5:]\n",
      "\n",
      "combo = zip(group1.index.tolist(), group2.sort_values().index.tolist(), group3.index.tolist(), group4.index.tolist(),group5.sort_values().index.tolist())\n",
      "combo = map(list,combo)\n",
      "combo[0].append(group6.index[0])\n",
      "combo[1].append(group6.index[1])\n",
      "combo[2].append(group6.index[2])\n",
      "combo[3].append(group6.index[3])\n",
      "\n",
      "pd.DataFrame(map(lambda x: map(lambda y: y.replace(',','-') ,x), combo)).to_csv('qap_ordered_groups.csv',header=None,index=None)\n",
      "import os\n",
      "from docopt import docopt\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "from collections import defaultdict\n",
      "from sklearn.preprocessing import normalize\n",
      "from sklearn.cluster import KMeans\n",
      "# from graph_tool.all import *\n",
      "import networkx as nx\n",
      "import csv\n",
      "\n",
      "import matplotlib\n",
      "matplotlib.use('Agg')\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import gzip\n",
      "# import StringIO\n",
      "import lucene\n",
      "from random import random, sample\n",
      "lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n",
      "from lucene import VERSION\n",
      "from org.apache.lucene import analysis, util\n",
      "from lupyne import engine\n",
      "from cStringIO import StringIO\n",
      "from itertools import izip\n",
      "from collections import defaultdict\n",
      "from string import Template\n",
      "\n",
      "def tokenizer(reader):\n",
      "    return analysis.standard.StandardTokenizer(util.Version.LATEST, reader)\n",
      "myAnalyzer = engine.indexers.Analyzer(tokenizer,\n",
      "     analysis.standard.StandardFilter, analysis.LowerCaseFilter)\n",
      "\n",
      "indexer = engine.IndexSearcher(\n",
      "      '/home/bryan/ukwakSentenceIndex', analyzer=myAnalyzer)\n",
      "\n",
      "import utdeftvs\n",
      "\n",
      "space = utdeftvs.load_numpy(\"data/lexsub_embeddings_withcontexts.npz\", True)\n",
      "\n",
      "patterns = [Template('text:\"$x but not $y\"'),\n",
      "                     Template('text:\"$x if not $y\"'),\n",
      "                     Template('text:\"$x although not $y\"'),\n",
      "                     Template('text:\"$x though not $y\"'),\n",
      "                     Template('text:\"$x and even $y\"'),\n",
      "                     Template('text:\"$x or even $y\"'),\n",
      "                     Template('text:\"$x and almost $y\"'),\n",
      "                     Template('text:\"$x or almost $y\"'),\n",
      "                     Template('text:\"not only $x but $y\"'),\n",
      "                     Template('text:\"not just $x but $y\"'),\n",
      "                Template('text:\"not $x just $y\"'),\n",
      "                  Template('text:\"not $x but just $y\"'),\n",
      "                  Template('text:\"not $x still $y\"'),\n",
      "                  Template('text:\"not $x but still $y\"'),\n",
      "                  Template('text:\"not $x although still $y\"'),\n",
      "                  Template('text:\"not $x though still $y\"'),\n",
      "                  Template('text:\"$x or very $y\"'),\n",
      "  Template('text:\"$y very $x\"'),\n",
      "                  Template('text:\"not $y but $x enough\"'),\n",
      "                  Template('text:\"$y unbelievably $x\"'),\n",
      "                  Template('text:\"$y not even $x\"'),\n",
      "  Template('text:\"$x even $y\"'),\n",
      "                     Template('text:\"$x no $y\"'),\n",
      "                     Template('text:\"$x perhaps $y\"'),\n",
      "                     Template('text:\"$x sometimes $y\"'),\n",
      "                    Template('text:\"extremely $x $y\"'),\n",
      "                    Template('text:\"are very $x $y\"'),\n",
      "                    Template('text:\"is very $x $y\"')]\n",
      "\n",
      "pair_counts = defaultdict(int)\n",
      " \n",
      "with open('miqap_seeds.csv') as f:\n",
      "    \n",
      "    for line in f:\n",
      "        pairs = sorted(line.strip().split(','))\n",
      "\tif pairs[0] in space.vocab and pairs[1] in space.vocab:\n",
      "\t\tfor pat in patterns:\n",
      "\t\t\tpat1 = pat.substitute({'x': pairs[0], 'y': pairs[1]})\n",
      "        \t        pat2 = pat.substitute({'x': pairs[1], 'y': pairs[0]})\n",
      "                \tpair_counts[','.join(pairs)] +=  indexer.search(pat1).count\n",
      "\t                pair_counts[','.join(pairs)] +=  indexer.search(pat2).count\n",
      "\n",
      "pc = pd.Series(pair_counts)\n",
      "pc = pc.sort_values(ascending=False)\n",
      "print len(pc)\n",
      "batch= 30\n",
      "group1 = pc.ix[0:batch]\n",
      "group2 = pc.ix[batch:batch*2]\n",
      "group3 = pc.ix[batch*2:batch*3]\n",
      "group4 = pc.ix[batch*3:batch*4]\n",
      "group5= pc.ix[batch*4:batch*5]\n",
      "group6 = pc.ix[batch*5:]\n",
      "\n",
      "combo = zip(group1.index.tolist(), group2.sort_values().index.tolist(), group3.index.tolist(), group4.index.tolist(),group5.sort_values().index.tolist())\n",
      "combo = map(list,combo)\n",
      "combo[0].append(group6.index[0])\n",
      "combo[1].append(group6.index[1])\n",
      "combo[2].append(group6.index[2])\n",
      "\n",
      "pd.DataFrame(map(lambda x: map(lambda y: y.replace(',','-') ,x), combo)).to_csv('qap_ordered_groups.csv',header=None,index=None)\n",
      "import os\n",
      "from docopt import docopt\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "from collections import defaultdict\n",
      "from sklearn.preprocessing import normalize\n",
      "from sklearn.cluster import KMeans\n",
      "# from graph_tool.all import *\n",
      "import networkx as nx\n",
      "import csv\n",
      "\n",
      "import matplotlib\n",
      "matplotlib.use('Agg')\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import gzip\n",
      "# import StringIO\n",
      "import lucene\n",
      "from random import random, sample\n",
      "lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n",
      "from lucene import VERSION\n",
      "from org.apache.lucene import analysis, util\n",
      "from lupyne import engine\n",
      "from cStringIO import StringIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from itertools import izip\n",
      "from collections import defaultdict\n",
      "from string import Template\n",
      "\n",
      "def tokenizer(reader):\n",
      "    return analysis.standard.StandardTokenizer(util.Version.LATEST, reader)\n",
      "myAnalyzer = engine.indexers.Analyzer(tokenizer,\n",
      "     analysis.standard.StandardFilter, analysis.LowerCaseFilter)\n",
      "\n",
      "indexer = engine.IndexSearcher(\n",
      "      '/home/bryan/ukwakSentenceIndex', analyzer=myAnalyzer)\n",
      "\n",
      "import utdeftvs\n",
      "\n",
      "space = utdeftvs.load_numpy(\"data/lexsub_embeddings_withcontexts.npz\", True)\n",
      "\n",
      "patterns = [Template('text:\"$x but not $y\"'),\n",
      "                     Template('text:\"$x if not $y\"'),\n",
      "                     Template('text:\"$x although not $y\"'),\n",
      "                     Template('text:\"$x though not $y\"'),\n",
      "                     Template('text:\"$x and even $y\"'),\n",
      "                     Template('text:\"$x or even $y\"'),\n",
      "                     Template('text:\"$x and almost $y\"'),\n",
      "                     Template('text:\"$x or almost $y\"'),\n",
      "                     Template('text:\"not only $x but $y\"'),\n",
      "                     Template('text:\"not just $x but $y\"'),\n",
      "                Template('text:\"not $x just $y\"'),\n",
      "                  Template('text:\"not $x but just $y\"'),\n",
      "                  Template('text:\"not $x still $y\"'),\n",
      "                  Template('text:\"not $x but still $y\"'),\n",
      "                  Template('text:\"not $x although still $y\"'),\n",
      "                  Template('text:\"not $x though still $y\"'),\n",
      "                  Template('text:\"$x or very $y\"'),\n",
      "  Template('text:\"$y very $x\"'),\n",
      "                  Template('text:\"not $y but $x enough\"'),\n",
      "                  Template('text:\"$y unbelievably $x\"'),\n",
      "                  Template('text:\"$y not even $x\"'),\n",
      "  Template('text:\"$x even $y\"'),\n",
      "                     Template('text:\"$x no $y\"'),\n",
      "                     Template('text:\"$x perhaps $y\"'),\n",
      "                     Template('text:\"$x sometimes $y\"'),\n",
      "                    Template('text:\"extremely $x $y\"'),\n",
      "                    Template('text:\"are very $x $y\"'),\n",
      "                    Template('text:\"is very $x $y\"')]\n",
      "\n",
      "pair_counts = defaultdict(int)\n",
      " \n",
      "with open('vanMiltenburg_pairs.csv') as f:\n",
      "    for line in f:\n",
      "        pairs = sorted(line.strip().split(','))\n",
      "\tif pairs[0] in space.vocab and pairs[1] in space.vocab:\n",
      "\t\tfor pat in patterns:\n",
      "\t\t\tpat1 = pat.substitute({'x': pairs[0], 'y': pairs[1]})\n",
      "        \t        pat2 = pat.substitute({'x': pairs[1], 'y': pairs[0]})\n",
      "                \tpair_counts[','.join(pairs)] +=  indexer.search(pat1).count\n",
      "\t                pair_counts[','.join(pairs)] +=  indexer.search(pat2).count\n",
      "\n",
      "pc = pd.Series(pair_counts)\n",
      "pc = pc.sort_values(ascending=False)\n",
      "print len(pc)\n",
      "batch= 348\n",
      "group1 = pc.ix[0:batch]\n",
      "group2 = pc.ix[batch:batch*2]\n",
      "group3 = pc.ix[batch*2:batch*3]\n",
      "group4 = pc.ix[batch*3:batch*4]\n",
      "group5= pc.ix[batch*4:batch*5]\n",
      "group6 = pc.ix[batch*5:]\n",
      "\n",
      "combo = zip(group1.index.tolist(), group2.sort_values().index.tolist(), group3.index.tolist(), group4.index.tolist(),group5.sort_values().index.tolist())\n",
      "combo = map(list,combo)\n",
      "combo[0].append(group6.index[0])\n",
      "combo[1].append(group6.index[1])\n",
      "\n",
      "pd.DataFrame(map(lambda x: map(lambda y: y.replace(',','-') ,x), combo)).to_csv('vanMiltenberg_ordered_groups.csv',header=None,index=None)\n",
      "import os\n",
      "from docopt import docopt\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import copy\n",
      "from collections import defaultdict\n",
      "from sklearn.preprocessing import normalize\n",
      "from sklearn.cluster import KMeans\n",
      "# from graph_tool.all import *\n",
      "import networkx as nx\n",
      "import csv\n",
      "\n",
      "import matplotlib\n",
      "matplotlib.use('Agg')\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import gzip\n",
      "# import StringIO\n",
      "import lucene\n",
      "from random import random, sample\n",
      "lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n",
      "from lucene import VERSION\n",
      "from org.apache.lucene import analysis, util\n",
      "from lupyne import engine\n",
      "from cStringIO import StringIO\n",
      "from itertools import izip\n",
      "from string import Template\n",
      "from collections import defaultdict\n",
      "\n",
      "def tokenizer(reader):\n",
      "    return analysis.standard.StandardTokenizer(util.Version.LATEST, reader)\n",
      "myAnalyzer = engine.indexers.Analyzer(tokenizer,\n",
      "     analysis.standard.StandardFilter, analysis.LowerCaseFilter)\n",
      "\n",
      "indexer = engine.IndexSearcher(\n",
      "      '/home/bryan/ukwakSentenceIndex', analyzer=myAnalyzer)\n",
      "\n",
      "\n",
      "patterns = [Template('text:\"$x but not $y\"'),\n",
      "                     Template('text:\"$x if not $y\"'),\n",
      "                     Template('text:\"$x although not $y\"'),\n",
      "                     Template('text:\"$x though not $y\"'),\n",
      "                     Template('text:\"$x and even $y\"'),\n",
      "                     Template('text:\"$x or even $y\"'),\n",
      "                     Template('text:\"$x and almost $y\"'),\n",
      "                     Template('text:\"$x or almost $y\"'),\n",
      "                     Template('text:\"not only $x but $y\"'),\n",
      "                     Template('text:\"not just $x but $y\"'),\n",
      "                Template('text:\"not $x just $y\"'),\n",
      "                  Template('text:\"not $x but just $y\"'),\n",
      "                  Template('text:\"not $x still $y\"'),\n",
      "                  Template('text:\"not $x but still $y\"'),\n",
      "                  Template('text:\"not $x although still $y\"'),\n",
      "                  Template('text:\"not $x though still $y\"'),\n",
      "                  Template('text:\"$x or very $y\"'),\n",
      "  Template('text:\"$y very $x\"'),\n",
      "                  Template('text:\"not $y but $x enough\"'),\n",
      "                  Template('text:\"$y unbelievably $x\"'),\n",
      "                  Template('text:\"$y not even $x\"'),\n",
      "  Template('text:\"$x even $y\"'),\n",
      "                     Template('text:\"$x no $y\"'),\n",
      "                     Template('text:\"$x perhaps $y\"'),\n",
      "                     Template('text:\"$x sometimes $y\"'),\n",
      "                    Template('text:\"extremely $x $y\"'),\n",
      "                    Template('text:\"are very $x $y\"'),\n",
      "                    Template('text:\"is very $x $y\"')]\n",
      "\n",
      "pair_counts = defaultdict(int)\n",
      "with open('wordnet_seeds.txt') as f:\n",
      "    for line in f:\n",
      "        pairs = sorted(line.strip().split(','))\n",
      "\tfor pat in patterns:\n",
      "\t\tpat1 = pat.substitute({'x': pairs[0], 'y': pairs[1]})\n",
      "\t\tpat2 = pat.substitute({'x': pairs[1], 'y': pairs[0]})\n",
      "\t\tpair_counts[','.join(pairs)] +=  indexer.search(pat1).count\n",
      "\t\tpair_counts[','.join(pairs)] +=  indexer.search(pat2).count\n",
      "\n",
      "pc = pd.Series(dict(pair_counts))\n",
      "#print pc.sort_values()\n",
      "#pc = pc[pc > 0]\n",
      "pc = pc.sort_values(ascending=False)\n",
      "group_size = 230\n",
      "group1 = pc.ix[0:group_size]\n",
      "group2 = pc.ix[group_size:group_size*2]\n",
      "group3 = pc.ix[group_size*2:group_size*3]\n",
      "group4 = pc.ix[group_size*3:group_size*4]\n",
      "group5 = pc.ix[group_size*4:group_size*5]\n",
      "group6 = pc.ix[230*5:]\n",
      "\n",
      "combo = zip(group1.index.tolist(), group2.sort_values().index.tolist(), group3.index.tolist(), group4.index.tolist(),group5.sort_values().index.tolist())\n",
      "combo = map(list,combo)\n",
      "combo[0].append(group6.index[0])\n",
      "combo[1].append(group6.index[1])\n",
      "#combo[2].append(group6.index[2])\n",
      "#combo[3].append(group6.index[3])\n",
      "\n",
      "pd.DataFrame(map(lambda x: map(lambda y: y.replace(',','/') ,x), combo)).to_csv('wordnet_ordered_groups.csv',header=None,index=None)\n",
      "import sys\n",
      "from scipy.stats import pearsonr\n",
      "import pandas\n",
      "import numpy as np\n",
      "import random\n",
      "import numpy\n",
      "import rpy2.robjects.packages as rpackages\n",
      "import rpy2.robjects.numpy2ri as np2ri\n",
      "import rpy2.robjects.pandas2ri as pd2ri\n",
      "import rpy2.robjects as ro\n",
      "np2ri.activate()\n",
      "pd2ri.activate()\n",
      "psych = rpackages.importr('psych')\n",
      "base = rpackages.importr('base')\n",
      "matrix_r = rpackages.importr('Matrix')\n",
      "from docopt import docopt\n",
      "import re\n",
      "\n",
      "# Get the arguments\n",
      "args = docopt(\"\"\"Aggregates lexical subsitution scores from sentences to form a scale membership list. The statistics are printed to std out.\n",
      "\n",
      "Usage:\n",
      "    data_creation.py <input> <output>\n",
      "\n",
      "Arguments:\n",
      "    <input> = File that holds subsitution suggestions, one sentence per line, comma sepearted\n",
      "    <output> = Where to write words that were selected for later inspection\n",
      "\"\"\")\n",
      "\n",
      "output = args['<output>']\n",
      "input_file = args['<input>']\n",
      "# Convert the resposnes of each informant into a binary\n",
      "# response matrix, where a 1 in cell [i,j] indicates\n",
      "# informant i listed word j.\n",
      "def buildMatrix(responses):\n",
      "    matrixDict = {}\n",
      "    for i,l in responses.iteritems():\n",
      "        row = {}\n",
      "        for pos,j in enumerate(l):\n",
      "            row[j] = 1\n",
      "        length = len(row)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        matrixDict[i] = row\n",
      "    matrix =  pandas.DataFrame(matrixDict)\n",
      "    return matrix.fillna(0).T\n",
      "\n",
      "# Calculate the informant by informant correlation matrix and\n",
      "# perform factor anaylsis to estimate the competencies of the\n",
      "# informants\n",
      "def CCT(matrix):\n",
      "    ro.r('''\n",
      "        cca_wrapper <- function(mat) {\n",
      "       \t\tpsych::fa(Matrix::nearPD(as.matrix(mat))$mat,fm='pa')\n",
      "        }\n",
      "        # call the function `f` with argument value 3\n",
      "       \n",
      "        ''')\n",
      "\n",
      "    agreementMatrix = matrix.T.cov()\n",
      "#    factors = psych.fa(matrix_r.nearPD(base.unname(base.as_matrix(agreementMatrix))).rx('mat')[0],fm='pa')\n",
      "    try:\n",
      "    \tfactors = ro.r['cca_wrapper'](agreementMatrix)\n",
      "    except:\n",
      "\treturn None\n",
      "    return factors\n",
      "\n",
      "# Function to return G_k as defined in Batchelder, William H., and A. Kimball Romney. 1988.\n",
      "# \"Test Theory without an Answer Key.\"\n",
      "# The parameters are the competency vector D, the resposne vector X,\n",
      "# and a bias variable g, which is cacluated per scale\n",
      "def estimator(D,X,g):\n",
      "    a = 1 - D\n",
      "    ratio1 = ((D + g * a) * ( 1 - g * a))/(a ** 2 * g * (1 - g))\n",
      "    ratio2 = (1 - g * a)/(a * (1 - g))\n",
      "    return X * numpy.log(ratio1) - numpy.log(ratio2)\n",
      "\n",
      "def p(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(pred)\n",
      "\n",
      "def r(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(true)\n",
      "\n",
      "\n",
      "data = pandas.read_csv(input_file,header=None,delim_whitespace = True)\n",
      "golds = []\n",
      "with open('data/scales.test.txt') as f:\n",
      "    for line in f:\n",
      "        golds.append(map(str.strip,line.split(\",\")))\n",
      "with open(output,'w') as f:\n",
      "    for index, group in data.groupby(0):\n",
      "\tgold = None\n",
      "        for g in golds:\n",
      "            if index.split(\"-\")[0].split(\"/\")[0] in g:\n",
      "\t\tscale_members = g\n",
      "\t\tgold = list(set(g) - set(map(lambda x: x.split(\"/\")[0] ,index.split(\"-\"))))\n",
      "\n",
      "        matrix = buildMatrix(group[1].str.split(','))\n",
      "\n",
      "        factors = CCT(matrix)\n",
      "\tif factors:\n",
      "\t        competencies = np2ri.ri2py((factors.rx('loadings')[0])).flatten()\n",
      "\t\teigen = np2ri.ri2py((factors.rx('values')[0])).flatten()\n",
      "\t\n",
      "        \teigenRatios = eigen[0]/eigen[1]\n",
      "\n",
      "\n",
      "        \tavgResponse =  matrix[matrix==1].count(1).mean()\n",
      "\t        length = matrix.shape[1]\n",
      "        \tcorrect = {}\n",
      "\n",
      "\t        for col in matrix:\n",
      "        \t        #Exclude responses for which the informant was given a word as a prompt word\n",
      "                \tcorrect[col] =(estimator(competencies,matrix[col],avgResponse/length)).sum()\n",
      "\n",
      "\n",
      "\t        x = pandas.Series(correct).sort_values()\n",
      "        \tpred =  x[x > 0]\n",
      "\t\tif gold:\n",
      "\t\t        print p(pred.index.tolist(), gold), r(pred.index.tolist(), gold), scale_members, input_file\n",
      "\t\t#else:\n",
      "\t\t#\tprint index, eigenRatios\n",
      "\t        f.write(index)\n",
      "        \tf.write('\\n')\n",
      "\t        f.write(pred.to_string())\n",
      "        \tf.write('\\n\\n')\n",
      "\telse:                                                          \n",
      "                print -100, -100, scale_members, input_file   \n",
      "                f.write(index)                                \n",
      "                f.write('\\n')                                 \n",
      "                f.write(\"ERROR ENCOUNTERED\")                  \n",
      "                f.write('\\n\\n')      \n",
      "import sys\n",
      "from scipy.stats import pearsonr\n",
      "import pandas\n",
      "import numpy as np\n",
      "import random\n",
      "import numpy\n",
      "import rpy2.robjects.packages as rpackages\n",
      "import rpy2.robjects.numpy2ri as np2ri\n",
      "import rpy2.robjects.pandas2ri as pd2ri\n",
      "import rpy2.robjects as ro\n",
      "np2ri.activate()\n",
      "pd2ri.activate()\n",
      "psych = rpackages.importr('psych')\n",
      "base = rpackages.importr('base')\n",
      "matrix_r = rpackages.importr('Matrix')\n",
      "from docopt import docopt\n",
      "import re\n",
      "\n",
      "# Get the arguments\n",
      "args = docopt(\"\"\"Aggregates lexical subsitution scores from sentences to form a scale membership list. The statistics are printed to std out.\n",
      "\n",
      "Usage:\n",
      "    data_creation.py <input> <output>\n",
      "\n",
      "Arguments:\n",
      "    <input> = File that holds subsitution suggestions, one sentence per line, comma sepearted\n",
      "    <output> = Where to write words that were selected for later inspection\n",
      "\"\"\")\n",
      "\n",
      "output = args['<output>']\n",
      "input_file = args['<input>']\n",
      "# Convert the resposnes of each informant into a binary\n",
      "# response matrix, where a 1 in cell [i,j] indicates\n",
      "# informant i listed word j.\n",
      "def buildMatrix(responses):\n",
      "    matrixDict = {}\n",
      "    for i,l in responses.iteritems():\n",
      "        row = {}\n",
      "        for pos,j in enumerate(l):\n",
      "            row[j] = 1\n",
      "        length = len(row)\n",
      "        matrixDict[i] = row\n",
      "    matrix =  pandas.DataFrame(matrixDict)\n",
      "    return matrix.fillna(0).T\n",
      "\n",
      "# Calculate the informant by informant correlation matrix and\n",
      "# perform factor anaylsis to estimate the competencies of the\n",
      "# informants\n",
      "def CCT(matrix):\n",
      "    ro.r('''\n",
      "        cca_wrapper <- function(mat) {\n",
      "       \t\tpsych::fa(Matrix::nearPD(as.matrix(mat))$mat,fm='pa')\n",
      "        }\n",
      "        # call the function `f` with argument value 3\n",
      "       \n",
      "        ''')\n",
      "\n",
      "    agreementMatrix = matrix.T.cov()\n",
      "#    factors = psych.fa(matrix_r.nearPD(base.unname(base.as_matrix(agreementMatrix))).rx('mat')[0],fm='pa')\n",
      "    try:\n",
      "        factors = ro.r['cca_wrapper'](agreementMatrix)\n",
      "    except:\n",
      "\treturn None\n",
      "    return factors\n",
      "\n",
      "# Function to return G_k as defined in Batchelder, William H., and A. Kimball Romney. 1988.\n",
      "# \"Test Theory without an Answer Key.\"\n",
      "# The parameters are the competency vector D, the resposne vector X,\n",
      "# and a bias variable g, which is cacluated per scale\n",
      "def estimator(D,X,g):\n",
      "    a = 1 - D\n",
      "    ratio1 = ((D + g * a) * ( 1 - g * a))/(a ** 2 * g * (1 - g))\n",
      "    ratio2 = (1 - g * a)/(a * (1 - g))\n",
      "    return X * numpy.log(ratio1) - numpy.log(ratio2)\n",
      "\n",
      "def p(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(pred)\n",
      "\n",
      "def r(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(true)\n",
      "\n",
      "\n",
      "data = pandas.read_csv(input_file,header=None,delim_whitespace = True)\n",
      "golds = []\n",
      "with open('data/scales.develop.txt') as f:\n",
      "    for line in f:\n",
      "        golds.append(map(str.strip,line.split(\",\")))\n",
      "with open(output,'w') as f:\n",
      "    for index, group in data.groupby(0):\n",
      "\tgold = None\n",
      "\tscale_members = \"\"\n",
      "        for g in golds:\n",
      "            if index.split(\"-\")[0].split(\"/\")[0] in g:\n",
      "\t#\tprint map(lambda x: x.split(\"/\")[0] ,index.split(\"-\"))\n",
      "#\t\tprint g\n",
      "\t\tscale_members = g\n",
      "                gold = list(set(g) - set(map(lambda x: x.split(\"/\")[0] ,index.split(\"-\"))))\n",
      "#\t\tprint gold\n",
      "\n",
      "        matrix = buildMatrix(group[1].str.split(','))\n",
      "\n",
      "        factors = CCT(matrix)\n",
      "\n",
      "\tif factors:\n",
      "\t        competencies = np2ri.ri2py((factors.rx('loadings')[0])).flatten()\n",
      "\t\teigen = np2ri.ri2py((factors.rx('values')[0])).flatten()\n",
      "\t\n",
      "        \teigenRatios = eigen[0]/eigen[1]\n",
      "\n",
      "\n",
      "\t        avgResponse =  matrix[matrix==1].count(1).mean()\n",
      "        \tlength = matrix.shape[1]\n",
      "\t        correct = {}\n",
      "\t        for col in matrix:\n",
      "\t                #Exclude responses for which the informant was given a word as a prompt word\n",
      "\t                correct[col] =(estimator(competencies,matrix[col],avgResponse/length)).sum()\n",
      "\t\n",
      "\n",
      "        \tx = pandas.Series(correct).sort_values()\n",
      "\t        pred =  x[x > 0]\n",
      "#\t\tif gold:\n",
      "\t\tif False:\n",
      "\t\t        print p(pred.index.tolist(), gold), r(pred.index.tolist(), gold), scale_members, input_file\n",
      "\t\telse:\n",
      "\t\t\tprint index, eigenRatios\n",
      "\t        f.write(index)\n",
      "\t        f.write('\\n')\n",
      "\t        f.write(pred.to_string())\n",
      "        \tf.write('\\n\\n')\n",
      "\telse:\n",
      "\t\tprint -100, -100, scale_members, input_file\n",
      "\t\tf.write(index)\n",
      "                f.write('\\n')\n",
      "                f.write(\"ERROR ENCOUNTERED\")\n",
      "                f.write('\\n\\n')\n",
      "from nltk import wordnet as wn\n",
      "import utdeftvs\n",
      "\n",
      "\n",
      "space = utdeftvs.load_numpy(\"data/lexsub_embeddings_withcontexts.npz\", True)\n",
      "\n",
      "wordnet_seeds = set()\n",
      "for synset in wn.wordnet.all_synsets(pos=wn.wordnet.ADJ):\n",
      "    for lemma in synset.lemmas():\n",
      "        if lemma.antonyms():\n",
      "            pair = [lemma.name(), lemma.antonyms()[0].name()]\n",
      "            pair = sorted(pair)\n",
      "\t    if pair[0] in space.vocab and pair[1] in space.vocab:\n",
      "\t            wordnet_seeds.add(','.join(pair))\n",
      "\n",
      "with open('wordnet_seeds.txt','w') as f:\n",
      "    for seed in wordnet_seeds:\n",
      "        f.write(seed)\n",
      "        f.write('\\n')\n",
      "import pandas as pd\n",
      "wn = pd.read_csv('wn.nopart.ensemble_filtered_scales.csv',index_col=0)\n",
      "\n",
      "wn['scale'] = wn.apply(lambda x: ','.join(x['0_x'].split(',') + [x['word1']] + [x['word2']]),axis=1)\n",
      "wn = wn.sample(frac=1)\n",
      "\n",
      "wn.drop(wn[wn['0_x'] == \"Series([],\"].index.tolist(),inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mturk = []\n",
      "for i in range(67):\n",
      "    subset = wn.iloc[10*i:10*(i+1)]\n",
      "    words =  subset['scale'].values.tolist()\n",
      "    idxs =  subset.index.tolist()\n",
      "    row = {}\n",
      "    for i, word in enumerate(zip(words,idxs)):\n",
      "        row['idx_%d' % i] = word[1]\n",
      "        row['words_%d' % i] = word[0]\n",
      "    mturk.append(row)\n",
      "\n",
      "pd.DataFrame(mturk).to_csv(\"wn.membership_for_mturk.csv\",index=None)\n",
      "import pandas as pd\n",
      "wn = pd.read_csv('vm.nopart.ensemble_filtered_scales.csv',index_col=0)\n",
      "\n",
      "wn['scale'] = wn.apply(lambda x: ','.join(x['0_x'].split(',') + [x['word1']] + [x['word2']]),axis=1)\n",
      "wn = wn.sample(frac=1)\n",
      "\n",
      "wn.drop(wn[wn['0_x'] == \"Series([],\"].index.tolist(),inplace=True)\n",
      "mturk = []\n",
      "for i in range(22):\n",
      "    subset = wn.iloc[10*i:10*(i+1)]\n",
      "    words =  subset['scale'].values.tolist()\n",
      "    idxs =  subset.index.tolist()\n",
      "    row = {}\n",
      "    for i, word in enumerate(zip(words,idxs)):\n",
      "        row['idx_%d' % i] = word[1]\n",
      "        row['words_%d' % i] = word[0]\n",
      "    mturk.append(row)\n",
      "\n",
      "pd.DataFrame(mturk).to_csv(\"vm.membership_for_mturk.csv\",index=None)\n",
      "##   Parameters\n",
      "## output location\n",
      "## two word search or one\n",
      "## extra data or not?\n",
      "import os\n",
      "from docopt import docopt\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import codecs \n",
      "import gzip\n",
      "#import StringIO\n",
      "import lucene\n",
      "from random import random, sample\n",
      "lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n",
      "from lucene import VERSION\n",
      "from org.apache.lucene import analysis,util\n",
      "from lupyne import engine\n",
      "from StringIO import StringIO\n",
      "from itertools import izip\n",
      "import sys\n",
      "\n",
      "\n",
      "#From https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python\n",
      "import unicodedata\n",
      "def remove_control_characters(s):\n",
      "    return \"\".join(ch for ch in s if unicodedata.category(ch)[0]!=\"C\" and ch != '\"')\n",
      "\n",
      "def tokenizer(reader):\n",
      "    return analysis.standard.StandardTokenizer(util.Version.LATEST, reader)\n",
      "\n",
      "\n",
      "# In[21]:\n",
      "\n",
      "from spacy.en import English\n",
      "nlp = English(parser=False,tagger=False,entity=False)\n",
      "\n",
      "\n",
      "# In[2]:\n",
      "\n",
      "from string import Template\n",
      "\n",
      "\n",
      "\n",
      "def make_generic(text):\n",
      "\tsent =  text[6:-1]\n",
      "        if sent.startswith('are'):\n",
      "        \tsentence = u\"They %s.\" % sent\n",
      "        elif sent.startswith('is'):\n",
      "                sentence = u\"It %s.\" %sent\n",
      "        else:\n",
      "\t\tsentence = u\"It was %s.\" % sent\n",
      "\n",
      "\treturn sentence\n",
      "\n",
      "def main():\n",
      "\n",
      "  args = docopt(\"\"\"Collects sentences and creates gold standard data from common pairs of scalar adjectives\n",
      "  \n",
      "  Usage:\n",
      "      data_creation.py [options] <output_dir>\n",
      "  \n",
      "  Arguments:\n",
      "      <output_dir> = Directory to output sentences to, in lexical subsitution evaluation format\n",
      "  \n",
      "  Options:\n",
      "      --oneword  Only use the second word in searching for the sentences and in testing the data\n",
      "      --expand  Replace pairs with other pairs in sentences to create more data\n",
      "      --generic  Don't search the corpus, just use the patterns as is, adding \"it was\" to them to make a complete sentence.\n",
      "      --allpairs <pairs_list>  Generate data for all pairs of adjectives, not just the common ones.\n",
      "      --patterns <pattern_group> Generate data using 'all' patterns, 'symmetric' patterns, or 'adjscale' patterns [default: all]\n",
      "      --sep <character> The character to split the pairs on when using allpairs [default: -]\n",
      "  \"\"\")\n",
      "  \n",
      "  output_dir = args['<output_dir>']\n",
      "  only_one = args['--oneword']\n",
      "  expand = args['--expand']\n",
      "  generic = args['--generic']\n",
      "  allpairs = args['--allpairs']\n",
      "  patterns = args['--patterns']\n",
      "  sep = args['--sep']\n",
      "\n",
      "  myAnalyzer = engine.indexers.Analyzer( tokenizer,\n",
      "     analysis.standard.StandardFilter, analysis.LowerCaseFilter)\n",
      "\n",
      "\n",
      "  indexer = engine.IndexSearcher('/home/bryan/ukwakSentenceIndex',analyzer=myAnalyzer)\n",
      "  shelf = shelve.open('ukwakShelf')\n",
      "  #string_shelf = shelve.open('string_shelf')\n",
      "  def getText(i):\n",
      "    env = lucene.getVMEnv()\n",
      "    env.attachCurrentThread()\n",
      "    d = indexer.doc(i.doc)\n",
      "    return d['text']\n",
      "    # Get the arguments\n",
      "    \n",
      "  #indexer.set('name', stored=True)\n",
      "  #indexer.set('text', stored=False)\n",
      "  # In[3]:\n",
      "  \n",
      "  adjScalePatterns = [Template('text:\"$x but not $y\"'),\n",
      "                     Template('text:\"$x if not $y\"'),\n",
      "                     Template('text:\"$x although not $y\"'),\n",
      "                     Template('text:\"$x though not $y\"'),\n",
      "                     Template('text:\"$x and even $y\"'),\n",
      "                     Template('text:\"$x or even $y\"'),\n",
      "                     Template('text:\"$x and almost $y\"'),\n",
      "                     Template('text:\"$x or almost $y\"'),\n",
      "                     Template('text:\"not only $x but $y\"'),\n",
      "                     Template('text:\"not just $x but $y\"'),\n",
      "  \t\tTemplate('text:\"not $x just $y\"'),\n",
      "                  Template('text:\"not $x but just $y\"'),\n",
      "                  Template('text:\"not $x still $y\"'),\n",
      "                  Template('text:\"not $x but still $y\"'),\n",
      "                  Template('text:\"not $x although still $y\"'),\n",
      "                  Template('text:\"not $x though still $y\"'),\n",
      "                  Template('text:\"$x or very $y\"'),\n",
      "  Template('text:\"$y very $x\"'),\n",
      "                  Template('text:\"not $y but $x enough\"'),\n",
      "                  Template('text:\"$y unbelievably $x\"'),\n",
      "                  Template('text:\"$y not even $x\"'),\n",
      "  Template('text:\"$x even $y\"'),\n",
      "                     Template('text:\"$x no $y\"'),\n",
      "                     Template('text:\"$x perhaps $y\"'),\n",
      "                     Template('text:\"$x sometimes $y\"'),\n",
      "                    Template('text:\"extremely $x $y\"'),\n",
      "                    Template('text:\"are very $x $y\"'),\n",
      "                    Template('text:\"is very $x $y\"')]\n",
      "  symPatterns = [\n",
      "   Template('text:\"$x and $y\"'),\n",
      "   Template('text:\"$x or $y\"'),\n",
      "   Template('text:\"$x and the $y\"'),\n",
      "   Template('text:\"from $x to $y\"'),\n",
      "   Template('text:\"$x or the $y\"'),\n",
      "   Template('text:\"$x as well as $y\"'),\n",
      "   Template('text:\"$x or a $y\"'),\n",
      "   Template('text:\"$x rather than $y\"'),\n",
      "   Template('text:\"$x nor $y\"'),\n",
      "   Template('text:\"$x and one $y\"'),\n",
      "   Template('text:\"either $x or $y\"'),\n",
      "                  ]\n",
      "  \n",
      "  #allWords = [['hideous', 'ugly', 'pretty', 'beautiful', 'gorgeous'],\n",
      "  #              ['same', 'alike', 'similar', 'different'],\n",
      "  #              ['ancient', 'old', 'fresh', 'new'],\n",
      "#\t        ['blah','bleh'],\n",
      "#\t        ['lorem','ipsum']]  \n",
      "  # In[4]:\n",
      "  \n",
      "  #allWords = [['minuscule', 'tiny', 'small', 'big', 'large', 'huge', 'enormous', 'gigantic'],\n",
      "  #            ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      "  #            ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent'],\n",
      "  #            ['horrible', 'terrible', 'awful', 'bad', 'good', 'great', 'wonderful', 'awesome'],\n",
      "  #            ['slow', 'quick', 'fast', 'speedy'],\n",
      "  #            ['simple', 'easy', 'hard', 'difficult'],\n",
      "  #            ['few', 'some', 'several', 'many'],\n",
      "  #            ['dark', 'dim', 'light', 'bright'],\n",
      "  #            ['freezing', 'cold', 'warm', 'hot'],\n",
      "  #           ]\n",
      "  allWords = [['hideous', 'ugly', 'pretty', 'beautiful', 'gorgeous'],\n",
      "                ['same', 'alike', 'similar', 'different'],\n",
      "                ['ancient', 'old', 'fresh', 'new'],\n",
      "\t\t['random', 'words', 'here', 'for'],\n",
      "\t\t['padding',' the', 'additional', 'words'],\n",
      "\t\t['another','random','list'],\n",
      "\t\t['im','not','sure','how'],\n",
      "\t\t['many','i','need'],\n",
      "\t\t['i','should','have','made'],\n",
      "\t\t['this','better']\n",
      "\t\t]\n",
      "  \n",
      "  # In[13]:\n",
      "\n",
      "  from itertools import combinations, product\n",
      "  \n",
      "  \n",
      "  # In[50]:\n",
      "  \n",
      "  import pandas\n",
      "  flatList = pandas.read_csv(\"commonAdjs.txt\",header=None,squeeze=True).str.strip().tolist()\n",
      "  \n",
      "  \n",
      "  # In[56]:\n",
      "  if not allpairs:\n",
      "      commonPairsSet = [[(\"small\",\"large\"),\n",
      "                     (\"moist\",\"wet\"),\n",
      "                     (\"smart\",\"intelligent\"),\n",
      "                     (\"good\",\"great\"),\n",
      "                    (\"slow\",\"fast\"),\n",
      "                    (\"simple\",\"easy\"),\n",
      "                    (\"some\",\"many\"),\n",
      "                    (\"light\",\"bright\"),\n",
      "                    (\"warm\",\"hot\")]]\n",
      "  else:\n",
      "#      print allpairs\n",
      "      pairs = pd.read_csv(allpairs,header=None)\n",
      "      pairs = pairs.fillna(\"-\")\n",
      " #     print pairs\n",
      "      pairsSplit = pairs.applymap(lambda x: x.split(sep))\n",
      "      commonPairsSet = pairsSplit.values.tolist()\n",
      "#      print commonPairsSet\n",
      "      output_dir_base = output_dir\n",
      "  \n",
      "  \n",
      "  \n",
      "  ident = 0\n",
      "  file_id = 0\n",
      "\n",
      "  pool = mp.ThreadingPool(nodes=8)\n",
      "    \n",
      "  for commonPairs in commonPairsSet:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      if allpairs:\n",
      "          output_dir = output_dir_base + \"_\" + str(file_id)\n",
      "          print output_dir\n",
      "          os.mkdir(output_dir)\n",
      "  \n",
      "      candidates = codecs.open(output_dir + '/candidates','w','utf-8')\n",
      "      sentFile = codecs.open(output_dir + '/sentences','w','utf-8')\n",
      "      gold = codecs.open(output_dir + '/gold','w','utf-8')\n",
      "  \n",
      "      i = 0\n",
      "      ident = 0\n",
      "\n",
      "      pair_sentences = {}\n",
      "      commonPairs =  filter(lambda x: len(x) == 2 and x[0] and x[1], commonPairs)\n",
      "      pairs_length = len(commonPairs)\n",
      "      \n",
      "      for pair in commonPairs:\n",
      "          a1 = pair[0]\n",
      "          a2 = pair[1]\n",
      "          for scale in allWords:\n",
      "              if a1 in scale:\n",
      "                  goldScale = [\"other\",\"gold\"]\n",
      "\t      else:\n",
      "\t\t  goldScale = [\"other\",\"gold\"]\n",
      "          if only_one:\n",
      "              a1 = \"*\"\n",
      "              # strongWeakPatterns = strongWeakPatterns[:-3]\n",
      "          sentences = set()\n",
      "          if patterns == 'both' or patterns == 'adjscale':\n",
      "  \t        for pattern in adjScalePatterns:\n",
      "          \t    pat1 = pattern.substitute({'x': a1, 'y': a2})\n",
      "  \t            reg1 = re.compile(pattern.substitute({'x': r'(\\w+)' , 'y': a2})[6:-1])\n",
      "  \n",
      "          \t    pat2 = pattern.substitute({'x': a2, 'y': a1})\n",
      "  \t            reg2 = re.compile(pattern.substitute({'x':a2 , 'y': r'(\\w+)'})[6:-1])\n",
      "          \t    if not generic:\n",
      "                        if pat1 in shelf:\n",
      "                            sentences.update(shelf[pat1])\n",
      "\t\t\telse:\n",
      "                    \t\thits = pool.map(getText,list(indexer.search(pat1 ,scores=False).scoredocs))\n",
      "                       \t\tif only_one:\n",
      "                            \t\tresults = []\n",
      "                            \t\tfor hit in hits:\n",
      "                            \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "                            \t\t#Also filter our sentences with out adjectives in wild card to try an reduce number of sentences\n",
      "                                \t\tif reg1.search(hit) is not None and reg1.search(hit).group(1) in flatList:\n",
      "\t\t\t\t\t\t\ttokenized = tuple([t.orth_ for t in nlp(hit)])\n",
      "                                                        sentences.add(tokenized)\n",
      "                                                        results.append(tokenized)\n",
      "                                    \t\t\t#sentences.add(hit)\n",
      "                                    \t\t\t#results.append(hit)\n",
      "                            \t\tshelf[pat1] = results\n",
      "  \t               \t\telse:\n",
      "\t\t\t\t\ttokenized = map(lambda hit: tuple([t.orth_ for t in nlp(hit)]),hits)\n",
      "                                        sentences.update(tokenized)\n",
      "                            \t\t#sentences.update(hits)\n",
      "                            \t\tshelf[pat1] = tokenized\n",
      "                    \n",
      "\n",
      "\t\t\tif pat2 in shelf:\n",
      "\t\t\t\tsentences.update(shelf[pat2])\n",
      "\t\t\telse:\n",
      "                    \t\thits = pool.map(getText,list(indexer.search(pat2 ,scores=False).scoredocs))\n",
      "\t\t\t\tif only_one:\n",
      "\t\t\t\t\tresults = []\n",
      "          \t        \t\tfor hit in hits:\n",
      "                  \t    \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "  \t                    \t\t\tif reg2.search(hit) is not None and reg2.search(hit).group(1) in flatList:\n",
      "\t\t\t\t\t\t\ttokenized = tuple([t.orth_ for t in nlp(hit)])\n",
      "                                                        sentences.add(tokenized)\n",
      "                                                        results.append(tokenized)\n",
      "\t\t\t\t\tshelf[pat2] = results\n",
      "                  \t    \telse:\n",
      "\t\t\t\t\ttokenized = map(lambda hit: tuple([t.orth_ for t in nlp(hit)]),hits)\n",
      "                                        sentences.update(tokenized)\n",
      "\t\t\t\t\t#sentences.update(hits)\n",
      "\t\t\t\t\tshelf[pat2] = tokenized\n",
      "                          \t\n",
      "  \t            else:\n",
      "          \t        sentences.add(make_generic(pat1))\n",
      "                  \tsentences.add(make_generic(pat2))\n",
      "\n",
      "          if patterns == 'both' or patterns == 'sym':\n",
      "  \t        for pattern in symPatterns:\n",
      "          \t    pat1 = pattern.substitute({'x': a1, 'y': a2})\n",
      "  \t            reg1 = re.compile(pattern.substitute({'x': r'(\\w+)' , 'y': a2})[6:-1])\n",
      "  \n",
      "  \t            pat2 = pattern.substitute({'x': a2, 'y': a1})\n",
      "  \t            reg2 = re.compile(pattern.substitute({'x':a2 , 'y': r'(\\w+)'})[6:-1])\n",
      "          \t    if not generic:\n",
      "\t\t\tif pat1 in shelf:\n",
      "                            sentences.update(shelf[pat1])\n",
      "                    \telse:\n",
      "\t\t\t\thits = pool.map(getText,list(indexer.search(pat1 ,scores=False).scoredocs))\n",
      "                  \t\tif only_one:\n",
      "\t\t\t\t\tresults = []\n",
      "\t\t\t\t\tfor hit in hits:\n",
      "  \t                    \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "\t\t\t\t\t\treg_results = reg1.search(hit)\n",
      "  \t                    \t\t\t#if reg1.search(hit) is not None and reg1.search(hit).group(1) in flatList:\n",
      "\t\t\t\t\t\tif reg_results is not None and reg_results.group(1) in flatList:\n",
      "\t\t\t\t\t\t\ttokenized = tuple([t.orth_ for t in nlp(hit)])\n",
      "          \t                \t\t\tsentences.add(tokenized)\n",
      "\t\t\t\t\t\t\tresults.append(tokenized)\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\tshelf[pat1] = results\n",
      "                  \t    \telse:\n",
      "\t\t\t\t\ttokenized = map(lambda hit: tuple([t.orth_ for t in nlp(hit)]),hits)\n",
      "                          \t\tsentences.update(tokenized)\n",
      "\t\t\t\t\tshelf[pat1] = tokenized\n",
      "  \t\t\t\n",
      "\t\t\tif pat2 in shelf:\n",
      "                        \tsentences.update(shelf[pat2])\n",
      "\t\t\telse:\n",
      "  \t                \thits = pool.map(getText,list(indexer.search(pat2 ,scores=False).scoredocs))\n",
      "\t\t\t\tif only_one:\n",
      "\t\t\t\t\tresults = []\n",
      "                  \t\t\tfor hit in hits:\n",
      "          \t            \t\t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "                  \t    \t\t\tif reg2.search(hit) is not None and reg2.search(hit).group(1) in flatList:\n",
      "\t\t\t\t\t\t\ttokenized = tuple([t.orth_ for t in nlp(hit)])\n",
      "                          \t\t\t\tsentences.add(tokenized)\n",
      "\t\t\t\t\t\t\tresults.append(tokenized)\n",
      "\t\t\t\t\tshelf[pat2] = results\n",
      "  \t                    \telse:\n",
      "\t\t\t\t\ttokenized = map(lambda hit: tuple([t.orth_ for t in nlp(hit)]),hits)\n",
      "          \t                \tsentences.update(tokenized)\n",
      "\t\t\t\t\t#print pat2\n",
      "\t\t\t\t\tshelf[pat2] = tokenized\n",
      "  \t            else:\n",
      "                  \tsentences.add(make_generic(pat1))\n",
      "          \t        sentences.add(make_generic(pat2))\n",
      " \t  \n",
      "\t  sentenceStrings = StringIO()\n",
      "\t  goldStrings = StringIO()\n",
      "          #for sentence in nlp.pipe(sentences, batch_size=10000, n_threads=8):\n",
      "\t  pair_sentences['-'.join(pair)] = sentences\n",
      "  \n",
      "      maxL = -10000\n",
      "      pair_sentences_lengths = {}\n",
      "      for pair in commonPairs:\n",
      "\tpair_sentences_lengths['-'.join(pair)] = len(pair_sentences['-'.join(pair)])\n",
      "\tif len(pair_sentences['-'.join(pair)]) > maxL:\n",
      "\t\tmaxL = len(pair_sentences['-'.join(pair)])\n",
      "\t        #print pair, maxL\n",
      "      for pair in commonPairs:\n",
      "\t  sentenceStrings = StringIO()\n",
      "          goldStrings = StringIO()\n",
      "\t  a1 = pair[0]\n",
      "          a2 = pair[1]\n",
      "          for scale in allWords:\n",
      "              if a1 in scale:\n",
      "                  goldScale = [\"other\",\"gold\"]\n",
      "              else:\n",
      "                  goldScale = [\"other\",\"gold\"]\n",
      "          if only_one:\n",
      "              a1 = \"*\"\n",
      "              # strongWeakPatterns = strongWeakPatterns[:-3]\n",
      "\n",
      "\t  for toks_tuple in pair_sentences['-'.join(pair)]:\n",
      "              try:\n",
      "                  #toks = [t.orth_ for t in sentence]\n",
      "                #Sometimes there is this weirdness where an empty token appears in the start of the list\n",
      "\t\ttoks = filter(lambda x: x, map(remove_control_characters,list(toks_tuple)))\n",
      "\t\tif a2 in toks and a1 in toks: \n",
      "\t\t  \n",
      "                  index_2 = toks.index(a2)\n",
      "\t\t  tokString = \" \".join(toks)\n",
      "\t\t  if 'pension' in tokString:\n",
      "\t\t\tprint toks\n",
      "                  if not only_one:\n",
      "                      index_1 = toks.index(a1)\n",
      "                      ident += 1\n",
      "                      sentenceStrings.write(u\"%s.j\\t%d\\t%d\\t%s\\n\" % (a1,ident,index_1,u\" \".join(toks)))\n",
      "#\t\t      print a1, toks\n",
      "#\t\t      print set(goldScale)\n",
      "#\t\t      print set([a1])\n",
      "#\t\t      print set(goldScale) - set([a1])\n",
      "                      goldStrings.write(u\"%s.j %d:: %s 1\\n\" % (a1,ident,u\" 1; \".join(set(goldScale) - set([a1]))))\n",
      "#\t\t      print index_1\n",
      "                  ident += 1\n",
      "                  sentenceStrings.write(u\"%s.j\\t%d\\t%d\\t%s\\n\" % (a2,ident,index_2,tokString))\n",
      "#\t\t  print a2\n",
      "                  goldStrings.write(u\"%s.j %d:: %s 1\\n\" % (a2,ident, u\" 1; \".join(set(goldScale) - set([a2]))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # \t          print index_2\n",
      "  \n",
      "                  #Use same sentence to generate sentences with others pairs\n",
      "                  if expand:\n",
      "                      for j in xrange(0,pairs_length):\n",
      "\t#\t\t  print commonPairs[j], pair_sentences_lengths['-'.join(commonPairs[j])]/float(maxL)\n",
      "                          if j != i and random() > pair_sentences_lengths['-'.join(commonPairs[j])]/float(maxL):\n",
      "\t\t\t      pair_sentences_lengths['-'.join(commonPairs[j])] += 1\n",
      "                              otherPair = commonPairs[j]\n",
      "                              b1 = otherPair[0]\n",
      "                              b2 = otherPair[1]\n",
      "  \n",
      "                              toks[index_2] = b2\n",
      "                              if not only_one:\n",
      "                                  toks[index_1] = b1\n",
      "                                  ident += 1\n",
      "                                  sentenceStrings.write(u\"%s.j\\t%d\\t%d\\t%s\\n\" % (b1,ident,index_1,u\" \".join(toks)))\n",
      "                                  goldStrings.write(u\"%s.j %d:: %s 1\\n\" % (b1,ident,\" 1; \".join(set(allWords[j]) - set([b1]))))\n",
      "                              ident += 1\n",
      "                              sentenceStrings.write(u\"%s.j\\t%d\\t%d\\t%s\\n\" % (b2,ident,index_2,u\" \".join(toks)))\n",
      "                              goldStrings.write(u\"%s.j %d:: %s 1\\n\" % (b2,ident,u\" 1; \".join(set(allWords[j]) - set([b2]))))\n",
      "  \n",
      "              except Exception as e:\n",
      "\t\t    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
      "  \t\t    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
      "\t\t    print(exc_type, fname, exc_tb.tb_lineno)\n",
      "\t\t    print e\n",
      "\t\t    pass\n",
      "  \n",
      "          candidates.write(\"%s.j::%s\\n\" % (a1,\";\".join(set(flatList) - set(pair))))\n",
      "          candidates.write(\"%s.j::%s\\n\" % (a2,\";\".join(set(flatList) - set(pair))))\n",
      "          i += 1\n",
      " \n",
      "      \t  sentFile.write(sentenceStrings.getvalue())\n",
      "          gold.write(goldStrings.getvalue())\n",
      "\n",
      "      candidates.close()\n",
      "      gold.close()\n",
      "      sentFile.close()\n",
      "      \n",
      "      file_id += 1\n",
      "     # print \"------------------\"\n",
      "\n",
      "  pool.close()\n",
      "  pool.terminate()\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  main()\n",
      "from docopt import docopt\n",
      "from collections import defaultdict\n",
      "from itertools import izip\n",
      "import os\n",
      "import shutil\n",
      "\n",
      "args = docopt(\"\"\"Splits lexical substitution datasets into smaller datasets consiting of only one part of speech each.\n",
      "\n",
      "Usage:\n",
      "    data_splitter.py <original_data_dir>\n",
      "\n",
      "Arguments:\n",
      "     <original_data_dir> = Directory containing lexical subsitution data to be split\n",
      "\"\"\")\n",
      "\n",
      "input_dir = args['<original_data_dir>']\n",
      "\n",
      "datasets_sentences = defaultdict(dict)\n",
      "datasets_gold = defaultdict(dict)\n",
      "\n",
      "with open(input_dir + \"/sentences\") as f:\n",
      "    with open(input_dir + \"/gold\") as gold_f:\n",
      "        for sentence, gold in izip(f,gold_f):\n",
      "            sentence_parts = sentence.split('\\t')\n",
      "            gold_parts = gold.split(' ')\n",
      "            pos =  sentence_parts[0][-1]\n",
      "            datasets_sentences[pos][sentence_parts[1]] = sentence\n",
      "            datasets_gold[pos][gold_parts[1]] = gold\n",
      "\n",
      "for key in datasets_sentences:\n",
      "    out_dir = input_dir.rstrip('/') + \"_\" + key\n",
      "    os.mkdir(out_dir)\n",
      "    with open(out_dir + \"/sentences\", 'w') as f:\n",
      "        with open(out_dir + \"/gold\", 'w') as gold_f:\n",
      "            for sentence in datasets_sentences[key]:\n",
      "                if sentence in datasets_gold[key]:\n",
      "                    f.write(datasets_sentences[key][sentence])\n",
      "                    gold_f.write(datasets_gold[key][sentence])\n",
      "\n",
      "    shutil.copyfile(input_dir + \"/candidates\", out_dir + \"/candidates\")\n",
      "    print key, len(datasets_sentences[key]), datasets_sentences[key][datasets_sentences[key].keys()[0]]\n",
      "from glob import iglob\n",
      "import sys\n",
      "l = []\n",
      "\n",
      "\n",
      "for location in iglob(sys.argv[1] + \"_*/orensm.out\"):\n",
      "\tif int(location.split('_')[1].split('/')[0]) < 500:\n",
      "\t\twith open(location) as f:\n",
      "\t\t    header = \"\"\n",
      "\t\t    cur = []\n",
      "\t\t    for line in f:\n",
      "\t        \tif \"-\" in line and \"/JJ\" in line:\n",
      "\t\t\t    if header:# and header != \"slow/JJ-fast/JJ\" and header != \"small/JJ-large/JJ\" and \\\n",
      "#\t\t\t       header != \"easy/JJ-hard/JJ\" and header != \"cold/JJ-warm/JJ\":\n",
      "\t        \t\t    l.append([header,\",\".join(cur)])\n",
      "\t\t            header = line.strip()\n",
      "        \t\t    cur = []\n",
      "\t\t        else:\n",
      "\t\t            word = line.strip().split(\" \")[0]\n",
      "        \t\t    if word:\n",
      "\t\t               cur.append(word)\n",
      "#\t    if header != \"slow/JJ-fast/JJ\" and header != \"small/JJ-large/JJ\" and \\\n",
      "#\t    header != \"easy/JJ-hard/JJ\" and header != \"cold/JJ-warm/JJ\":\n",
      "\t\t    l.append([header,\",\".join(cur)])\n",
      "import pandas as pd\n",
      "scales = pd.DataFrame(l).reset_index()\n",
      "scales[\"word1\"] = scales[0].str.replace(\"/JJ\",\"\").str.split(\"-\",expand=True)[0]\n",
      "scales[\"word2\"] = scales[0].str.replace(\"/JJ\",\"\").str.split(\"-\",expand=True)[1]\n",
      "print scales\n",
      "del scales[0]\n",
      "scales.to_csv(sys.argv[2] + \"_scales.csv\")\n",
      "scales[scales.apply(lambda x: x[\"word1\"] in x[1] and x[\"word2\"] in x[1], axis = 1)].to_csv(sys.argv[2] + \"_filtered_scales.csv\")\n",
      "from glob import iglob\n",
      "import sys\n",
      "l = {}\n",
      "eigens = {}\n",
      "\n",
      "\n",
      "for location in iglob(sys.argv[1] + \"_*/\" + sys.argv[2]):\n",
      "\tbatch = location.split('/')[0].split('_')[1]\n",
      "\twith open(sys.argv[3] + \"_\" + batch) as f:\n",
      "\t\tfor line in f:\n",
      "\t\t\ttry:\n",
      "\t\t\t\tpair, eigen = line.split(' ')\n",
      "\t\t\t\teigens[pair] = eigen\n",
      "\t\t\texcept:\n",
      "\t\t\t\tprint line\n",
      "\t\n",
      "\n",
      "\twith open(location) as f:\n",
      "\t    header = \"\"\n",
      "\t    cur = []\n",
      "\t    for line in f:\n",
      "        \tif \"-\" in line and \"/JJ\" in line:\n",
      "\t\t    if header and header != \"slow/JJ-fast/JJ\" and header != \"small/JJ-large/JJ\" and \\\n",
      "\t\t       header != \"easy/JJ-hard/JJ\" and header != \"cold/JJ-warm/JJ\":\n",
      "\t        \t    l[header] = \",\".join(cur)\n",
      "\t            header = line.strip()\n",
      "        \t    cur = []\n",
      "\t        else:\n",
      "\t            word = line.strip().split(\" \")[0]\n",
      "        \t    if word:\n",
      "\t               cur.append(word)\n",
      "\t    if header != \"slow/JJ-fast/JJ\" and header != \"small/JJ-large/JJ\" and \\\n",
      "\t    header != \"easy/JJ-hard/JJ\" and header != \"cold/JJ-warm/JJ\" and header:\n",
      "\t    \tl[header] = \",\".join(cur)\n",
      "#\t\tif header == \"\":\n",
      "#\t\t\tprint \"E: \", location\n",
      "\t\n",
      "\t\n",
      "import pandas as pd\n",
      "eigens = pd.Series(eigens).to_frame().reset_index()\n",
      "scales = pd.Series(l).to_frame().reset_index()\n",
      "scales[\"word1\"] = scales['index'].str.replace(\"/JJ\",\"\").str.split(\"-\",expand=True)[0]\n",
      "scales[\"word2\"] = scales['index'].str.replace(\"/JJ\",\"\").str.split(\"-\",expand=True)[1]\n",
      "\n",
      "scales = scales.merge(eigens, on=\"index\",how='left')\n",
      "scales['eigens'] = scales['0_y'].str.strip().apply(float)\n",
      "\n",
      "print scales\n",
      "del scales[\"index\"]\n",
      "del scales['0_y']\n",
      "scales.to_csv(sys.argv[4] + \"_scales.csv\")\n",
      "scales[scales['eigens'] > 3].to_csv(sys.argv[4] + \"_filtered_scales.csv\")\n",
      "from glob import iglob\n",
      "import sys\n",
      "l = {}\n",
      "\n",
      "\n",
      "\n",
      "for location in iglob(sys.argv[1] + \"_*/orensm.out\"):\n",
      "\tprint location\n",
      "\twith open(location) as f:\n",
      "\t    header = \"\"\n",
      "\t    cur = []\n",
      "\t    for line in f:\n",
      "        \tif \"-\" in line and \"/JJ\" in line:\n",
      "\t\t    if header and header != \"slow/JJ-fast/JJ\" and header != \"small/JJ-large/JJ\" and \\\n",
      "\t\t       header != \"easy/JJ-hard/JJ\" and header != \"cold/JJ-warm/JJ\":\n",
      "\t        \t    l[header] = \",\".join(cur)\n",
      "\t            header = line.strip()\n",
      "        \t    cur = []\n",
      "\t        else:\n",
      "\t            word = line.strip().split(\" \")[0]\n",
      "        \t    if word:\n",
      "\t               cur.append(word)\n",
      "\t    if header != \"slow/JJ-fast/JJ\" and header != \"small/JJ-large/JJ\" and \\\n",
      "\t    header != \"easy/JJ-hard/JJ\" and header != \"cold/JJ-warm/JJ\" and header:\n",
      "\t    \tl[header] = \",\".join(cur)\n",
      "#\t\tif header == \"\":\n",
      "#\t\t\tprint \"E: \", location\n",
      "\t\n",
      "\t\n",
      "import pandas as pd\n",
      "scales = pd.Series(l).to_frame().reset_index()\n",
      "scales[\"word1\"] = scales['index'].str.replace(\"/JJ\",\"\").str.split(\"-\",expand=True)[0]\n",
      "scales[\"word2\"] = scales['index'].str.replace(\"/JJ\",\"\").str.split(\"-\",expand=True)[1]\n",
      "print scales\n",
      "del scales[\"index\"]\n",
      "scales.to_csv(sys.argv[2] + \"_scales.csv\")\n",
      "scales[scales.apply(lambda x: x[\"word1\"] in x[0] and x[\"word2\"] in x[0], axis = 1)].to_csv(sys.argv[2] + \"_filtered_scales.csv\")\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from itertools import izip\n",
      "\n",
      "def read_semeval(fname):\n",
      "    output = []\n",
      "    idents = []\n",
      "    with open(fname) as f:\n",
      "        for line in f:\n",
      "            line = line.strip()\n",
      "            if not line: continue\n",
      "            left, right = line.split(\" ::: \")\n",
      "            word, ident = left.split(\" \")\n",
      "            ident = int(ident)\n",
      "            subs = right.split(\";\")\n",
      "            row = {\n",
      "                'target': word,\n",
      "            }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            idents.append(ident)\n",
      "            for i, s in enumerate(subs):\n",
      "                row[\"sub%02d\" % (i + 1)] = s\n",
      "            output.append(row)\n",
      "    return pd.DataFrame(output, index=idents)\n",
      "\n",
      "ooc = pd.read_table(\"recompute_predictions/ooc.tsv\")\n",
      "ooc_p = read_semeval(\"recompute_predictions/se_ooc.boot\")\n",
      "baloren = pd.read_table(\"recompute_predictions/baloren.tsv\")\n",
      "baloren_p = read_semeval(\"recompute_predictions/se_baloren.boot\")\n",
      "orensm = pd.read_table(\"recompute_predictions/orensm.tsv\")\n",
      "orensm_p = read_semeval(\"recompute_predictions/se_orensm.boot\")\n",
      "pic = pd.read_table(\"recompute_predictions/pic.tsv\")\n",
      "pic_p = read_semeval(\"recompute_predictions/pic.boot\")\n",
      "\n",
      "mask_cherry = (baloren['p@3av'] < orensm['p@3av']) & (orensm['p@1av'] < pic['p@1av'])\n",
      "mask_lemon = (baloren['p@3av'] > orensm['p@3av']) & (pic['p@1av'] == 0.0) & (baloren['p@1av'] == 1.0)\n",
      "\n",
      "#mask = mask_cherry | mask_lemon\n",
      "#mask = mask_lemon\n",
      "mask = mask_cherry\n",
      "\n",
      "sel_baloren = baloren[mask].iterrows()\n",
      "sel_orensm = orensm[mask].iterrows()\n",
      "sel_pic = pic[mask].iterrows()\n",
      "\n",
      "tables = (ooc_p, baloren_p, orensm_p, pic_p)\n",
      "\n",
      "def bf(yes):\n",
      "    if yes:\n",
      "        return '\\\\bf '\n",
      "    else:\n",
      "        return '    '\n",
      "\n",
      "for (b, brow), (o, orow), (u, row) in izip(sel_baloren, sel_orensm, sel_pic):\n",
      "    assert (brow['ident'] == orow['ident'] == row['ident'])\n",
      "    print \"ID: %d\" % brow['ident']\n",
      "    print \"Target: %s\" % row['target']\n",
      "    word = row['target'][:row['target'].rindex('/')]\n",
      "    print \"Context: %s\" % row['sentence']\n",
      "    golds = row['gold'].split(' ')\n",
      "    golds = [(k, float(v)) for k, v in [g.split(':') for g in golds]]\n",
      "    golds = set([k for k, v in golds if v > 0])\n",
      "    print \"Gold: %s\" % \" \".join(list(golds))\n",
      "    print \"P@1AV: %.3f  %.3f  %.3f\" % (brow['p@1av'], orow['p@1av'], row['p@1av'])\n",
      "    print \"P@3AV: %.3f  %.3f  %.3f\" % (brow['p@3av'], orow['p@3av'], row['p@3av'])\n",
      "    print \"Table:\"\n",
      "    print \"\\\\hline\"\n",
      "    print \" & \".join(\"{    %-20s}\" % c for c in ('OOC', '\\\\balAddCos', '\\\\ourmeas', '\\\\ourmeasparam')) + \" \\\\\\\\\"\n",
      "    print \"\\\\hline\\\\hline\"\n",
      "    print \"\\\\multicolumn{4}{|c|}{%s}\\\\\\\\\" % (row['sentence'].replace(word, \"{\\\\bf %s}\" % word))\n",
      "    print \"\\\\hline\"\n",
      "    for i in range(1, 6):\n",
      "        cols = [t[\"sub0%d\" % i][brow['ident']] for t in tables]\n",
      "        print \" & \".join(\"{%s%-20s}\" % (bf(c in golds), c) for c in cols) + \" \\\\\\\\\"\n",
      "    print \"\\\\hline\"\n",
      "    print\n",
      "\n",
      "#!/bin/bash\n",
      "#mkdir data/adj\n",
      "#rm data/adj/parses*\n",
      "#python data_creation.py data/adj\n",
      "\n",
      "for pattern in 'adjscale'; do # 'sym' 'both'; do\n",
      "for oneword in ''; do # '--oneword'; do\n",
      "   for expand in '--expand'; do\n",
      "\n",
      "    echo lexub.py\n",
      "    echo data/adj${expand/--/.}${oneword/--/.}.$pattern\n",
      "\n",
      "    #mkdir data/adj${expand/--/.}${oneword/--/.}\n",
      "    #rm data/adj${expand/--/.}${oneword/--/.}/parses*\n",
      "    echo \"python data_creation.py --allpairs pairs_list.csv $expand $oneword data/adj${expand/--/.}${oneword/--/.}.$pattern --patterns $pattern\"\n",
      "#    python data_creation.py --allpairs pairs_list.csv $expand $oneword data.test/adj${expand/--/.}${oneword/--/.}.$pattern --patterns $pattern\n",
      "    for i in {0..500}; do\n",
      "#     mkdir \"results.prop_random.2${expand/--/.}${oneword/--/.}.${pattern}_$i\"\n",
      "      for baseline in orensm baloren; do #context2vec; do\n",
      "#        python ../PIC.old/lexsub.py --baseline $baseline -d data/adj${expand/--/.}${oneword/--/.}.${pattern}_$i --save \"results.prop_random.2${expand/--/.}${oneword/--/.}.${pattern}_$i/$baseline\" --random -p ~/hyperwords/depSDPMI.smoothed/pmi1\n",
      "        #python cca.py \"results.prop_random${expand/--/.}${oneword/--/.}.${pattern}_$i/$baseline.answers\" \"results.prop_random${expand/--/.}${oneword/--/.}.${pattern}_$i/$baseline.out\" >> proportional_random_stats_$i\n",
      "#        python ../PIC.old/lexsub.py --baseline $baseline -d data/adj${expand/--/.}${oneword/--/.}.${pattern}_$i --save \"results${expand/--/.}${oneword/--/.}.${pattern}_$i/$baseline.random\" --random -p ~/hyperwords/depSDPMI.smoothed/pmi1\n",
      "        python cca.py \"results.prop_random.2${expand/--/.}${oneword/--/.}.${pattern}_$i/$baseline.answers.random\" \"results.prop_random.2${expand/--/.}${oneword/--/.}.${pattern}_$i/$baseline.random.answers.out.fixed\" >> proportional_random_stats.2.fixed_$i\n",
      "      done\n",
      "    done\n",
      "  done\n",
      "done\n",
      "done\n",
      "\n",
      "##mkdir data/adj.generic\n",
      "##rm data/adj.generic/parses*\n",
      "##python data_creation.py --generic --allpairs pairsList.pkl data/adj.generic --patterns 'adjscale'\n",
      "#\n",
      "##for i in {0..999}; do\n",
      "  #mkdir results.generic_$i\n",
      "#  #for baseline in random orensm baloren context2vec; do\n",
      "    ##python ../PIC.old/lexsub.py --baseline $baseline -d data/adj.generic_$i --save \"results.generic_$i/$baseline\" --random\n",
      "    ##python cca.py \"results.generic_$i/$baseline.answers\" \"results.generic_$i/$baseline.out\" >> stats_$i\n",
      "#    python ../PIC.old/lexsub.py --baseline $baseline -d data/adj.generic_$i --save \"results.generic_$i/$baseline.random\" --random\n",
      "    #python cca.py \"results.generic_$i/$baseline.answers.random\" \"results.generic_$i/$baseline.random.answers.out\" >> stats_$i\n",
      "  #done\n",
      "#done\n",
      "from sobol_seq import i4_sobol_generate\n",
      "from itertools import combinations, product\n",
      "import numpy as np\n",
      "allWords = [['minuscule', 'tiny', 'small', 'big', 'large', 'huge', 'enormous', 'gigantic'],\n",
      "            ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      "            ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent'],\n",
      "            ['horrible', 'terrible', 'awful', 'bad', 'good', 'great', 'wonderful', 'awesome'],\n",
      "            ['slow', 'quick', 'fast', 'speedy'],\n",
      "            ['simple', 'easy', 'hard', 'difficult'],\n",
      "            ['few', 'some', 'several', 'many'],\n",
      "            ['dark', 'dim', 'light', 'bright'],\n",
      "            ['freezing', 'cold', 'warm', 'hot'],\n",
      "            ]\n",
      "\n",
      "pairCombos = [combinations(scale,2) for scale in allWords]\n",
      "pairs = [[x for x in y] for y in pairCombos ]\n",
      "dimensions =  len(pairs)\n",
      "multiplier = np.array( map(lambda x: len(x), pairs) )\n",
      "randoms = i4_sobol_generate(dimensions,1064)\n",
      "randoms = randoms[64:]\n",
      "indx = np.trunc(multiplier * randoms).astype(int)\n",
      "with open('pairs_list.csv','w') as f:\n",
      "  for row in indx:\n",
      "    trials = []\n",
      "    for i, scale in enumerate(pairs):\n",
      "      trials.append(\"%s-%s\" % scale[row[i]])\n",
      "    f.write(\",\".join(trials))\n",
      "    f.write(\"\\n\")\n",
      "#print len(commonPairsSet)\n",
      "#pickle.dump(commonPairsSet,open('pairsList.pkl','w'))\n",
      "from sobol_seq import i4_sobol_generate\n",
      "from itertools import combinations, product\n",
      "import numpy as np\n",
      "allWords = [['minuscule', 'tiny', 'small', 'big', 'large', 'huge', 'enormous', 'gigantic'],\n",
      "            ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      "            ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent'],\n",
      "            ['horrible', 'terrible', 'awful', 'bad', 'good', 'great', 'wonderful', 'awesome'],\n",
      "            ['slow', 'quick', 'fast', 'speedy'],\n",
      "            ['simple', 'easy', 'hard', 'difficult'],\n",
      "            ['few', 'some', 'several', 'many'],\n",
      "            ['dark', 'dim', 'light', 'bright'],\n",
      "            ['freezing', 'cold', 'warm', 'hot'],\n",
      "            ]\n",
      "\n",
      "pairCombos = [combinations(scale,2) for scale in allWords]\n",
      "pairs = [[x for x in y] for y in pairCombos ]\n",
      "dimensions =  len(pairs)\n",
      "multiplier = np.array( map(lambda x: len(x), pairs) )\n",
      "print multiplier\n",
      "#randoms = i4_sobol_generate(dimensions,1064)\n",
      "#randoms = randoms[64:]\n",
      "#indx = np.trunc(multiplier * randoms).astype(int)\n",
      "#with open('pairs_list.csv','w') as f:\n",
      "#  for row in indx:\n",
      "#    trials = []\n",
      "#    for i, scale in enumerate(pairs):\n",
      "#      trials.append(\"%s-%s\" % scale[row[i]])\n",
      "#    f.write(\",\".join(trials))\n",
      " #   f.write(\"\\n\")\n",
      "#print len(commonPairsSet)\n",
      "#pickle.dump(commonPairsSet,open('pairsList.pkl','w'))\n",
      "from itertools import combinations, product\n",
      "import numpy as np\n",
      "import random\n",
      "import sys\n",
      "allWords = [['hideous', 'ugly', 'pretty', 'beautiful', 'gorgeous'],\n",
      "\t    ['same', 'alike', 'similar', 'different'],\n",
      "\t    ['ancient', 'old', 'fresh', 'new']]\n",
      "otherWords = [['minuscule', 'tiny', 'small', 'big', 'large', 'huge', 'enormous', 'gigantic'],\n",
      "            ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      "            ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent'],\n",
      "            ['horrible', 'terrible', 'awful', 'bad', 'good', 'great', 'wonderful', 'awesome'],\n",
      "            ['slow', 'quick', 'fast', 'speedy'],\n",
      "            ['simple', 'easy', 'hard', 'difficult'],\n",
      "            ['few', 'some', 'several', 'many'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ['dark', 'dim', 'light', 'bright'],\n",
      "            ['freezing', 'cold', 'warm', 'hot'],\n",
      "            ]\n",
      "\n",
      "pairCombos = [combinations(scale,2) for scale in allWords]\n",
      "otherCombos = [list(combinations(scale,2)) for scale in otherWords]\n",
      "print pairCombos\n",
      "pairs = [[x for x in y] for y in pairCombos ]\n",
      "with open('test_pairs_augmented.csv','w') as f:\n",
      "\tfor prod in product(*pairs):\n",
      "\t\ttrials = []\n",
      "\t\tfor pair in prod:\n",
      "\t\t\ttrials.append(\"%s-%s\" % pair)\n",
      "\n",
      "\t\taugmenters = random.sample(otherCombos,2)\t\t\n",
      "\t\ttrials.append(\"%s-%s\" % random.choice(augmenters[0]))\n",
      "\t\ttrials.append(\"%s-%s\" % random.choice(augmenters[1]))\n",
      "\n",
      "\t\tf.write(','.join(trials))\n",
      "\t\tf.write(\"\\n\")\n",
      "from sobol_seq import i4_sobol_generate\n",
      "from itertools import combinations, product\n",
      "import numpy as np\n",
      "allWords = [['hideous', 'ugly', 'pretty', 'beautiful', 'gorgeous'],\n",
      "\t    ['same', 'alike', 'similar', 'different'],\n",
      "\t    ['ancient', 'old', 'fresh', 'new']]\n",
      "#allWords = [['minuscule', 'tiny', 'small', 'big', 'large', 'huge', 'enormous', 'gigantic'],\n",
      "#            ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      "#            ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent'],\n",
      "#            ['horrible', 'terrible', 'awful', 'bad', 'good', 'great', 'wonderful', 'awesome'],\n",
      "#            ['slow', 'quick', 'fast', 'speedy'],\n",
      "#            ['simple', 'easy', 'hard', 'difficult'],\n",
      "#            ['few', 'some', 'several', 'many'],\n",
      "#            ['dark', 'dim', 'light', 'bright'],\n",
      "#            ['freezing', 'cold', 'warm', 'hot'],\n",
      "#            ]\n",
      "\n",
      "pairCombos = [combinations(scale,2) for scale in allWords]\n",
      "print pairCombos\n",
      "pairs = [[x for x in y] for y in pairCombos ]\n",
      "with open('test_pairs.csv','w') as f:\n",
      "\tfor prod in product(*pairs):\n",
      "\t\ttrials = []\n",
      "\t\tfor pair in prod:\n",
      "\t\t\ttrials.append(\"%s-%s\" % pair)\n",
      "\t\tf.write(','.join(trials))\n",
      "\t\tf.write(\"\\n\")\n",
      "#dimensions =  len(pairs)\n",
      "#multiplier = np.array( map(lambda x: len(x), pairs) )\n",
      "#randoms = i4_sobol_generate(dimensions,1064)\n",
      "#r#andoms = randoms[64:]\n",
      "#indx = np.trunc(multiplier * randoms).astype(int)\n",
      "#with open('pairs_list.csv','w') as f:\n",
      "#  for row in indx:\n",
      "#    trials = []\n",
      "#    for i, scale in enumerate(pairs):\n",
      "#      trials.append(\"%s-%s\" % scale[row[i]])\n",
      "#    f.write(\",\".join(trials))\n",
      "#    f.write(\"\\n\")\n",
      "#print len(commonPairsSet)\n",
      "#pickle.dump(commonPairsSet,open('pairsList.pkl','w'))\n",
      "import pandas as pd\n",
      "import sys\n",
      "import csv\n",
      "\n",
      "def get_answers_greater_than(answer_dir, results_dir, algorithm, limit):\n",
      "\tdata_seeds = pd.read_csv(answer_dir + '/' + algorithm + '.answers',delimiter=\" \",header=None)\n",
      "\tdata2 = pd.read_csv(answer_dir + '/' + algorithm,delimiter=\"\\t\")\n",
      "\tdata2[\"Seeds\"] = data_seeds[0]\n",
      "\tanswer_col = []\n",
      "\tfor x in data2[\"predicted\"].str.split():\n",
      "\t    answers = []\n",
      "\t    for pair in map(lambda y: str.split(y,\":\"),x):\n",
      "\t        if float(pair[1]) > limit:\n",
      "\t             answers.append(pair[0])\n",
      "\t    answer_col.append(\",\".join(answers))\n",
      "\tdata2['answer_string'] = answer_col\n",
      "\tdata2 = data2[data2['answer_string'].apply(lambda x: len(x) > 0)]        \n",
      "\tdata2[['Seeds','answer_string']].to_csv(results_dir + '/' + algorithm + '.answers', sep=' ',header=None, index=None,quoting = csv.QUOTE_NONE)\n",
      "\n",
      "\n",
      "answers = sys.argv[1]\n",
      "results = sys.argv[2]\n",
      "\n",
      "get_answers_greater_than(answers,results,'orensm',0.01)\n",
      "get_answers_greater_than(answers,results,'baloren',0.01)\n",
      "#get_answers_greater_than(answers,results,'context2vec',0.01)\n",
      "import sys\n",
      "\n",
      "def process_file(old_dir,new_dir,algorithm,length):\n",
      "\tgen = []\n",
      "\twith open(old_dir  + '/' + algorithm + '.answers') as f:\n",
      "\t\tfor line in f:\n",
      "\t    \t    pair, responses = line.split()\n",
      "        \t    responses = responses.split(',')\n",
      "\t            gen.append(pair +' ' + ','.join(responses[:length]))\n",
      "\n",
      "\twith open(new_dir  + '/' + algorithm + '.answers','w') as f:\n",
      "\t\tf.write('\\n'.join(gen))\n",
      "\n",
      "\n",
      "process_file(sys.argv[1],sys.argv[2],'baloren',int(sys.argv[3]))\n",
      "process_file(sys.argv[1],sys.argv[2],'orensm',int(sys.argv[3]))\n",
      "#process_file(sys.argv[1],sys.argv[2],'context2vec',int(sys.argv[3]))\n",
      "         \n",
      "#   Parameters\n",
      "## output location\n",
      "## two word search or one\n",
      "## extra data or not?\n",
      "import os\n",
      "from docopt import docopt\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "import codecs\n",
      "import gzip\n",
      "#import StringIO\n",
      "import lucene\n",
      "from random import random, sample\n",
      "lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n",
      "from lucene import VERSION\n",
      "from org.apache.lucene import analysis,util\n",
      "from lupyne import engine\n",
      "from cStringIO import StringIO\n",
      "from itertools import izip\n",
      "import sys\n",
      "from collections import defaultdict\n",
      "import codecs\n",
      "from string import Template\n",
      "\n",
      "def tokenizer(reader):\n",
      "    return analysis.standard.StandardTokenizer(util.Version.LATEST, reader)##   Parameters\n",
      "\n",
      "\n",
      "myAnalyzer = engine.indexers.Analyzer( tokenizer,\n",
      "     analysis.standard.StandardFilter, analysis.LowerCaseFilter)\n",
      "\n",
      "\n",
      "indexer = engine.IndexSearcher('/home/bryan/ukwakSentenceIndex',analyzer=myAnalyzer)\n",
      "\n",
      "import pandas   \n",
      "flatList = pandas.read_csv(\"commonAdjs.txt\",header=None,squeeze=True).str.strip().tolist()\n",
      "\n",
      "def getText(i):\n",
      "    env = lucene.getVMEnv()\n",
      "    env.attachCurrentThread()\n",
      "    d = indexer.doc(i.doc)\n",
      "    return d['text']                 \n",
      "\n",
      "\n",
      "pool = mp.ThreadingPool(nodes=8) \n",
      "\n",
      "find_counts = defaultdict(int)\n",
      "\n",
      "attr_patterns = [\n",
      "   Template('text:\"of * is $x\"'),\n",
      "   Template('text:\"of * was $x\"'),\n",
      "   Template('text:\"of a * is $x\"'),\n",
      "   Template('text:\"of a * was $x\"'),\n",
      "   Template('text:\"of an * is $x\"'),\n",
      "   Template('text:\"of an * was $x\"'),\n",
      "   Template('text:\"of the * is $x\"'),\n",
      "   Template('text:\"of the * was $x\"'),\n",
      "\n",
      "\n",
      "   Template('text:\"$x or *\"'),\n",
      "   Template('text:\"or $x\"'),\n",
      "\n",
      "   Template('text:\"\\' * was $x\"'),\n",
      "   Template('text:\"\\' * is $x\"'),\n",
      "\n",
      "   Template('text:\"is $x in\"'),\n",
      "   Template('text:\"is $x of\"'),\n",
      "   Template('text:\"was $x in\"'), \n",
      "   Template('text:\"was $x of\"'),\n",
      "   Template('text:\"are $x in\"'), \n",
      "   Template('text:\"are $x of\"'),\n",
      "   Template('text:\"were $x in\"'), \n",
      "   Template('text:\"were $x of\"'),\n",
      "\n",
      "   ]\n",
      "\n",
      "with codecs.open('hartungCorpora.txt','w','utf-8') as f:\n",
      "\tfor adj in flatList:\n",
      "\t\tfor pattern in attr_patterns:\n",
      "\t\t\tpat1 = pattern.substitute({'x': adj})\n",
      "\t\t\thits = pool.map(getText,list(indexer.search(pat1 ,scores=False).scoredocs))\n",
      "\t\t\tfind_counts[adj] += len(hits)\n",
      "\t\t\n",
      "\t\t\tf.write('\\n'.join(hits))\n",
      "\n",
      "\n",
      "with codecs.open('hartungCounts.txt','w','utf-8') as f:\n",
      "\tfor adj in flatList:\n",
      "\t\tf.write(adj + '\\t' + str(find_counts[adj]))\n",
      "#!/usr/bin/env python\n",
      "import sys\n",
      "import os.path\n",
      "import argparse\n",
      "import numpy as np\n",
      "import numexpr as ne\n",
      "import random\n",
      "from unidecode import unidecode\n",
      "from itertools import izip\n",
      "from collections import defaultdict\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import normalize\n",
      "\n",
      "import depextract\n",
      "import utdeftvs\n",
      "import ctxpredict.models\n",
      "import numpy.ma as ma\n",
      "import copy\n",
      "import dill as pickle\n",
      "import pathos.multiprocessing as mp\n",
      "import shelve\n",
      "from klepto.archives import dir_archive\n",
      "from klepto import keymaps\n",
      "\t\n",
      "sys.path.append('./hyperwords/hyperwords')\n",
      "\n",
      "from representations.explicit import Explicit\n",
      "from context2vec.common.model_reader import ModelReader\n",
      "import codecs\n",
      "\n",
      "#from nn import *\n",
      "\n",
      "USE_LEMMAPOS = False\n",
      "\n",
      "\n",
      "def splitpop(string, delimeter):\n",
      "    \"\"\"\n",
      "    Splits a string along a delimiter, and returns the\n",
      "    string without the last field, and the last field.\n",
      "\n",
      "    >>> splitpop('hello.world.test', '.')\n",
      "    'hello.world', 'test'\n",
      "    \"\"\"\n",
      "    fields = string.split(delimeter)\n",
      "    return delimeter.join(fields[:-1]), fields[-1]\n",
      "\n",
      "\n",
      "def find_start(string, index):\n",
      "    words = string.split(\" \")\n",
      "    before = words[:index]\n",
      "    if not before:\n",
      "        return 0\n",
      "    return len(\" \".join(before)) + 1\n",
      "\n",
      "\n",
      "def revsorted(arr):\n",
      "    random.shuffle(arr)\n",
      "    return sorted(arr, key=lambda x: x[1], reverse=True)\n",
      "\n",
      "\n",
      "def rewrite_pos(string):\n",
      "    # stupid data exceptions :(\n",
      "    if string == '.':\n",
      "        return string\n",
      "    elif string == '..N':\n",
      "        return '.'\n",
      "    word, pos = splitpop(string, \".\")\n",
      "    if '.' in word:\n",
      "        word, pos = splitpop(word, \".\")\n",
      "    word = word.lower()\n",
      "    pos = pos.lower()\n",
      "    if pos == 'n':\n",
      "        return word + '/NN'\n",
      "    elif pos == 'j' or pos == 'a':\n",
      "        return word + '/JJ'\n",
      "    elif pos == 'v':\n",
      "        return word + '/VB'\n",
      "    elif pos == 'r':\n",
      "        return word + '/RB'\n",
      "    else:\n",
      "        raise ValueError(\n",
      "            \"Don't know how to handle a POS tag of '%s' in '%s'\" % (pos, string))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def scrub_substitutes(before, target):\n",
      "    \"\"\"\n",
      "    Not all substitutes are good substitutes. We need to clean them up,\n",
      "    lemmatize them, add POS tags, and remove multiword expressions.\n",
      "\n",
      "    Input: a dictionary with uncleaned substitute/weight as key/value\n",
      "    Returns: a dictionary with cleaned subsititute/weight as key/value\n",
      "\n",
      "    This dictionary will (probably) be smaller than the input dictionary.\n",
      "    \"\"\"\n",
      "    targetpos = USE_LEMMAPOS and target[-3:] or \"\"\n",
      "    before_iter = before.iteritems()\n",
      "    remove_mwe = ((k, v) for k, v in before_iter if ' ' not in k)\n",
      "    remove_dash = ((k, v) for k, v in remove_mwe if '-' not in k)\n",
      "    add_pos = ((k + targetpos, v) for k, v in remove_mwe)\n",
      "    return dict(add_pos)\n",
      "\n",
      "\n",
      "def scrub_candidates(before, target):\n",
      "    fakesubs = {k: 1 for k in before}\n",
      "    return scrub_substitutes(fakesubs, target).keys()\n",
      "\n",
      "\n",
      "class LexsubData(object):\n",
      "#    @profile\n",
      "    def __init__(self, foldername):\n",
      "        # load gold file\n",
      "        golds = {}\n",
      "        targets = {}\n",
      "        original_targets = {}\n",
      "        candidates = {}\n",
      "        sentences = defaultdict(list)\n",
      "        starts = defaultdict(list)\n",
      "        tokens = {}\n",
      "\n",
      "        with codecs.open(os.path.join(foldername, \"gold\"),'r','utf-8') as f:\n",
      "            for line in f:\n",
      "                line = line.strip()\n",
      "                if not line:\n",
      "                    continue\n",
      "                if \"::\" not in line:\n",
      "                    continue\n",
      "                left, right = line.split(\"::\")\n",
      "                left = left.strip()\n",
      "                right = right.strip()\n",
      "                target, ident = splitpop(left, \" \")\n",
      "                ident = int(ident)\n",
      "                original_targets[ident] = target\n",
      "                target = rewrite_pos(target)\n",
      "                rights = [r.strip() for r in right.split(\";\") if r.strip()]\n",
      "                # gold_golds is what the data explicitly says are the gold substitutes\n",
      "                # but these are not lemmatized or POS tagged, and contain MWE. We\n",
      "                # need to clean them up with scrub_substitutes\n",
      "                gold_golds = {k: int(v) for k, v in (\n",
      "                    splitpop(r, ' ') for r in rights)}\n",
      "                golds[ident] = scrub_substitutes(gold_golds, target)\n",
      "                targets[ident] = target\n",
      "\n",
      "        with codecs.open(os.path.join(foldername, \"sentences\"),'r','utf-8') as f:\n",
      "            for line in f:\n",
      "                line = line.strip()\n",
      "                if not line:\n",
      "\t\t    print \"E: \", line\n",
      "                    continue\n",
      "                if \"\\t\" not in line:\n",
      "\t\t    print \"ERR: \" , line\n",
      "                    continue\n",
      "                target, ident, index, sentence = line.split(\"\\t\")\n",
      "                target = rewrite_pos(target)\n",
      "                ident = int(ident)\n",
      "                index = int(index)\n",
      "                if ident not in targets:\n",
      "\t\t    print line, target, ident\n",
      "                    continue\n",
      "                if targets[ident] != target:\n",
      "                    raise ValueError(\"Something didn't line up: found %s, expected %s\" % (\n",
      "                        target, targets[ident]))\n",
      "                sentences[sentence].append(ident)\n",
      "                starts[ident] = find_start(sentence, index)\n",
      "\n",
      "        # parse the sentences\n",
      "        plaintexts = sentences.keys()\n",
      "        parsed_sentences = depextract.preprocess_with_corenlp(\n",
      "            os.path.join(foldername, \"parses\"), plaintexts)\n",
      "        parsed_sentences = depextract.parse_corenlp_xml(os.path.join(\n",
      "            foldername, \"parses.xml\"), dependencytype='basic-dependencies')\n",
      "        parses = {}\n",
      "\tsentence_keys = sentences.iterkeys()\n",
      "\n",
      "#\t@profile\n",
      "\tdef process_pair(sentence,parse):\n",
      "#   sentence, parse = zipped\n",
      "\t   idents = sentences[sentence]\n",
      "\t   for ident in idents:\n",
      "                parses[ident] = parse\n",
      "                for t in parse.tokens:\n",
      "                    if t.start == starts[ident]:\n",
      "                        tokens[ident] = t\n",
      "                        break\n",
      "                else:\n",
      "                    print ident,targets[ident], starts[ident], [t.start for t in parse.tokens], [t.word_normed for t in parse.tokens]\n",
      "                    raise IndexError(\n",
      "                        \"These are not the words you are looking for.\")\n",
      "\n",
      "        #pool = \tmp.ThreadingPool(10)\n",
      "\tmap(process_pair,sentence_keys,parsed_sentences)\n",
      "#\t_ = [process_pair(pair) for pair in izip(sentence_keys,parsed_sentences)]\n",
      "\t#pool.close()\n",
      "\t#pool.terminate()\n",
      "\t\n",
      "#        for sentence, parse in izip(sentence_keys, parsed_sentences):\n",
      "#            idents = sentences[sentence]\n",
      "#            for ident in idents:\n",
      "#                parses[ident] = parse\n",
      "#                for t in parse.tokens:\n",
      "#                    if t.start == starts[ident]:\n",
      "#                        tokens[ident] = t\n",
      "#                        break\n",
      "#                else:\n",
      "#                    print targets[ident], starts[ident], [(t, t.start) for t in parse.tokens]\n",
      "#                    raise IndexError(\n",
      "#                        \"These are not the words you are looking for.\")\n",
      "\n",
      "        # generate candidates\n",
      "        #candidates = defaultdict(set)\n",
      "        # for ident, subs in golds.iteritems():\n",
      "        #    candidates[targets[ident]].update(subs.keys())\n",
      "        candidates = {}\n",
      "        with open(os.path.join(foldername, \"candidates\")) as f:\n",
      "            for line in f:\n",
      "                line = line.strip()\n",
      "                left, right = line.split(\"::\")\n",
      "                target = rewrite_pos(left)\n",
      "                candidates[target] = scrub_candidates(right.split(\";\"), target)\n",
      "\n",
      "        idents = targets.keys()\n",
      "\n",
      "        self.idents = idents\n",
      "\n",
      "        try:\n",
      "            self.tokens = [tokens[k] for k in idents]\n",
      "        except:\n",
      "            print tokens\n",
      "        self.original_targets = [original_targets[k] for k in idents]\n",
      "        self.targets = [targets[k] for k in idents]\n",
      "        self.parses = [parses[k] for k in idents]\n",
      "\tprint len(self.targets)\n",
      "\tprint len(self.idents)\n",
      "        self.golds = defaultdict(dict)\n",
      "        for ident, subs in golds.iteritems():\n",
      "\t    get = subs.get\n",
      "            for c in candidates[targets[ident]]:\n",
      "                self.golds[ident][c] = get(c, 0)\n",
      "\t#print self.golds\n",
      "        self.golds = [self.golds[k] for k in idents]\n",
      "\t#print self.golds\n",
      "        assert len(self.targets) == len(self.parses) == len(\n",
      "            self.golds) == len(self.idents) == len(self.original_targets)\n",
      "#    @profile\n",
      "    def generate_matrices(self, vocablookup):\n",
      "#\tprint self.targets\n",
      "        targets = np.zeros(len(self.targets), dtype=np.int32)\n",
      "        numtargets = len(self.targets)\n",
      "        maxcands = max(len(g) for g in self.golds)\n",
      "\n",
      "        # the sister matrices\n",
      "        subs = np.zeros((numtargets, maxcands), dtype=np.int32)\n",
      "        scores = np.zeros((numtargets, maxcands), dtype=np.float32)\n",
      "\n",
      "        targets_with_pos = []\n",
      "\n",
      "        # we want to produce a a matrix which contains one target per row\n",
      "        # each column will have the ID of the substitute and the corresponding\n",
      "        # number of substitutions in its sister matrix\n",
      "        # but we want them to be ordered so the ID with the most subs is first\n",
      "\tget = vocablookup.get\n",
      "\t#gold_get = gold.get\n",
      "\tsubs_set = subs.itemset\n",
      "\tscores_set = scores.itemset\n",
      "        for i, (token, gold) in enumerate(izip(self.tokens, self.golds)):\n",
      "            target = token.word_normed  # USE_LEMMAPOS and token.lemma_pos or token.word_normed\n",
      "            idx = get(target, 0)\n",
      "\t    if idx == 0:\n",
      "\t\tprint token, target, i\n",
      "            targets[i] = idx\n",
      "            # we want items not in our vocab to have a 0 weight\n",
      "            orderedgold = sorted(gold.keys(), key=lambda x: gold[\n",
      "                                 x] or 0, reverse=True)\n",
      "            j = 0\n",
      "\t    sub_row = subs[i,:]\n",
      "\t    score_row = scores[i,:]\n",
      "            #for g in orderedgold:\n",
      "            #    if g not in vocablookup:\n",
      "            #        continue\n",
      "#           #     subs[i, j] = get(g, 0)\n",
      "#\t\tsub_row[j] = get(g,0)\n",
      "#                scores[i, j] = gold[g]\n",
      "#\t\tscore_row[j] = gold[g]\n",
      " #               j += 1\n",
      "             \n",
      "\t    subs_map = filter(lambda x: x is not None, map(get,orderedgold))\n",
      "\t    sub_row[:len(subs_map)] = np.array(subs_map)\n",
      "\t\n",
      "            scores_map = filter(lambda x: x is not None, map(gold.get,orderedgold))\n",
      "\t    score_row[:len(scores_map)] = np.array(scores_map)\t\n",
      "#\t    if not np.array_equal(sub_row[:len(other)],np.array(other)):\n",
      "#\t\tprint sub_row[:len(other)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\t\tprint np.array(other)\n",
      "\t     \n",
      "\n",
      "        # get rid of data points where the target was OOV\n",
      "\tprint np.where(np.array(targets) == 0)\n",
      "        subs = subs[targets != 0]\n",
      "        self.targets = np.array(self.targets)[targets != 0]\n",
      "        self.idents = np.array(self.idents)[targets != 0]\n",
      "        self.tokens = np.array(self.tokens)[targets != 0]\n",
      "        self.parses = np.array(self.parses)[targets != 0]\n",
      "        scores = scores[targets != 0]\n",
      "        targets = targets[targets != 0]\n",
      "\tprint len(self.targets)\n",
      "        assert len(self.targets) == len(self.parses) == len(self.idents)\n",
      "\n",
      "        return self.idents, targets, subs, scores\n",
      "\n",
      "\n",
      "def dependencies_to_indices(target_tokens, parses, lookup, space):\n",
      "    deps = []\n",
      "    x = 0\n",
      "    y = 0\n",
      "    for target, parse in izip(target_tokens, parses):\n",
      "        deps.append([])\n",
      "        if USE_LEMMAPOS:\n",
      "            extractor = depextract.extract_relations_for_token(parse, target)\n",
      "        else:\n",
      "            extractor = depextract.extract_relations_for_token_melamud(\n",
      "                parse, target, inverter='I')\n",
      "        for relation, attachment, in extractor:\n",
      "            if USE_LEMMAPOS:\n",
      "                dep = relation + \"+\" + attachment.lemma_pos\n",
      "            else:\n",
      "                dep = relation + \"_\" + attachment.word_normed\n",
      "            if dep in lookup:\n",
      "                deps[-1].append(lookup[dep])\n",
      "                x += 1\n",
      "            else:\n",
      "                #print attachment.word_normed, dep\n",
      "                y += 1\n",
      "                pass\n",
      "                # if attachment.word_normed in space.lookup:\n",
      "                # print '-', dep\n",
      "                #    pass\n",
      "    print x, y\n",
      "    numrows = len(deps)\n",
      "    numcols = max(len(d) for d in deps)\n",
      "    depmat = np.zeros((numrows, numcols), dtype=np.int32)\n",
      "\n",
      "    for i, d in enumerate(deps):\n",
      "        l = len(d)\n",
      "        depmat[i, :l] = d\n",
      "\n",
      "    return depmat[:, ::-1]\n",
      "\n",
      "\n",
      "def dependencies_to_indicies(target_tokens, parses, vlookup, rlookup):\n",
      "    deps = []\n",
      "    rels = []\n",
      "    for target, parse in izip(target_tokens, parses):\n",
      "        d = np.zeros(10)\n",
      "        r = np.zeros(10)\n",
      "        i = 0\n",
      "        relattachments = list(depextract.extract_relations_for_token_melamud(\n",
      "            parse, target, inverter='I'))\n",
      "        for relation, attachment in relattachments:\n",
      "            if i >= 10:\n",
      "                break\n",
      "            if relation not in rlookup or attachment.word_normed not in vlookup:\n",
      "                continue\n",
      "            rid = rlookup[relation] + 1\n",
      "            vid = vlookup[attachment.word_normed]\n",
      "            d[i] = vid\n",
      "            r[i] = rid\n",
      "            i += 1\n",
      "        deps.append(d)\n",
      "        rels.append(r)\n",
      "    return [np.array(deps), np.array(rels)]\n",
      "\n",
      "#@profile\n",
      "def parses_to_context2vec(target_tokens, parses, right_only, model, vocablookup):\n",
      "    contexts = []\n",
      "  \n",
      "    #cache = shelve.open(\"context2vec.cache\")\n",
      "    cache = dir_archive('context2vec.klepto',serialized=True,cached=False)\n",
      "    hasher = keymaps.hashmap()    \n",
      "\n",
      "    def process_pair(zipped):\n",
      "\timport traceback\n",
      "\ttry:\n",
      "\t\ttarget = zipped[0]\n",
      "\t\tparse = zipped[1]\n",
      "\t\tif target is None or parse is None:\n",
      "\t\t\treturn None\n",
      "\t\tif target.word_normed in vocablookup:\n",
      "                \ttokens = copy.deepcopy(parse.tokens)\n",
      "                \ttarget_index = tokens.index(target)\n",
      "\n",
      "               \t\ttokens[target_index] = None\n",
      "                \ttokens = map(lambda x: x.word_normed if x is not None else None, tokens)\n",
      "                \tkeysent = hasher.encode(''.join(map(lambda x: str(x.encode('ascii','ignore')) if x is not None else '', tokens)))\n",
      "\n",
      "                \tif keysent in cache:\n",
      "#\t\t\tif False:\n",
      "                        \treturn cache[keysent]\n",
      "                \telse:\n",
      "                        \tvector = model.context2vec(tokens, [target_index])[0]\n",
      "                        \tcache[keysent] = vector\n",
      "\t\t\t\t#cache.sync()\n",
      "\t\t\t\t#cache.dump()\n",
      "\t\t\t\treturn vector\n",
      "\texcept Exception as E:\n",
      "\t\tprint E\n",
      "\t\ttraceback.print_exc()\n",
      "\t\treturn None\n",
      "\n",
      "\t\t\n",
      "        \n",
      "\treturn None \t\n",
      "\n",
      "\n",
      "    #for target, parse in izip(target_tokens, parses):\n",
      "   #\tif target.word_normed in vocablookup:\n",
      "    #\t        tokens = copy.deepcopy(parse.tokens)\n",
      " #\t\tkeytokens = copy.deepcopy(parse.tokens)\n",
      "#\t        target_index = tokens.index(target)\n",
      "#\t\tkeytokens[target_index] = ''\n",
      "#\t\t\n",
      "#\t        if right_only:\n",
      " #       \t    tokens[target_index] = None\n",
      "#\t\ttokens = map(lambda x: x.word_normed if x is not None else None, tokens)\n",
      "#\n",
      "#\t\tkeysent = ''.join(map(lambda x: x.word_normed if x != '' else '', keytokens))\n",
      "#\t\tif keysent in cache:\n",
      "#\t\t\tcontexts.append(cache[keysent])\n",
      "#\t\telse:\n",
      "#\t        \tvector = model.context2vec(tokens, target_index)\n",
      "#\t\t\tcontexts.append(vector)\n",
      "#\t\t\tcache[keysent] = vector\n",
      "#\t        i += 1\n",
      "#\telse:\n",
      "#\t\tprint target.word, target.word_normed, target.word_normed in vocablookup\n",
      "\n",
      "    contexts = [process_pair(pair) for pair in izip(target_tokens, parses)]\n",
      "    #cache.close()\n",
      "        \n",
      "    contexts = [x for x in contexts if x is not None]\n",
      "\n",
      "    contexts = np.asarray(contexts)\n",
      "    return contexts / np.sqrt((contexts * contexts).sum(axis=1)).reshape(-1,1)\n",
      "\n",
      "\n",
      "def read_relationships(filename, max_relationships):\n",
      "    labels = []\n",
      "    with open(filename) as f:\n",
      "        for i, line in enumerate(f):\n",
      "            if i >= max_relationships:\n",
      "                break\n",
      "            labels.append(line.strip().split(\"\\t\")[0])\n",
      "    return {l: i for i, l in enumerate(labels)}\n",
      "\n",
      "\n",
      "def magnitude(x):\n",
      "    return np.sqrt(np.sum(np.square(x), axis=-1))\n",
      "\n",
      "\n",
      "def fix_pred_scores(pred_scores, candidates):\n",
      "    r = pred_scores.copy()\n",
      "    r[candidates == 0] = -1e9\n",
      "    return r\n",
      "\n",
      "\n",
      "def fix_allvocab_pred_scores(pred_scores, targets):\n",
      "    r = pred_scores.copy()\n",
      "    r[:, 0] = -1e9\n",
      "    for i, t in enumerate(targets):\n",
      "        r[i, t] = -1e9\n",
      "    return r\n",
      "\n",
      "\n",
      "def fix_lemma_problem(pred_scores, targets, space):\n",
      "    from nltk.stem.snowball import EnglishStemmer\n",
      "    es = EnglishStemmer()\n",
      "    r = pred_scores.copy()\n",
      "    lemmas = np.array([es.stem(v) for v in space.vocab])\n",
      "    for i, t in enumerate(targets):\n",
      "        g = es.stem(space.vocab[t])\n",
      "        mask = (lemmas == g)\n",
      "        # print space.vocab[t], np.sum(mask)\n",
      "        r[i][mask] = -1e9\n",
      "        # print r[i][mask]\n",
      "    return r\n",
      "\n",
      "\n",
      "def compute_softmax_oren(space, targets, depmat, candidates, right_only):\n",
      "    targetvecs = space.matrix[targets]  # (2003, 600)\n",
      "    depvecs = space.cmatrix[depmat]\n",
      "    candvecs = space.matrix[candidates]  # (2003, 38, 600)\n",
      "    left = np.einsum('ij,ikj->ik', targetvecs, candvecs)\n",
      "    right = np.einsum('ik,ilk->il', depvecs.sum(axis=1), candvecs)\n",
      "\n",
      "    print \"NAN\"\n",
      "    print True in np.isnan(np.exp(right))\n",
      "    print True in np.isnan(np.exp(left))\n",
      "\n",
      "    print \"INF\"\n",
      "    print True in np.isinf(np.exp(right))\n",
      "    print True in np.isinf(np.exp(left))\n",
      "    \n",
      "    if right_only:\n",
      "        return np.log(normalize(np.exp(right), norm='l1'))\n",
      "    else:\n",
      "        return np.log(normalize(np.exp(left), norm='l1')) + np.log(normalize(np.exp(right), norm='l1'))\n",
      "\n",
      "\n",
      "def compute_softmax_pmi_log(space, targets, depmat, candidates, pspace, pc):\n",
      "\n",
      "    mc = ma.masked_equal(candidates, 0)\n",
      "    right = np.empty((0, pc.shape[1]))\n",
      "    i = 0\n",
      "    pspace.m.data = np.log(pspace.m.data)\n",
      "    for row in depmat:\n",
      "        right = np.append(right, pspace.m[np.ix_(\n",
      "            pc[i], row[row.nonzero()])].todense().sum(axis=1).T, axis=0)\n",
      "        i += 1\n",
      "\n",
      "    right[right > 6] = 6\n",
      "\n",
      "    return np.log(normalize(ma.masked_array(np.exp(right), mask=mc.mask).filled(0), norm='l1'))\n",
      "\n",
      "#@profile\n",
      "def compute_softmax_pmi(space, targets, depmat, candidates, pspace, pc, scale, cutoff, right_only):\n",
      "\n",
      "    mc = ma.masked_equal(candidates, 0)\n",
      "    right = np.empty((0, pc.shape[1]))\n",
      "    i = 0\n",
      "    print pspace.m.shape, pc.shape, depmat.shape, depmat[depmat.nonzero()].shape\n",
      "    # print pc[0,:]\n",
      "    # print candidates[0,:]\n",
      "    # print ma.masked_array(pc,mask=mc.mask).filled(0)\n",
      "    # print mc.shape, candidates.shape, pc.shape\n",
      "    #Should do some assert here to confirm that things are all good\n",
      "\n",
      "    for row in depmat:\n",
      "        # if row.any():\n",
      "        #    right = np.append(right, np.max(pspace.m[np.ix_(pc[i],row[row.nonzero()])].todense(),axis=1).T, axis=0)\n",
      "        # else:\n",
      "        #    right =  np.append(right, np.prod(pspace.m[np.ix_(pc[i],row[row.nonzero()])].todense(),axis=1).T, axis=0)\n",
      "        #    print \"HERE:\"\n",
      "        right = np.append(\n",
      "            right, scale * np.prod(pspace.m[np.ix_(pc[i], row[row.nonzero()])].todense(), axis=1).T, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        i += 1\n",
      "\n",
      "    #rows = np.arange(depmat.shape[0])\n",
      "    #right2 = np.prod(pspace.m[pc,depmat].todense(),axis=1)\n",
      "    #print right.shape\n",
      "    #print right2.shape\n",
      "    #print np.all(right2 == right)\n",
      "    right[right > cutoff] = cutoff\n",
      "\n",
      "    #mx = ma.masked_array(right,mask=mc.mask).filled(-1000)\n",
      "    # print mx.ravel().shape\n",
      "    # print mx.ravel().shape[1] - np.count_nonzero(np.asarray(mx))\n",
      "\n",
      "    # print\n",
      "    # np.log(normalize(ma.masked_array(np.exp(right),mask=mc.mask).filled(0),\n",
      "    # norm='l1'))\n",
      "\n",
      "    if right_only:\n",
      "        return np.log(normalize(ma.masked_array(np.exp(right), mask=mc.mask).filled(0), norm='l1'))\n",
      "    else:\n",
      "        targetvecs = space.matrix[targets]  # (2003, 600)\n",
      "        candvecs = space.matrix[candidates]\n",
      "\n",
      "        left = np.einsum('ij,ikj->ik', targetvecs, candvecs)\n",
      "        #v1 =  np.var(normalize(np.exp(left), norm='l1'),axis=1,keepdims=True)\n",
      "        # print v1\n",
      "        # print v1.mean()\n",
      "        # print v1.shape\n",
      "        #v2 =  np.var(normalize(ma.masked_array(np.exp(right),mask=mc.mask).filled(0), norm='l1'),axis=1,keepdims=True)\n",
      "        # print v2.mean()\n",
      "        # print v2.shape\n",
      "        # print v2\n",
      "\n",
      "        #w1 = (1/v1 ** 2) / (1/v1 ** 2 + 1/v2 ** 2)\n",
      "        #w2 = (1/v2 ** 2) / (1/v1 ** 2 + 1/v2 ** 2)\n",
      "# return 0.05 * np.log(normalize(np.exp(left), norm='l1')) +  0.95 *\n",
      "# np.log(normalize(ma.masked_array(np.exp(right),mask=mc.mask).filled(0),\n",
      "# norm='l1'))\n",
      "        return np.log(normalize(np.exp(left), norm='l1')) + np.log(normalize(ma.masked_array(np.exp(right), mask=mc.mask).filled(0), norm='l1'))\n",
      "\n",
      "#@profile\n",
      "def compute_softmax_pmi_allvocab(space, targets, depmat, pspace, pc, scale, cutoff, right_only):\n",
      "    i = 0\n",
      "\n",
      "    c = pspace.m.tocsc()\n",
      "    rows = np.arange(pspace.m.shape[0])\n",
      "    right = np.empty((0, pspace.m.shape[0]))\n",
      "    for row in depmat:\n",
      "\t#print np.prod(pspace.m[np.ix_(rows, row[row.nonzero()])].todense(), axis=1).T.shape\n",
      "        right = np.append(\n",
      "            right, scale * np.prod(c[:, row[row.nonzero()]].todense(), axis=1).T, axis=0)\n",
      "        i += 1\n",
      "    right[right > cutoff] = cutoff\n",
      "\n",
      "    #mx = ma.masked_array(right,mask=mc.mask).filled(-1000)\n",
      "    # print mx.ravel().shape\n",
      "    # print mx.ravel().shape[1] - np.count_nonzero(np.asarray(mx))\n",
      "\n",
      "    # print\n",
      "    # np.log(normalize(ma.masked_array(np.exp(right),mask=mc.mask).filled(0),\n",
      "    # norm='l1'))\n",
      "\n",
      "    if right_only:\n",
      "        print normalize(np.exp(right), norm='l1').sum(axis=1)\n",
      "        return np.log(normalize(np.exp(right), norm='l1'))\n",
      "    else:\n",
      "        targetvecs = space.matrix[targets]  # (2003, 600)\n",
      "        left = targetvecs.dot(space.matrix.T)\n",
      "\n",
      "        #v1 =  np.var(normalize(np.exp(left), norm='l1'),axis=1,keepdims=True)\n",
      "        # print v1\n",
      "        # print v1.mean()\n",
      "        # print v1.shape\n",
      "        #v2 =  np.var(normalize(ma.masked_array(np.exp(right),mask=mc.mask).filled(0), norm='l1'),axis=1,keepdims=True)\n",
      "        # print v2.mean()\n",
      "        # print v2.shape\n",
      "        # print v2\n",
      "\n",
      "        #w1 = (1/v1 ** 2) / (1/v1 ** 2 + 1/v2 ** 2)\n",
      "        #w2 = (1/v2 ** 2) / (1/v1 ** 2 + 1/v2 ** 2)\n",
      "# return 0.05 * np.log(normalize(np.exp(left), norm='l1')) +  0.95 *\n",
      "# np.log(normalize(ma.masked_array(np.exp(right),mask=mc.mask).filled(0),\n",
      "# norm='l1'))\n",
      "        return np.log(normalize(np.exp(left), norm='l1')) + np.log(normalize(np.exp(right), norm='l1'))\n",
      "\n",
      "\n",
      "def compute_softmax_pmi_max(space, targets, depmat, candidates, pspace, pc, right_only):\n",
      "\n",
      "    mc = ma.masked_equal(candidates, 0)\n",
      "    right = np.empty((0, pc.shape[1]))\n",
      "    i = 0\n",
      "\n",
      "    for row in depmat:\n",
      "        if row.any():\n",
      "            right = np.append(right, np.max(\n",
      "                pspace.m[np.ix_(pc[i], row[row.nonzero()])].todense(), axis=1).T, axis=0)\n",
      "        else:\n",
      "            right = np.append(right, np.prod(\n",
      "                pspace.m[np.ix_(pc[i], row[row.nonzero()])].todense(), axis=1).T, axis=0)\n",
      "        #right =  np.append(right, .01 * np.prod(pspace.m[np.ix_(pc[i],row[row.nonzero()])].todense(),axis=1).T, axis=0)\n",
      "        # i+=1\n",
      "\n",
      "    right[right > 500] = 500\n",
      "\n",
      "    #mx = ma.masked_array(right,mask=mc.mask).filled(-1000)\n",
      "    # print mx.ravel().shape\n",
      "    # print mx.ravel().shape[1] - np.count_nonzero(np.asarray(mx))\n",
      "\n",
      "    # print\n",
      "    # np.log(normalize(ma.masked_array(np.exp(right),mask=mc.mask).filled(0),\n",
      "    # norm='l1'))\n",
      "    if right_only:\n",
      "        return np.log(normalize(ma.masked_array(np.exp(right), mask=mc.mask).filled(0), norm='l1'))\n",
      "    else:\n",
      "        targetvecs = space.matrix[targets]  # (2003, 600)\n",
      "        candvecs = space.matrix[candidates]\n",
      "\n",
      "        left = np.einsum('ij,ikj->ik', targetvecs, candvecs)\n",
      "\n",
      "        return np.log(normalize(np.exp(left), norm='l1')) + np.log(normalize(ma.masked_array(np.exp(right), mask=mc.mask).filled(0), norm='l1'))\n",
      "\n",
      "\n",
      "def compute_softmax_oren_pmi(space, targets, depmat, candidates, pspace, pc):\n",
      "\n",
      "    mc = ma.masked_equal(candidates, 0)\n",
      "\n",
      "    right = np.empty((0, candidates.shape[1]))\n",
      "\n",
      "    print pspace.m.max()\n",
      "    for row in depmat:\n",
      "\n",
      "        right = np.append(right, pspace.m[np.ix_(\n",
      "            pc[i], row[row.nonzero()])].todense().prod(axis=1).T, axis=0)\n",
      "        i += 1\n",
      "    pspace.m = None\n",
      "\n",
      "    print depmat.shape\n",
      "\n",
      "    targetvecs = space.matrix[targets]\n",
      "    candvecs = space.matrix[candidates]\n",
      "    left = np.einsum('ij,ikj->ik', targetvecs, candvecs)\n",
      "\n",
      "    #mx = ma.masked_array(right,mask=mc.mask).filled(-1000)\n",
      "    # print mx.ravel().shape\n",
      "    # print mx.ravel().shape[1] - np.count_nonzero(np.asarray(mx))\n",
      "\n",
      "    # return np.log(normalize(np.exp(left), norm='l1')) +\n",
      "    # np.log(normalize(np.exp(right), norm='l1'))\n",
      "    return np.log(normalize(np.exp(left), norm='l1')) + np.log(normalize(ma.masked_array(np.exp(right), mask=mc.mask).filled(0), norm='l1'))\n",
      "\n",
      "\n",
      "def compute_softmax_oren_allvocab(space, targets, depmat):\n",
      "    targetvecs = space.matrix[targets]\n",
      "    depvecs = space.cmatrix[depmat]\n",
      "    left = targetvecs.dot(space.matrix.T)\n",
      "    right = depvecs.sum(axis=1).dot(space.matrix.T)\n",
      "    return np.log(normalize(np.exp(left), norm='l1')) + np.log(normalize(np.exp(right), norm='l1'))\n",
      "\n",
      "\n",
      "def compute_mymodel(space, targets, model, depmat, candidates, batch_size=1024):\n",
      "    predvecs = model.predict([depmat, candidates], batch_size=batch_size)\n",
      "    predvecs = normalize(predvecs, norm='l1')\n",
      "    targetvecs = space.matrix[targets]  # (2003, 600)\n",
      "    candvecs = space.matrix[candidates]\n",
      "    ooc = np.exp(np.einsum('ij,ikj->ik', targetvecs, candvecs))\n",
      "    ooc = normalize(ooc, norm='l1', axis=1)\n",
      "    scores = np.log(predvecs) + np.log(ooc)\n",
      "    return scores\n",
      "\n",
      "\n",
      "def compute_mymodel_allwords(space, targets, model, depmat):\n",
      "    candidates = np.repeat([np.arange(len(space.vocab))], len(targets), axis=0)\n",
      "    predvecs = model.predict([depmat, candidates], batch_size=8)\n",
      "    predvecs = np.log(normalize(predvecs, norm='l1'))\n",
      "    targetvecs = space.matrix[targets]\n",
      "    ooc = np.exp(np.dot(targetvecs, space.matrix.T))\n",
      "    ooc = np.log(normalize(ooc, norm='l1', axis=1))\n",
      "    scores = predvecs + ooc\n",
      "    return scores\n",
      "\n",
      "\n",
      "def compute_oren(space, targets, depmat, candidates, right_only, balanced=False):\n",
      "    normspace = space.normalize()\n",
      "\n",
      "    targetvecs = normspace.matrix[targets]  # (2003, 600)\n",
      "    depvecs = normspace.cmatrix[depmat]  # (2003, 14, 600)\n",
      "    candvecs = normspace.matrix[candidates]  # (2003, 38, 600)\n",
      "\n",
      "    left = np.einsum('ij,ikj->ik', targetvecs, candvecs)\n",
      "    if balanced:\n",
      "        left = left * (depmat > 0).sum(axis=1)[:, np.newaxis]\n",
      "    right = np.einsum('ijk,ilk->il', depvecs, candvecs)\n",
      "\n",
      "    if right_only:\n",
      "        pred_scores = right\n",
      "    else:\n",
      "        pred_scores = (left + right)\n",
      "\n",
      "    return pred_scores\n",
      "\n",
      "\n",
      "def compute_oren_allvocab(space, targets, depmat, balanced=False):\n",
      "    normspace = space.normalize()\n",
      "\n",
      "    targetvecs = normspace.matrix[targets]\n",
      "    depvecs = normspace.cmatrix[depmat]\n",
      "    left = targetvecs.dot(normspace.matrix.T)\n",
      "\n",
      "    if balanced:\n",
      "        left = left * (depmat > 0).sum(axis=1)[:, np.newaxis]\n",
      "    right = depvecs.sum(axis=1).dot(normspace.matrix.T)\n",
      "    pred_scores = (left + right)\n",
      "    return pred_scores\n",
      "\n",
      "\n",
      "def compute_ooc(space, targets, candidates):\n",
      "    normspace = space.normalize()\n",
      "    targetvecs = normspace.matrix[targets]\n",
      "    candvecs = normspace.matrix[candidates]\n",
      "\n",
      "    pred_scores = np.einsum('ij,ikj->ik', targetvecs, candvecs)\n",
      "    print pred_scores\n",
      "    return pred_scores\n",
      "\n",
      "\n",
      "def compute_ooc_allvocab(space, targets):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    normspace = space.normalize()\n",
      "    targetvecs = normspace.matrix[targets]\n",
      "\n",
      "    pred_scores = targetvecs.dot(normspace.matrix.T)\n",
      "    return pred_scores\n",
      "\n",
      "\n",
      "def compute_random(candidates):\n",
      "    return np.random.rand(*candidates.shape)\n",
      "\n",
      "\n",
      "def compute_oracle(candidates, scores, space):\n",
      "    normspace = space.normalize()\n",
      "    perfect = np.multiply(space.matrix[candidates], scores[\n",
      "                          :, :, np.newaxis]).sum(axis=1)\n",
      "    perfect = normalize(perfect, axis=1, norm='l2')\n",
      "    pred_scores = np.einsum('ij,ikj->ik', perfect,\n",
      "                            normspace.matrix[candidates])\n",
      "    return pred_scores\n",
      "\n",
      "\n",
      "def compute_oracle_allvocab(scores, space):\n",
      "    normspace = space.normalize\n",
      "    perfect = normalize(scores.dot(space.matrix))\n",
      "    pred_scores = perfect.dot(normspace.matrix.T)\n",
      "    return pred_scores\n",
      "\n",
      "\n",
      "# def gap(pred_scores, true_scores):\n",
      "#    maxcands = pred_scores.shape[1]\n",
      "#    maxi = np.sum(true_scores > 0, axis=1) - 1\n",
      "#\n",
      "#    pred_scores = pred_scores[maxi >= 0]\n",
      "#    true_scores = true_scores[maxi >= 0]\n",
      "#    maxi = maxi[maxi >= 0]\n",
      "#\n",
      "#    ranks = (-pred_scores).argsort(axis=1)\n",
      "#    points_by_pred = np.array([ts[r] for r, ts in izip(ranks, true_scores)])\n",
      "#\n",
      "#    bot = np.arange(maxcands) + 1\n",
      "#    max_points = true_scores.cumsum(axis=1) / bot[np.newaxis,:]\n",
      "#    actual_points = points_by_pred.cumsum(axis=1) / bot[np.newaxis,:]\n",
      "#    numers = np.array([np.sum(ap[pbp > 0]) for ap, pbp in izip(actual_points, points_by_pred)])\n",
      "#    denoms = np.array([np.sum(mp[:i+1]) for i, mp in izip(maxi, max_points)])\n",
      "#\n",
      "#    gaps = numers / denoms\n",
      "#\n",
      "#    return gaps\n",
      "\n",
      "def gap_correct(gold_indices, gold_weights, ranked_candidate_indices):\n",
      "    # generalized average precision\n",
      "    gold_reverse_lookup = {id: i for i, id in enumerate(gold_indices)}\n",
      "    cumsum = 0.0\n",
      "    gap = 0.0\n",
      "    for i, id in enumerate(ranked_candidate_indices):\n",
      "        if id not in gold_reverse_lookup:\n",
      "            continue\n",
      "        subj = gold_reverse_lookup[id]\n",
      "        cumsum += gold_weights[subj]\n",
      "        gap += cumsum / (i + 1)\n",
      "        # rpint \"i = %f, cumsum = %f, gap = %f\" % (i, cumsum, gap)\n",
      "\n",
      "    cumsum = 0.0\n",
      "    denom = 0.0\n",
      "    for i, weight in enumerate(gold_weights):\n",
      "        #cumsum += weight\n",
      "        #denom += cumsum / (i + 1)\n",
      "        denom += np.mean(gold_weights[:i + 1])\n",
      "    # print \"final num = %f, final denom = %f\" % (gap, denom)\n",
      "    gap = gap / denom\n",
      "\n",
      "    return gap\n",
      "\n",
      "def many_gaps(pred_scores, candidates, scores):\n",
      "    assert scores.shape == pred_scores.shape\n",
      "    gaps = []\n",
      "    for i in xrange(pred_scores.shape[0]):\n",
      "        cands = candidates[i]\n",
      "        gw = scores[i]\n",
      "        gold_indices = cands[gw > 0]\n",
      "        gold_weights = gw[gw > 0]\n",
      "        if np.all(gw == 0):\n",
      "            gaps.append(np.nan)\n",
      "            continue\n",
      "        ranks = (-pred_scores[i]).argsort()\n",
      "        ranked_candidate_indices = cands[ranks]\n",
      "        ranked_candidate_indices = ranked_candidate_indices[\n",
      "            ranked_candidate_indices != 0]\n",
      "        gaps.append(gap_correct(gold_indices, gold_weights,\n",
      "                                ranked_candidate_indices))\n",
      "    return np.array(gaps)\n",
      "\n",
      "def align_indexes_model(space, model_reader):\n",
      "  space_order = []\n",
      "  space_subset = []\n",
      "  combined_vocab = []\n",
      "\n",
      "  for word in space.vocab:\n",
      "    if word in model_reader.word2index:\n",
      "      space_order.append(model_reader.word2index[word])\n",
      "      space_subset.append(space.lookup[word])\n",
      "      combined_vocab.append(word)\n",
      "\n",
      "\n",
      "  space.matrix = space.matrix[space_subset]\n",
      "  space.vocab = np.asarray(combined_vocab)\n",
      "  space.lookup = {x:i for i,x in enumerate(combined_vocab)}\n",
      "\n",
      "  model_reader.w = model_reader.w[space_order]\n",
      "  model_reader.index2word = combined_vocab\n",
      "  model_reader.word2index = {x:i for i,x in enumerate(combined_vocab)}\n",
      "\n",
      "  return space, model_reader\n",
      "\n",
      "\n",
      "def align_indexes_explicit(space, explicit):\n",
      "  space_order = []\n",
      "  space_subset = []\n",
      "  combined_vocab = []\n",
      "\n",
      "  for word in space.vocab:\n",
      "    if word in explicit.wi:\n",
      "      space_order.append(explicit.wi[word])\n",
      "      space_subset.append(space.lookup[word])\n",
      "      combined_vocab.append(word)\n",
      "\n",
      "\n",
      "  space.matrix = space.matrix[space_subset]\n",
      "  space.vocab = np.asarray(combined_vocab)\n",
      "  space.lookup = {x:i for i,x in enumerate(combined_vocab)}\n",
      "\n",
      "  explicit.m = explicit.m[space_order]\n",
      "  explicit.iw = combined_vocab\n",
      "  explicit.wi = {x:i for i,x in enumerate(combined_vocab)}\n",
      "\n",
      "  return space, explicit\n",
      "\n",
      "def nanmean(arr):\n",
      "    return np.mean(arr[~np.isnan(arr)])\n",
      "\n",
      "\n",
      "def many_prec1(pred_scores, scores):\n",
      "    assert scores.shape == pred_scores.shape\n",
      "    bestpick = pred_scores.argmax(axis=1)\n",
      "    score_at_best = scores[np.arange(len(scores)), bestpick]\n",
      "    gold_values = (scores.max(axis=1) > 0)\n",
      "    retval = np.array(score_at_best > 0, dtype=np.float) / gold_values\n",
      "    return retval\n",
      "\n",
      "\n",
      "def prec_at_k(pred_scores, scores, k=10, mask=None):\n",
      "    top_preds = np.argpartition(pred_scores, -k, 1)[:, -k:]\n",
      "    num_possible = (scores > 0).sum(axis=1, dtype=np.float).clip(0, k)\n",
      "\n",
      "    top_pred_true = np.array([s[tp] for s, tp in izip(scores, top_preds)])\n",
      "    num_right = (top_pred_true > 0).sum(axis=1, dtype=np.float)\n",
      "    num_right[num_possible == 0] = np.nan\n",
      "    return num_right / k\n",
      "\n",
      "#from common import *\n",
      "#from nn import my_load_weights\n",
      "\n",
      "#@profile\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser('Performs lexical substitution')\n",
      "    parser.add_argument('--model', '-m')\n",
      "    parser.add_argument('--data', '-d')\n",
      "    parser.add_argument('--pmi', '-p')\n",
      "    parser.add_argument('--allvocab', action='store_true')\n",
      "    parser.add_argument('--baseline', choices=('oren', 'random', 'ooc',\n",
      "                                               'oracle', 'orensm', 'baloren', 'orenpmi', 'pmimax', 'context2vec'))\n",
      "    parser.add_argument('--save')\n",
      "    parser.add_argument('--semeval')\n",
      "    parser.add_argument('--random', action='store_true')\n",
      "    parser.add_argument('--scale')\n",
      "    parser.add_argument('--cutoff')\n",
      "    parser.add_argument('--right', action='store_true')\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    if (args.model and args.baseline) or (not args.model and not args.baseline):\n",
      "        raise ValueError(\"Please supply exactly one of model or baseline.\")\n",
      "    if args.semeval and not args.allvocab:\n",
      "        raise ValueError(\n",
      "            \"Need to evaluate on allvocab to output semeval predictions.\")\n",
      "\n",
      "    if not args.data:\n",
      "        raise ValueError(\"You must specify a data folder\")\n",
      "\n",
      "    if not args.scale:\n",
      "        args.scale = 1.0\n",
      "\n",
      "    if not args.cutoff:\n",
      "        args.cutoff = 700\n",
      "\n",
      "    # load the data\n",
      "    semeval = LexsubData(args.data)\n",
      "    space = utdeftvs.load_numpy(\n",
      "        \"data/lexsub_embeddings_withcontexts.npz\", True)\n",
      "\n",
      "\n",
      "    if args.pmi:\n",
      "\tpspace = Explicit(args.pmi, False)\n",
      "\tspace,pspace = align_indexes_explicit(space,pspace)\n",
      "\t_, _, pc, _ = semeval.generate_matrices(pspace.wi)\n",
      "\n",
      "    #relations = read_relationships(\"/work/01813/roller/maverick/nnexp/relations.txt\", 1000)\n",
      "    #model = ctxpredict.models.get_model(\"2d\", space, len(relations), space.matrix.shape[1])\n",
      "    # load the space\n",
      "    #space = utdeftvs.load_numpy(\"/scratch/cluster/roller/spaces/giga+bnc+uk+wiki2015/output/dependency.svd300.ppmi.250k.1m.npz\", True)\n",
      "    #space = utdeftvs.load_numpy(\"/scratch/cluster/roller/spaces/giga+bnc+uk+wiki2015/dependency/output/dependency.w2v500.top250k.top1m.npz\", True)\n",
      "    #space = utdeftvs.load_numpy(\"/scratch/cluster/roller/spaces/levy/lexsub_embeddings.npz\", True)\n",
      "    # need to map our vocabulary to their indices\n",
      "\n",
      "    if args.baseline and args.baseline != 'context2vec':\n",
      "\t    ids, targets, candidates, scores = semeval.generate_matrices(space.lookup)\n",
      "\t    depmat = dependencies_to_indices(\n",
      "\t        semeval.tokens, semeval.parses, space.clookup, space)\n",
      "    #print targets.shape\n",
      "    print \"Done preprocessing\"\n",
      "\n",
      "    if args.allvocab:\n",
      "        allvocab_scores = np.zeros((len(targets), len(space.vocab)))\n",
      "        for i in xrange(len(targets)):\n",
      "            for j in xrange(candidates.shape[1]):\n",
      "                c = candidates[i, j]\n",
      "                s = scores[i, j]\n",
      "                if s > 0:\n",
      "                    allvocab_scores[i, c] = s\n",
      "    allvocab_pred_scores = np.zeros(len(space.vocab))\n",
      "    if args.baseline:\n",
      "        print \"Computing baseline %s\" % args.baseline\n",
      "        if args.baseline == 'oren':\n",
      "            pred_scores = compute_oren(\n",
      "                space, targets, depmat, candidates, args.right)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            if args.allvocab:\n",
      "                allvocab_pred_scores = compute_oren_allvocab(\n",
      "                    space, targets, depmat)\n",
      "        elif args.baseline == 'baloren':\n",
      "            pred_scores = compute_oren(\n",
      "                space, targets, depmat, candidates, args.right, balanced=True)\n",
      "            if args.allvocab:\n",
      "                allvocab_pred_scores = compute_oren_allvocab(\n",
      "                    space, targets, depmat, balanced=True)\n",
      "        elif args.baseline == 'orensm':\n",
      "            pred_scores = compute_softmax_oren(\n",
      "                space, targets, depmat, candidates, args.right)\n",
      "            if args.allvocab:\n",
      "                allvocab_pred_scores = compute_softmax_oren_allvocab(\n",
      "                    space, targets, depmat)\n",
      "        elif args.baseline == 'ooc':\n",
      "            pred_scores = compute_ooc(space, targets, candidates)\n",
      "            if args.allvocab:\n",
      "                allvocab_pred_scores = compute_ooc_allvocab(space, targets)\n",
      "        elif args.baseline == 'random':\n",
      "            pred_scores = compute_random(candidates)\n",
      "        elif args.baseline == 'oracle':\n",
      "            pred_scores = compute_oracle(candidates, scores, space)\n",
      "        elif args.baseline == 'orenpmi':\n",
      "\n",
      "\n",
      "\n",
      "            depmat = dependencies_to_indices(\n",
      "                semeval.tokens, semeval.parses, pspace.ci, pspace)\n",
      "            pred_scores = compute_softmax_pmi(space, targets, depmat, candidates, pspace, pc, float(\n",
      "                args.scale), int(args.cutoff), args.right)\n",
      "\t    if args.allvocab:\n",
      "\t\tallvocab_pred_scores = compute_softmax_pmi_allvocab(space,targets,depmat,pspace,pc, float(\n",
      "\t\targs.scale), int(args.cutoff), args.right)\n",
      "        elif args.baseline == 'pmimax':\n",
      "            #pspace = Explicit(args.pmi, False)\n",
      "            #space,pspace = sync_indexes_explicit(space,pspace)\n",
      "\n",
      "            #ids, targets, candidates, scores = semeval.generate_matrices(space.lookup)\n",
      "            #_, _, pc, _ = semeval.generate_matrices(pspace.wi)\n",
      "\n",
      "            depmat = dependencies_to_indices(\n",
      "                semeval.tokens, semeval.parses, pspace.ci, pspace)\n",
      "            pred_scores = compute_softmax_pmi_max(\n",
      "                space, targets, depmat, candidates, pspace, pc, args.right)\n",
      "        elif args.baseline == \"context2vec\":\n",
      "            space = utdeftvs.load_numpy(\n",
      "             \"data/lexsub_embeddings_withcontexts.npz\", True)\n",
      "\n",
      "            model_reader = ModelReader(\n",
      "                \"/home/bryan/context2vec/model/context2vec.ukwac.model.params\")\n",
      "\n",
      "            space,model_reader = align_indexes_model(space,model_reader)\n",
      "            ids, targets, candidates, scores = semeval.generate_matrices(space.lookup)\n",
      "            allvocab_pred_scores = np.zeros(len(space.vocab))\n",
      "            w = model_reader.w\n",
      "            word2index = model_reader.word2index\n",
      "            index2word = model_reader.index2word\n",
      "            model = model_reader.model\n",
      "\n",
      "\n",
      "            contexts = parses_to_context2vec(\n",
      "                semeval.tokens, semeval.parses, args.right, model,word2index)\n",
      "            _, targs, pc, _ = semeval.generate_matrices(word2index)\n",
      "\t    rows = np.arange(pc.shape[0])\n",
      "\t    #print w.shape, targs.shape\n",
      "\t    #del semeval\n",
      "\t    if args.right:\n",
      "\t            allvocab_pred_scores = w.dot(contexts.T)\n",
      "\t    else:\n",
      "\t    \t    target_similarity = w.dot(w[targs].T)\n",
      "\t\t    target_similarity[target_similarity<0] = 0.0\n",
      "\t\t    context_similarity = w.dot(contexts.T)\n",
      "\t\t    context_similarity[context_similarity<0] = 0.0\n",
      "\t\t    context_similarity = context_similarity.astype(np.float32, copy=False)\n",
      "                    target_similarity = target_similarity.astype(np.float32, copy=False)\n",
      "\t\t    target_similarity[target_similarity<0] = 0.0\n",
      "\t\t    context_similarity[context_similarity<0] = 0.0\n",
      "\t            del w\n",
      "\t\t    del model\n",
      "\t            del model_reader\n",
      "\t\t    allvocab_pred_scores =  ne.evaluate(\"target_similarity * context_similarity\")\n",
      "\t\t    \n",
      "            #print np.asarray(contexts).shape, allvocab_pred_scores.shape\n",
      "            del contexts\n",
      "\t    del context_similarity\n",
      "\t    del target_similarity\n",
      "#            rows = np.arange(pc.shape[0])\n",
      "#            print rows.max()\n",
      " #           print rows.shape[0]\n",
      "            # Memory Issue Below\n",
      "\t    \n",
      "            pred_scores = allvocab_pred_scores.T[rows[:, None], pc]\n",
      "            if args.allvocab:\n",
      "              allvocab_scores = np.zeros((len(targets), len(space.vocab)))\n",
      "              for i in xrange(len(targets)):\n",
      "                for j in xrange(candidates.shape[1]):\n",
      "                  c = candidates[i, j]\n",
      "                  s = scores[i, j]\n",
      "                  if s > 0:\n",
      "                      allvocab_scores[i, c] = s\n",
      "            allvocab_pred_scores = allvocab_pred_scores.T\n",
      "\t    #print pred_scores.shape\n",
      "\t    #print targets.shape\n",
      "\t    #print candidates.shape\n",
      "\t    space.matrix = None\n",
      "\t    space.cmatrix = None\n",
      "        else:\n",
      "            pred_scores = np.zeros(candidates.shape)\n",
      "        modelname = \"baseline\"\n",
      "        modelinfo = args.baseline\n",
      "    elif args.model:\n",
      "        model = my_model_from_json(args.model + \"/model.json\")\n",
      "        filename = sorted(os.listdir(args.model))[-1]\n",
      "        my_load_weights(model, \"%s/%s\" % (args.model, filename))\n",
      "\n",
      "        pred_scores = compute_mymodel(\n",
      "            space, targets, model, depmat, candidates)\n",
      "        pred_scores = fix_pred_scores(pred_scores, candidates)\n",
      "        if args.allvocab:\n",
      "            allvocab_pred_scores = compute_mymodel_allwords(\n",
      "                space, targets, model, depmat)\n",
      "        modelname = args.model\n",
      "        modelinfo = filename\n",
      "    else:\n",
      "        raise ValueError(\"Not given model or baseline to compute...\")\n",
      "\n",
      "    # make sure we're not guessing the target, or the empty vector\n",
      "#    print pred_scores.shape\n",
      "#    print np.exp(pred_scores).sum(axis=1)\n",
      "#    print np.exp(pred_scores).sum(axis=1).shape\n",
      "    pred_scores = fix_pred_scores(pred_scores, candidates)\n",
      "    if args.allvocab:\n",
      "        allvocab_pred_scores = fix_allvocab_pred_scores(\n",
      "            allvocab_pred_scores, targets)\n",
      "        allvocab_pred_scores = fix_lemma_problem(\n",
      "            allvocab_pred_scores, targets, space)\n",
      "\n",
      "    if args.baseline == \"context2vec\":\n",
      "\t    pred_scores[pred_scores < 0] = 0.0\n",
      "\t    pred_scores = normalize(pred_scores,norm='l1')\n",
      "    else:\n",
      "\t    pred_scores = normalize(np.exp(pred_scores),norm='l1')\n",
      "\t    \n",
      "    # compute evaluations\n",
      "    gaps = many_gaps(pred_scores, candidates, scores)\n",
      "    prec1s = many_prec1(pred_scores, scores)\n",
      "    prec3s = prec_at_k(pred_scores, scores, 3)\n",
      "    # allvocab is slow; only compute that if we have to\n",
      "    if args.allvocab:\n",
      "        prec1s_av = many_prec1(allvocab_pred_scores, allvocab_scores)\n",
      "        prec3s_av = prec_at_k(allvocab_pred_scores, allvocab_scores, 3)\n",
      "    else:\n",
      "        prec1s_av = np.zeros(len(targets))\n",
      "        prec3s_av = np.zeros(len(targets))\n",
      "\n",
      "    print (\"%s\\t%s\\t%s\\tgap %.3f\\tp@1 %.3f\\tp@3 %.3f\\tp@1av %.3f\\tp@3av %.3f\" %\n",
      "           (args.data, modelname, modelinfo,\n",
      "            nanmean(gaps), nanmean(prec1s), nanmean(prec3s), nanmean(prec1s_av), nanmean(prec3s_av)))\n",
      "\n",
      "    if args.save:\n",
      "\t#print semeval.idents\n",
      "        with open(args.save, 'w') as f:\n",
      "            f.write('\\t'.join(['ident', 'target', 'gap', 'p@1', 'p@3',\n",
      "                               'p@1av', 'p@3av', 'sentence', 'gold', 'predicted']))\n",
      "            f.write('\\n')\n",
      "            for i in xrange(len(semeval.idents)):\n",
      "                ident = semeval.idents[i]\n",
      "                target = semeval.targets[i]\n",
      "                parse = semeval.parses[i]\n",
      "                scores_i = scores[i]\n",
      "                pred_scores_i = pred_scores[i]\n",
      "                candidates_i = candidates[i]\n",
      "                gap = gaps[i]\n",
      "                prec1 = prec1s[i]\n",
      "                prec3 = prec3s[i]\n",
      "                prec1av = prec1s_av[i]\n",
      "                prec3av = prec3s_av[i]\n",
      "\n",
      "                sentence = \" \".join(t.word_normed for t in parse.tokens)\n",
      "\n",
      "                score_string = \" \".join(\"%s:%3.1f\" % (space.vocab[c], s) for c, s in zip(\n",
      "                    candidates_i, scores_i)[:10] if c != 0)\n",
      "                pred_string = \" \".join(\"%s:%2.10f\" % (space.vocab[c], p) for c, p in revsorted(\n",
      "                    zip(candidates_i, pred_scores_i))[:10] if c != 0)\n",
      "                outline = '\\t'.join([str(ident), target, str(gap), str(prec1), str(\n",
      "                    prec3), str(prec1av), str(prec3av), sentence, score_string, pred_string])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                outline = unidecode(outline)\n",
      "                f.write(outline)\n",
      "                f.write('\\n')\n",
      "\n",
      "        with open(args.save + \".answers\", 'w') as f:\n",
      "            for i in xrange(len(semeval.idents)):\n",
      "                target = semeval.targets[i]\n",
      "                parse = semeval.parses[i]\n",
      "                sentence = \" \".join(t.word_normed for t in parse.tokens)\n",
      "                if i != 0:\n",
      "                    parseBefore = semeval.parses[i - 1]\n",
      "                    sentenceBefore = \" \".join(\n",
      "                        t.word_normed for t in parseBefore.tokens)\n",
      "                else:\n",
      "                    sentenceBefore = \"\"\n",
      "\n",
      "                if i + 1 != len(semeval.idents):\n",
      "                    parseAfter = semeval.parses[i + 1]\n",
      "                    sentenceAfter = \" \".join(\n",
      "                        t.word_normed for t in parseAfter.tokens)\n",
      "                else:\n",
      "                    sentenceAfter = \"\"\n",
      "                if sentence == sentenceAfter or sentence == sentenceBefore:\n",
      "                    if i % 2 == 0:\n",
      "                        target = target + \"-\" + semeval.targets[i + 1]\n",
      "                    else:\n",
      "                        target = semeval.targets[i - 1] + \"-\" + target\n",
      "                pred_scores_i = pred_scores[i]\n",
      "                candidates_i = candidates[i]\n",
      "#                end = 10\n",
      "\t\tend = 3\n",
      "                outline = \",\".join(\"%s\" % (space.vocab[c]) for c, p in revsorted(\n",
      "                    zip(candidates_i, pred_scores_i))[:end] if c != 0)\n",
      "                outline = unidecode(target + \" \" + outline)\n",
      "                f.write(outline)\n",
      "                f.write('\\n')\n",
      "\tif args.random:\n",
      "\t  with open(args.save + \".answers.random\", 'w') as f:\n",
      "            for i in xrange(len(semeval.idents)):\n",
      "                target = semeval.targets[i]\n",
      "                parse = semeval.parses[i]\n",
      "                sentence = \" \".join(t.word_normed for t in parse.tokens)\n",
      "                if i != 0:\n",
      "                    parseBefore = semeval.parses[i - 1]\n",
      "                    sentenceBefore = \" \".join(\n",
      "                        t.word_normed for t in parseBefore.tokens)\n",
      "                else:\n",
      "                    sentenceBefore = \"\"\n",
      "\n",
      "                if i + 1 != len(semeval.idents):\n",
      "                    parseAfter = semeval.parses[i + 1]\n",
      "                    sentenceAfter = \" \".join(\n",
      "                        t.word_normed for t in parseAfter.tokens)\n",
      "                else:\n",
      "                    sentenceAfter = \"\"\n",
      "                if sentence == sentenceAfter or sentence == sentenceBefore:\n",
      "                    if i % 2 == 0:\n",
      "                        target = target + \"-\" + semeval.targets[i + 1]\n",
      "                    else:\n",
      "                        target = semeval.targets[i - 1] + \"-\" + target\n",
      "                pred_scores_i = pred_scores[i]\n",
      "                candidates_i = candidates[i]\n",
      "#\t\ttry:\n",
      "#\t\t\tsortd = np.sort(np.exp(pred_scores[i]))[::-1]\n",
      "#\t\t\trands = np.sort(np.random.random(pred_scores[i].shape))\n",
      "#\t\t\tbins = sortd > rands\n",
      "#\t\t\t# I DONT LIKE THIS IT SEEMS FISHY, ESP THE SORTING OF RANDOM NUMBERS.....\n",
      "#\t\t\tif args.baseline != \"context2vec\":\n",
      "#\t\t\t\tend = ((np.sort(pred_scores[i])[::-1]) < np.random.random(pred_scores[i].shape)).tolist().index(True) + 1\n",
      "#\t\t\telse:\n",
      "#\t\t\t\tend = len(pred_scores_i[pred_scores_i > 0])\n",
      "#\t\texcept:\n",
      "#\t\t\tend = 1\n",
      "\t\tend = random.randint(3, 10)\n",
      "\t\toutline = \",\".join(\"%s\" % (space.vocab[c]) for c, p in revsorted(\n",
      "        \t    zip(candidates_i, pred_scores_i))[:end] if c != 0)\n",
      "\t\t\t\n",
      "                outline = unidecode(target + \" \" + outline)\n",
      "\t\t\n",
      "                f.write(outline)\n",
      "                f.write('\\n')\n",
      "\n",
      "\n",
      "    if args.semeval:\n",
      "        bestf = open(args.semeval + \".best\", \"w\")\n",
      "        bootf = open(args.semeval + \".boot\", \"w\")\n",
      "        bests = allvocab_pred_scores.argmax(axis=1)\n",
      "        boots = np.argpartition(allvocab_pred_scores, -10, 1)[:, -10:]\n",
      "        for i in xrange(len(semeval.idents)):\n",
      "            ident = semeval.idents[i]\n",
      "            ot = semeval.original_targets[i]\n",
      "            bestf.write(\"%s %d :: %s\\n\" % (ot, ident, space.vocab[bests[i]]))\n",
      "            bootf.write(\"%s %d ::: %s\\n\" %\n",
      "                        (ot, ident, \";\".join(space.vocab[boots[i]])))\n",
      "        bestf.close()\n",
      "        bootf.close()\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "from numba import jit\n",
      "\n",
      "@jit\n",
      "def f(x,y):\n",
      "\treturn x * y\n",
      "\n",
      "def q(x,y):\n",
      "\treturn x *y\n",
      "\n",
      "from docopt import docopt\n",
      "\n",
      "args = docopt(\"\"\"\n",
      "\tUsage:\n",
      "\t\tpre_van_pairs.py <pair_files>\n",
      "\n",
      "\"\"\")\n",
      "\n",
      "lines = []\n",
      "\n",
      "with open(args['<pair_files>']) as f:\n",
      "\tfor line in f:\n",
      "\t\tlines.append(line.strip().replace(',','-'))\n",
      "\t\n",
      "\n",
      "#for i in range(0,len(lines),5):\n",
      "#\tprint \",\".join(lines[i:i+5])\n",
      "\n",
      "for line in lines:\n",
      "\tbase = [\"easy-hard\",\"small-large\",\"cold-warm\",\"slow-fast\",line]\n",
      "\tprint \",\".join(base)\n",
      "import pandas as pd\n",
      "import sys\n",
      "\n",
      "wn_results = pd.read_csv(sys.argv[1] + '_membership_mturk_results.csv',low_memory=False)\n",
      "\n",
      "words = wn_results.columns[wn_results.columns.str.contains('Answer') & (wn_results.columns.str.contains('pos') == False) & (wn_results.columns.str.contains('order') == False)].tolist()\n",
      "counts = {}\n",
      "for word in words:\n",
      "    counts[word] = wn_results[word].value_counts().to_dict()\n",
      "\n",
      "checked = pd.DataFrame(counts).loc[\"on\"]\n",
      "\n",
      "scales = pd.read_csv(sys.argv[1] + '.nopart.ensemble_filtered_scales.csv',index_col=0)\n",
      "scale_idx = scales.index.tolist()\n",
      "\n",
      "\n",
      "counts = []\n",
      "word_counts = []\n",
      "for idx in scale_idx:\n",
      "    results = checked[checked.index.str.endswith(\"_\"+str(idx))]\n",
      "    scales.loc[idx,'removed'] = ','.join(map(lambda x: x.split('_')[0].split('.')[1], results.index.tolist()))\n",
      "    scales.loc[idx,'removed_count'] = ','.join(map(str,results.values))\n",
      "scales.to_csv('membership.csv')\n",
      "better = scales[scales.apply(lambda x: x['word2'] not in x['removed'] and x['word1'] not in x['removed'], axis=1)]\n",
      "scales['final_output'] = scales.apply(lambda x: list(set(x['0_x'].split(',') + [x['word1']] + [x['word2']]) - set(x['removed'].split(','))),axis=1)\n",
      "scales['percent'] = scales.apply(lambda x: len(x['final_output'])/(len(x['0_x'].split(',')) + 2.0),axis=1)\n",
      "scales['removed_count'].apply(lambda x: counts.extend(x.split(',')) if x else None)\n",
      "scales['final_output'].apply(lambda x: word_counts.append(len(x)))\n",
      "scales['final_output_len'] = scales['final_output'].apply(len)\n",
      "scales.to_csv(sys.argv[1] + '.scales_filtered_by_mturk.csv')\n",
      "print scales['percent'].describe()\n",
      "print pd.Series(counts).value_counts()\n",
      "print pd.Series(counts).value_counts().sum()\n",
      "print sum(word_counts)\n",
      "print scales['final_output_len'].mean(), \"Average number of words per scale\"\n",
      "print scales[scales['final_output_len'] > 1]['final_output_len'].mean()\n",
      "import lucene\n",
      "from random import random, sample\n",
      "lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n",
      "from lucene import VERSION\n",
      "from org.apache.lucene import analysis,util\n",
      "from lupyne import engine\n",
      "import pathos.multiprocessing as mp\n",
      "import dask\n",
      "from dask.delayed import delayed, compute\n",
      "import dask.multiprocessing\n",
      "import dask.threaded\n",
      "from itertools import product\n",
      "from random import shuffle\n",
      "import time\n",
      "\n",
      "def tokenizer(reader):\n",
      "    return analysis.standard.StandardTokenizer(util.Version.LATEST, reader)\n",
      "\n",
      "\n",
      "myAnalyzer = engine.indexers.Analyzer( tokenizer,\n",
      "     analysis.standard.StandardFilter, analysis.LowerCaseFilter)\n",
      "\n",
      "\n",
      "indexer = engine.IndexSearcher('/home/bryan/ukwakSentenceIndex',analyzer=myAnalyzer)\n",
      "\n",
      "\n",
      "pool = mp.ThreadingPool(nodes=8)\n",
      "\n",
      "@profile\n",
      "def getText(i):\n",
      "\tenv = lucene.getVMEnv()\n",
      "    \tenv.attachCurrentThread()\n",
      "\td = indexer.doc(i.doc)\n",
      "    \treturn d['text']\n",
      "\n",
      "@profile\n",
      "def slow(text):\n",
      "\tl = []\n",
      "\tfor hit in indexer.search('text:\"%s\"' % text,scores=False):\n",
      "\t\tl.append(hit)\n",
      "\n",
      "\n",
      "@profile\n",
      "def pathosmp(text):\n",
      "\tpool = mp.ProcessingPool(nodes=8)\t\n",
      "\thits = pool.map(getText,map(lambda y: y.doc , list(indexer.search('text:\"%s\"' % text,scores=False).scoredocs)))\n",
      "\tpool.close()\n",
      "\tpool.terminate()\n",
      "@profile\n",
      "def pathosthread(text):\n",
      "#\tpool = mp.ThreadingPool(nodes=8)\n",
      "#        hits = pool.map(getText,map(lambda y: y.doc , list(indexer.search('text:\"%s\"' % text ,scores=False).scoredocs)))\n",
      "\thits = pool.map(getText,list(indexer.search('text:\"%s\"' % text ,scores=False).scoredocs))\n",
      "#        pool.close()\n",
      "#        pool.terminate()\n",
      "\n",
      "@profile\n",
      "def traditional(text):\n",
      "#\thits = map(getText,map(lambda y: y.doc , list(indexer.search('text:\"%s\"' %text,scores=False).scoredocs)))\n",
      "\thits = map(getText,list(indexer.search('text:\"%s\"' %text,scores=False).scoredocs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "@profile\n",
      "def daskmp(text):\n",
      "\thits = [delayed(getText)(i) for i in map(lambda y: y.doc , list(indexer.search('text:\"%s\"' %text,scores=False).scoredocs))]\n",
      "\tcompute(*hits,get=dask.multiprocessing.get,num_workers=8)\n",
      "\n",
      "@profile\n",
      "def daskthread(text):\n",
      "        hits = [delayed(getText)(i) for i in map(lambda y: y.doc , list(indexer.search('text:\"%s\"' %text,scores=False).scoredocs))]\n",
      "        compute(*hits,get=dask.threaded.get,num_workers=8)\n",
      "\n",
      "\n",
      "words = ['large', 'big','small','good','bad','pretty','ugly','some','all']\n",
      "cons = ['and', 'or']\n",
      "combos = list(map(list,product(words,cons)))\n",
      "\n",
      "for i in range(20):\n",
      "\tshuffle(combos)\n",
      "\tfor pair in combos:\n",
      "\t\ttext1 = ' '.join(pair)\n",
      "\t\ttext2 = ' '.join(pair[::-1])\n",
      "\t\t#daskthread(text1)\n",
      "\t\t#time.sleep(1)\n",
      "\t\t#daskthread(text2)\n",
      "\t\t#time.sleep(1)\n",
      "\t\t#pathosmp(\"large and\")\n",
      "\t\tpathosthread(text1)\n",
      "\t\ttime.sleep(1)\n",
      "\t\tpathosthread(text2)\n",
      "\t\ttime.sleep(1)\n",
      "\t\ttraditional(text1)\n",
      "\t\ttime.sleep(1)\n",
      "\t\ttraditional(text2)\n",
      "\t\ttime.sleep(1)\n",
      "\t\tslow(text1)\n",
      "\t\ttime.sleep(1)\n",
      "\t\tslow(text2)\n",
      "#daskmp(\"large and\")\n",
      "\n",
      "pool.close()\n",
      "pool.terminate()\n",
      "import sys\n",
      "import shutil\n",
      "import os\n",
      "from glob import iglob\n",
      "import pandas as pd\n",
      "\n",
      "seeds = pd.read_csv('pairs_list.csv',header=None).values.tolist()\n",
      "seeds = map(lambda x: map(lambda y: y.split('-'),x),seeds)\n",
      "\n",
      "dir = sys.argv[1]\n",
      "\n",
      "for location in iglob(os.path.join(dir,\"adj*adjscale*\")):\n",
      "\n",
      "\ttrial = int(location.split('_')[1])\n",
      "\tignore = []\n",
      "#\tignore = ['data/adj.adjscale_302','data/adj.expand.adjscale_319','data/adj.adjscale_922','data/adj.expand.adjscale_886','data/adj.adjscale_256',\n",
      "#\t\t  'data/adj.expand.adjscale_529','data/adj.expand.adjscale_525','data/adj.expand.adjscale_709','data/adj.adjscale_791','data/adj.adjscale_903',\n",
      "#\t\t  'data/adj.adjscale_984','data/adj.expand.adjscale_929','data/adj.expand.oneword.adjscale_723','data/adj.expand.adjscale_25',\n",
      "#\t\t  'data/adj.adjscale_803','data/adj.adjscale_403','data/adj.expand.oneword.adjscale_24']\n",
      "\tif trial > 500 or 'oneword' in location or location in ignore:\n",
      "\t\tcontinue\n",
      "\n",
      "\n",
      "        print location\n",
      "\tseed = seeds[trial]\n",
      "\tshutil.copyfile(os.path.join(location,\"candidates\"),os.path.join(location,\"candidates.bak\"))\n",
      "\twith open(os.path.join(location,\"candidates.bak\")) as f:\n",
      "\t\twith open(os.path.join(location,\"candidates\"),'w') as g:\n",
      "\t\t\tfor line in f:\n",
      "\t\t\t\ttarget, candidates_str = line.strip().split(\"::\")\n",
      "\t\t\t\tword = target.replace('.j','')\n",
      "\t\t\t\tpair = [x for x in seed if word in x][0]\n",
      "\t\t\t\tcandidates = list(set(candidates_str.split(\";\"))-set(pair))\n",
      "\t\t\t        g.write(\"%s.j::%s\\n\" % (word,\";\".join(candidates)))\t\n",
      "\t\n",
      "\t\n",
      "\n",
      "import sys\n",
      "import shutil\n",
      "import os\n",
      "from glob import iglob\n",
      "import pandas as pd\n",
      "\n",
      "seeds = pd.read_csv('test_pairs_augmented.csv',header=None).values.tolist()\n",
      "seeds = map(lambda x: map(lambda y: y.split('-'),x),seeds)\n",
      "\n",
      "dir = sys.argv[1]\n",
      "\n",
      "for location in iglob(os.path.join(dir,\"adj*adjscale*\")):\n",
      "\n",
      "\ttrial = int(location.split('_')[1])\n",
      "\tignore = []\n",
      "#\tignore = ['data/adj.adjscale_302','data/adj.expand.adjscale_319','data/adj.adjscale_922','data/adj.expand.adjscale_886','data/adj.adjscale_256',\n",
      "#\t\t  'data/adj.expand.adjscale_529','data/adj.expand.adjscale_525','data/adj.expand.adjscale_709','data/adj.adjscale_791','data/adj.adjscale_903',\n",
      "#\t\t  'data/adj.adjscale_984','data/adj.expand.adjscale_929','data/adj.expand.oneword.adjscale_723','data/adj.expand.adjscale_25',\n",
      "#\t\t  'data/adj.adjscale_803','data/adj.adjscale_403','data/adj.expand.oneword.adjscale_24']\n",
      "\tif trial > 500 or 'oneword' in location or location in ignore:\n",
      "\t\tcontinue\n",
      "\n",
      "\n",
      "        print location\n",
      "\tseed = seeds[trial]\n",
      "\tshutil.copyfile(os.path.join(location,\"candidates\"),os.path.join(location,\"candidates.bak\"))\n",
      "\twith open(os.path.join(location,\"candidates.bak\")) as f:\n",
      "\t\twith open(os.path.join(location,\"candidates\"),'w') as g:\n",
      "\t\t\tfor line in f:\n",
      "\t\t\t\ttarget, candidates_str = line.strip().split(\"::\")\n",
      "\t\t\t\tword = target.replace('.j','')\n",
      "\t\t\t\tpair = [x for x in seed if word in x][0]\n",
      "\t\t\t\tcandidates = list(set(candidates_str.split(\";\"))-set(pair))\n",
      "\t\t\t        g.write(\"%s.j::%s\\n\" % (word,\";\".join(candidates)))\t\n",
      "\t\n",
      "\t\n",
      "\n",
      "import sys\n",
      "import shutil\n",
      "import os\n",
      "from glob import iglob\n",
      "import pandas as pd\n",
      "\n",
      "seeds = pd.read_csv('test_pairs.csv',header=None).values.tolist()\n",
      "seeds = map(lambda x: map(lambda y: y.split('-'),x),seeds)\n",
      "\n",
      "dir = sys.argv[1]\n",
      "\n",
      "for location in iglob(os.path.join(dir,\"adj*adjscale*\")):\n",
      "\n",
      "\ttrial = int(location.split('_')[1])\n",
      "\tignore = []\n",
      "#\tignore = ['data/adj.adjscale_302','data/adj.expand.adjscale_319','data/adj.adjscale_922','data/adj.expand.adjscale_886','data/adj.adjscale_256',\n",
      "#\t\t  'data/adj.expand.adjscale_529','data/adj.expand.adjscale_525','data/adj.expand.adjscale_709','data/adj.adjscale_791','data/adj.adjscale_903',\n",
      "#\t\t  'data/adj.adjscale_984','data/adj.expand.adjscale_929','data/adj.expand.oneword.adjscale_723','data/adj.expand.adjscale_25',\n",
      "#\t\t  'data/adj.adjscale_803','data/adj.adjscale_403','data/adj.expand.oneword.adjscale_24']\n",
      "\tif trial > 500 or 'oneword' in location or location in ignore:\n",
      "\t\tcontinue\n",
      "\n",
      "\n",
      "        print location\n",
      "\tseed = seeds[trial]\n",
      "\tshutil.copyfile(os.path.join(location,\"candidates\"),os.path.join(location,\"candidates.bak\"))\n",
      "\twith open(os.path.join(location,\"candidates.bak\")) as f:\n",
      "\t\twith open(os.path.join(location,\"candidates\"),'w') as g:\n",
      "\t\t\tfor line in f:\n",
      "\t\t\t\ttarget, candidates_str = line.strip().split(\"::\")\n",
      "\t\t\t\tword = target.replace('.j','')\n",
      "\t\t\t\tpair = [x for x in seed if word in x][0]\n",
      "\t\t\t\tcandidates = list(set(candidates_str.split(\";\"))-set(pair))\n",
      "\t\t\t        g.write(\"%s.j::%s\\n\" % (word,\";\".join(candidates)))\t\n",
      "\t\n",
      "\t\n",
      "\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy.stats import wilcoxon\n",
      "\n",
      "model1 = \"orensm\"\n",
      "model2 = \"uniform_dotted\"\n",
      "\n",
      "#column = \"p@3av\"\n",
      "#column = \"p@1av\"\n",
      "column = \"gap\"\n",
      "\n",
      "data = \"semeval_all\"\n",
      "\n",
      "\n",
      "for data in ['semeval_all', 'coinco', 'twsi2']:\n",
      "    out1 = pd.read_table(\"lemma_predictions/%s/%s.tsv\" % (data, model1))\n",
      "    out2 = pd.read_table(\"lemma_predictions/%s/%s.tsv\" % (data, model2))\n",
      "    for column in ['gap', 'p@1av', 'p@3av']:\n",
      "\n",
      "\n",
      "        samples1 = np.array(out1[column])\n",
      "        samples2 = np.array(out2[column])\n",
      "\n",
      "        mask = ~(np.isnan(samples1) | np.isnan(samples2))\n",
      "        samples1 = samples1[mask]\n",
      "        samples2 = samples2[mask]\n",
      "\n",
      "        statistic, pvalue = wilcoxon(samples1, samples2)\n",
      "\n",
      "        print \"Means: %.3f , %.3f\" % (samples1.mean(), samples2.mean())\n",
      "\n",
      "        print \"Comparing %s vs %s on %s [%s]\" % (model1, model2, data, column)\n",
      "        if pvalue < 0.01:\n",
      "            print \"Significant, p = %.3f\" % pvalue\n",
      "        else:\n",
      "            print \"Not significant %.3f\" % pvalue\n",
      "\n",
      "        print\n",
      "##   Parameters\n",
      "## output location\n",
      "## two word search or one\n",
      "## extra data or not?\n",
      "import os\n",
      "from docopt import docopt\n",
      "import re\n",
      "import dill as pickle\n",
      "import shelve\n",
      "import pandas as pd\n",
      "import pathos.multiprocessing as mp\n",
      "\n",
      "import gzip\n",
      "#import StringIO\n",
      "import lucene\n",
      "from random import random, sample\n",
      "lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n",
      "from lucene import VERSION\n",
      "from org.apache.lucene import analysis,util\n",
      "from lupyne import engine\n",
      "from cStringIO import StringIO\n",
      "from itertools import izip\n",
      "\n",
      "def tokenizer(reader):\n",
      "    return analysis.standard.StandardTokenizer(util.Version.LATEST, reader)\n",
      "\n",
      "\n",
      "# In[21]:\n",
      "\n",
      "from spacy.en import English\n",
      "nlp = English(parser=False,tagger=False,entity=False)\n",
      "\n",
      "\n",
      "# In[2]:\n",
      "\n",
      "from string import Template\n",
      "\n",
      "\n",
      "\n",
      "def make_generic(text):\n",
      "\tsent =  text[6:-1]\n",
      "        if sent.startswith('are'):\n",
      "        \tsentence = u\"They %s.\" % sent\n",
      "        elif sent.startswith('is'):\n",
      "                sentence = u\"It %s.\" %sent\n",
      "        else:\n",
      "\t\tsentence = u\"It was %s.\" % sent\n",
      "\n",
      "\treturn sentence\n",
      "\n",
      "def main():\n",
      "\n",
      "  args = docopt(\"\"\"Collects sentences and creates gold standard data from common pairs of scalar adjectives\n",
      "  \n",
      "  Usage:\n",
      "      data_creation.py [options] <output_dir>\n",
      "  \n",
      "  Arguments:\n",
      "      <output_dir> = Directory to output sentences to, in lexical subsitution evaluation format\n",
      "  \n",
      "  Options:\n",
      "      --oneword  Only use the second word in searching for the sentences and in testing the data\n",
      "      --expand  Replace pairs with other pairs in sentences to create more data\n",
      "      --generic  Don't search the corpus, just use the patterns as is, adding \"it was\" to them to make a complete sentence.\n",
      "      --allpairs <pairs_list>  Generate data for all pairs of adjectives, not just the common ones.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      --patterns <pattern_group> Generate data using 'all' patterns, 'symmetric' patterns, or 'adjscale' patterns [default: all]\n",
      "  \"\"\")\n",
      "  \n",
      "  output_dir = args['<output_dir>']\n",
      "  only_one = args['--oneword']\n",
      "  expand = args['--expand']\n",
      "  generic = args['--generic']\n",
      "  allpairs = args['--allpairs']\n",
      "  patterns = args['--patterns']\n",
      "  myAnalyzer = engine.indexers.Analyzer( tokenizer,\n",
      "     analysis.standard.StandardFilter, analysis.LowerCaseFilter)\n",
      "\n",
      "\n",
      "  indexer = engine.IndexSearcher('/home/bryan/ukwakSentenceIndex',analyzer=myAnalyzer)\n",
      "  shelf = shelve.open('ukwakShelf')\n",
      "  #string_shelf = shelve.open('string_shelf')\n",
      "  def getText(i):\n",
      "    env = lucene.getVMEnv()\n",
      "    env.attachCurrentThread()\n",
      "    d = indexer.doc(i.doc)\n",
      "    return d['text']\n",
      "    # Get the arguments\n",
      "    \n",
      "  #indexer.set('name', stored=True)\n",
      "  #indexer.set('text', stored=False)\n",
      "  # In[3]:\n",
      "  \n",
      "  adjScalePatterns = [Template('text:\"$x but not $y\"'),\n",
      "                     Template('text:\"$x if not $y\"'),\n",
      "                     Template('text:\"$x although not $y\"'),\n",
      "                     Template('text:\"$x though not $y\"'),\n",
      "                     Template('text:\"$x and even $y\"'),\n",
      "                     Template('text:\"$x or even $y\"'),\n",
      "                     Template('text:\"$x and almost $y\"'),\n",
      "                     Template('text:\"$x or almost $y\"'),\n",
      "                     Template('text:\"not only $x but $y\"'),\n",
      "                     Template('text:\"not just $x but $y\"'),\n",
      "  \t\tTemplate('text:\"not $x just $y\"'),\n",
      "                  Template('text:\"not $x but just $y\"'),\n",
      "                  Template('text:\"not $x still $y\"'),\n",
      "                  Template('text:\"not $x but still $y\"'),\n",
      "                  Template('text:\"not $x although still $y\"'),\n",
      "                  Template('text:\"not $x though still $y\"'),\n",
      "                  Template('text:\"$x or very $y\"'),\n",
      "  Template('text:\"$y very $x\"'),\n",
      "                  Template('text:\"not $y but $x enough\"'),\n",
      "                  Template('text:\"$y unbelievably $x\"'),\n",
      "                  Template('text:\"$y not even $x\"'),\n",
      "  Template('text:\"$x even $y\"'),\n",
      "                     Template('text:\"$x no $y\"'),\n",
      "                     Template('text:\"$x perhaps $y\"'),\n",
      "                     Template('text:\"$x sometimes $y\"'),\n",
      "                    Template('text:\"extremely $x $y\"'),\n",
      "                    Template('text:\"are very $x $y\"'),\n",
      "                    Template('text:\"is very $x $y\"')]\n",
      "  symPatterns = [\n",
      "   Template('text:\"$x and $y\"'),\n",
      "   Template('text:\"$x or $y\"'),\n",
      "   Template('text:\"$x and the $y\"'),\n",
      "   Template('text:\"from $x to $y\"'),\n",
      "   Template('text:\"$x or the $y\"'),\n",
      "   Template('text:\"$x as well as $y\"'),\n",
      "   Template('text:\"$x or a $y\"'),\n",
      "   Template('text:\"$x rather than $y\"'),\n",
      "   Template('text:\"$x nor $y\"'),\n",
      "   Template('text:\"$x and one $y\"'),\n",
      "   Template('text:\"either $x or $y\"'),\n",
      "                  ]\n",
      "  \n",
      "  \n",
      "  # In[4]:\n",
      "  \n",
      "  allWords = [['minuscule', 'tiny', 'small', 'big', 'large', 'huge', 'enormous', 'gigantic'],\n",
      "              ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      "              ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent'],\n",
      "              ['horrible', 'terrible', 'awful', 'bad', 'good', 'great', 'wonderful', 'awesome'],\n",
      "              ['slow', 'quick', 'fast', 'speedy'],\n",
      "              ['simple', 'easy', 'hard', 'difficult'],\n",
      "              ['few', 'some', 'several', 'many'],\n",
      "              ['dark', 'dim', 'light', 'bright'],\n",
      "              ['freezing', 'cold', 'warm', 'hot'],\n",
      "             ]\n",
      "  \n",
      "  \n",
      "  # In[13]:\n",
      "  \n",
      "  from itertools import combinations, product\n",
      "  \n",
      "  \n",
      "  # In[50]:\n",
      "  \n",
      "  import pandas\n",
      "  flatList = pandas.read_csv(\"commonAdjs.txt\",header=None,squeeze=True).str.strip().tolist()\n",
      "  \n",
      "  \n",
      "  # In[56]:\n",
      "  if not allpairs:\n",
      "      commonPairsSet = [[(\"small\",\"large\"),\n",
      "                     (\"moist\",\"wet\"),\n",
      "                     (\"smart\",\"intelligent\"),\n",
      "                     (\"good\",\"great\"),\n",
      "                    (\"slow\",\"fast\"),\n",
      "                    (\"simple\",\"easy\"),\n",
      "                    (\"some\",\"many\"),\n",
      "                    (\"light\",\"bright\"),\n",
      "                    (\"warm\",\"hot\")]]\n",
      "  else:\n",
      "      print allpairs\n",
      "      pairs = pd.read_csv('pairs_list.csv',header=None)\n",
      "      pairsSplit = pairs.applymap(lambda x: x.split(\"-\"))\n",
      "      commonPairsSet = pairsSplit.values.tolist()\n",
      "      output_dir_base = output_dir\n",
      "  \n",
      "  \n",
      "  \n",
      "  ident = 0\n",
      "  file_id = 0\n",
      "\n",
      "  pool = mp.ThreadingPool(nodes=8)\n",
      "    \n",
      "  for commonPairs in commonPairsSet:\n",
      "      if allpairs:\n",
      "          output_dir = output_dir_base + \"_\" + str(file_id)\n",
      "          print output_dir\n",
      "          os.mkdir(output_dir)\n",
      "  \n",
      "      candidates = open(output_dir + '/candidates','w')\n",
      "      sentFile = open(output_dir + '/sentences','w')\n",
      "      gold = open(output_dir + '/gold','w')\n",
      "  \n",
      "      i = 0\n",
      "      ident = 0\n",
      "      pairs_length = len(commonPairs)\n",
      "      for pair in commonPairs:\n",
      "          a1 = pair[0]\n",
      "          a2 = pair[1]\n",
      "          for scale in allWords:\n",
      "              if a1 in scale:\n",
      "                  goldScale = scale\n",
      "          if only_one:\n",
      "              a1 = \"*\"\n",
      "              # strongWeakPatterns = strongWeakPatterns[:-3]\n",
      "          sentences = set()\n",
      "          if patterns == 'both' or patterns == 'adjscale':\n",
      "  \t        for pattern in adjScalePatterns:\n",
      "          \t    pat1 = pattern.substitute({'x': a1, 'y': a2})\n",
      "  \t            reg1 = re.compile(pattern.substitute({'x': r'(\\w+)' , 'y': a2})[6:-1])\n",
      "  \n",
      "          \t    pat2 = pattern.substitute({'x': a2, 'y': a1})\n",
      "  \t            reg2 = re.compile(pattern.substitute({'x':a2 , 'y': r'(\\w+)'})[6:-1])\n",
      "          \t    if not generic:\n",
      "                        if pat1 in shelf:\n",
      "                            sentences.update(shelf[pat1])\n",
      "\t\t\telse:\n",
      "                    \t\thits = pool.map(getText,list(indexer.search(pat1 ,scores=False).scoredocs))\n",
      "                       \t\tif only_one:\n",
      "                            \t\tresults = []\n",
      "                            \t\tfor hit in hits:\n",
      "                            \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "                            \t\t#Also filter our sentences with out adjectives in wild card to try an reduce number of sentences\n",
      "                                \t\tif reg1.search(hit) is not None and reg1.search(hit).group(1) in flatList:\n",
      "\t\t\t\t\t\t\ttokenized = tuple([t.orth_ for t in nlp(hit)])\n",
      "                                                        sentences.add(tokenized)\n",
      "                                                        results.append(tokenized)\n",
      "                                    \t\t\t#sentences.add(hit)\n",
      "                                    \t\t\t#results.append(hit)\n",
      "                            \t\tshelf[pat1] = results\n",
      "  \t               \t\telse:\n",
      "\t\t\t\t\ttokenized = map(lambda hit: tuple([t.orth_ for t in nlp(hit)]),hits)\n",
      "                                        sentences.update(tokenized)\n",
      "                            \t\t#sentences.update(hits)\n",
      "                            \t\tshelf[pat1] = tokenized\n",
      "                    \n",
      "\n",
      "\t\t\tif pat2 in shelf:\n",
      "\t\t\t\tsentences.update(shelf[pat2])\n",
      "\t\t\telse:\n",
      "                    \t\thits = pool.map(getText,list(indexer.search(pat2 ,scores=False).scoredocs))\n",
      "\t\t\t\tif only_one:\n",
      "\t\t\t\t\tresults = []\n",
      "          \t        \t\tfor hit in hits:\n",
      "                  \t    \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "  \t                    \t\t\tif reg2.search(hit) is not None and reg2.search(hit).group(1) in flatList:\n",
      "\t\t\t\t\t\t\ttokenized = tuple([t.orth_ for t in nlp(hit)])\n",
      "                                                        sentences.add(tokenized)\n",
      "                                                        results.append(tokenized)\n",
      "\t\t\t\t\tshelf[pat2] = results\n",
      "                  \t    \telse:\n",
      "\t\t\t\t\ttokenized = map(lambda hit: tuple([t.orth_ for t in nlp(hit)]),hits)\n",
      "                                        sentences.update(tokenized)\n",
      "\t\t\t\t\t#sentences.update(hits)\n",
      "\t\t\t\t\tshelf[pat2] = tokenized\n",
      "                          \t\n",
      "  \t            else:\n",
      "          \t        sentences.add(make_generic(pat1))\n",
      "                  \tsentences.add(make_generic(pat2))\n",
      "\n",
      "          if patterns == 'both' or patterns == 'sym':\n",
      "  \t        for pattern in symPatterns:\n",
      "          \t    pat1 = pattern.substitute({'x': a1, 'y': a2})\n",
      "  \t            reg1 = re.compile(pattern.substitute({'x': r'(\\w+)' , 'y': a2})[6:-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "  \t            pat2 = pattern.substitute({'x': a2, 'y': a1})\n",
      "  \t            reg2 = re.compile(pattern.substitute({'x':a2 , 'y': r'(\\w+)'})[6:-1])\n",
      "          \t    if not generic:\n",
      "\t\t\tif pat1 in shelf:\n",
      "                            sentences.update(shelf[pat1])\n",
      "                    \telse:\n",
      "\t\t\t\thits = pool.map(getText,list(indexer.search(pat1 ,scores=False).scoredocs))\n",
      "                  \t\tif only_one:\n",
      "\t\t\t\t\tresults = []\n",
      "\t\t\t\t\tfor hit in hits:\n",
      "  \t                    \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "\t\t\t\t\t\treg_results = reg1.search(hit)\n",
      "  \t                    \t\t\t#if reg1.search(hit) is not None and reg1.search(hit).group(1) in flatList:\n",
      "\t\t\t\t\t\tif reg_results is not None and reg_results.group(1) in flatList:\n",
      "\t\t\t\t\t\t\ttokenized = tuple([t.orth_ for t in nlp(hit)])\n",
      "          \t                \t\t\tsentences.add(tokenized)\n",
      "\t\t\t\t\t\t\tresults.append(tokenized)\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\tshelf[pat1] = results\n",
      "                  \t    \telse:\n",
      "\t\t\t\t\ttokenized = map(lambda hit: tuple([t.orth_ for t in nlp(hit)]),hits)\n",
      "                          \t\tsentences.update(tokenized)\n",
      "\t\t\t\t\tshelf[pat1] = tokenized\n",
      "  \t\t\t\n",
      "\t\t\tif pat2 in shelf:\n",
      "                        \tsentences.update(shelf[pat2])\n",
      "\t\t\telse:\n",
      "  \t                \thits = pool.map(getText,list(indexer.search(pat2 ,scores=False).scoredocs))\n",
      "\t\t\t\tif only_one:\n",
      "\t\t\t\t\tresults = []\n",
      "                  \t\t\tfor hit in hits:\n",
      "          \t            \t\t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "                  \t    \t\t\tif reg2.search(hit) is not None and reg2.search(hit).group(1) in flatList:\n",
      "\t\t\t\t\t\t\ttokenized = tuple([t.orth_ for t in nlp(hit)])\n",
      "                          \t\t\t\tsentences.add(tokenized)\n",
      "\t\t\t\t\t\t\tresults.append(tokenized)\n",
      "\t\t\t\t\tshelf[pat2] = results\n",
      "  \t                    \telse:\n",
      "\t\t\t\t\ttokenized = map(lambda hit: tuple([t.orth_ for t in nlp(hit)]),hits)\n",
      "          \t                \tsentences.update(tokenized)\n",
      "\t\t\t\t\tshelf[pat2] = tokenized\n",
      "  \t            else:\n",
      "                  \tsentences.add(make_generic(pat1))\n",
      "          \t        sentences.add(make_generic(pat2))\n",
      " \t  \n",
      "\t  sentenceStrings = StringIO()\n",
      "\t  goldStrings = StringIO()\n",
      "          #for sentence in nlp.pipe(sentences, batch_size=10000, n_threads=8):\n",
      "\n",
      "  \t  for toks_tuple in sentences:\n",
      "              try:\n",
      "                  #toks = [t.orth_ for t in sentence]\n",
      "\t\t  toks = list(toks_tuple)\n",
      "\t\n",
      "                  index_2 = toks.index(a2)\n",
      "\t\t  tokString = \" \".join(toks)\n",
      "\t\t\n",
      "                  if not only_one:\n",
      "                      index_1 = toks.index(a1)\n",
      "                      ident += 1\n",
      "                      sentenceStrings.write(\"%s.j\\t%d\\t%d\\t%s\\n\" % (a1,ident,index_1,\" \".join(toks)))\n",
      "                      goldStrings.write(\"%s.j %d:: %s 1\\n\" % (a1,ident,\" 1; \".join(set(goldScale) - set([a1]))))\n",
      "                  ident += 1\n",
      "                  sentenceStrings.write(\"%s.j\\t%d\\t%d\\t%s\\n\" % (a2,ident,index_2,tokString))\n",
      "                  goldStrings.write(\"%s.j %d:: %s 1\\n\" % (a2,ident, \" 1; \".join(set(goldScale) - set([a2]))))\n",
      "  \n",
      "  \n",
      "                  #Use same sentence to generate sentences with others pairs\n",
      "                  if expand:\n",
      "                      for j in xrange(0,pairs_length):\n",
      "                          if j != i and random() < .2:\n",
      "                              otherPair = commonPairs[j]\n",
      "                              b1 = otherPair[0]\n",
      "                              b2 = otherPair[1]\n",
      "  \n",
      "                              toks[index_2] = b2\n",
      "                              if not only_one:\n",
      "                                  toks[index_1] = b1\n",
      "                                  ident += 1\n",
      "                                  sentenceStrings.write(\"%s.j\\t%d\\t%d\\t%s\\n\" % (b1,ident,index_1,\" \".join(toks)))\n",
      "                                  goldStrings.write(\"%s.j %d:: %s 1\\n\" % (b1,ident,\" 1; \".join(set(allWords[j]) - set([b1]))))\n",
      "                              ident += 1\n",
      "                              sentenceStrings.write(\"%s.j\\t%d\\t%d\\t%s\\n\" % (b2,ident,index_2,\" \".join(toks)))\n",
      "                              goldStrings.write(\"%s.j %d:: %s 1\\n\" % (b2,ident,\" 1; \".join(set(allWords[j]) - set([b2]))))\n",
      "  \n",
      "              except Exception as e:\n",
      "                  pass\n",
      "  \n",
      "          candidates.write(\"%s.j::%s\\n\" % (a1,\";\".join(set(flatList) - set([a1]))))\n",
      "          candidates.write(\"%s.j::%s\\n\" % (a2,\";\".join(set(flatList) - set([a2]))))\n",
      "          i += 1\n",
      " \n",
      "      \t  sentFile.write(sentenceStrings.getvalue())\n",
      "          gold.write(goldStrings.getvalue())\n",
      "\n",
      "      candidates.close()\n",
      "      gold.close()\n",
      "      sentFile.close()\n",
      "      \n",
      "      file_id += 1\n",
      "\n",
      "  pool.close()\n",
      "  pool.terminate()\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  main()\n",
      "\n",
      "# coding: utf-8\n",
      "\n",
      "# In[73]:\n",
      "\n",
      "import lucene\n",
      "from random import random, sample\n",
      "lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n",
      "from lucene import VERSION\n",
      "from org.apache.lucene import analysis,util\n",
      "from lupyne import engine\n",
      "from cStringIO import StringIO\n",
      "from itertools import izip\n",
      "import shelve \n",
      "def tokenizer(reader):\n",
      "    return analysis.standard.StandardTokenizer(util.Version.LATEST, reader)\n",
      "\n",
      "\n",
      "# In[21]:\n",
      "\n",
      "from spacy.en import English\n",
      "\n",
      "\n",
      "# In[79]:\n",
      "\n",
      "import spacy\n",
      "nlp = spacy.load('en')\n",
      "\n",
      "\n",
      "# In[2]:\n",
      "\n",
      "myAnalyzer = engine.indexers.Analyzer( tokenizer,\n",
      "     analysis.standard.StandardFilter, analysis.LowerCaseFilter)\n",
      "\n",
      "\n",
      "indexer = engine.IndexSearcher('/home/bryan/ukwakSentenceIndex',analyzer=myAnalyzer)\n",
      "shelf = shelve.open('ukwakShelf')\n",
      "\n",
      "\n",
      "# In[4]:\n",
      "\n",
      "adjscale_queries = ['text:\"but not\"',\n",
      "                    'text:\"if not\"',\n",
      "                    'text:\"although not\"',\n",
      "                    'text:\"though not\"',\n",
      "                    'text:\"and even\"',\n",
      "                    'text:\"or even\"',\n",
      "                    'text:\"and almost\"',\n",
      "                    'text:\"or almost\"',\n",
      "                    'text:\"not only * but\"',\n",
      "                    'text:\"not just * but\"',\n",
      "                    'text:\"not * just\"',\n",
      "                    'text:\"not * but just\"',\n",
      "                    'text:\"not * still\"',\n",
      "                    'text:\"not * but still\"',\n",
      "                    'text:\"not * although still\"',\n",
      "                    'text:\"not * though still\"',\n",
      "                    'text:\"very\"',\n",
      "                    'text:\"not * but * enough\"',\n",
      "                    'text:\"unbelievably\"',\n",
      "                    'text:\"even\"',\n",
      "                    'text:\"no\"',\n",
      "                    'text:\"perhaps\"',\n",
      "                    'text:\"sometimes\"',\n",
      "                    'text:\"extremely\"']\n",
      "\n",
      "\n",
      "# In[3]:\n",
      "\n",
      "queries = ['text:\"if not\"',\n",
      "'text:\"and perhaps\"', \n",
      "'text:\"but not\"',\n",
      "'text:\"between * and \"',\n",
      "'text:\"from * to \"',\n",
      "'text:\"or at least\"']\n",
      "\n",
      "\n",
      "# In[4]:\n",
      "\n",
      "def getText(i):\n",
      "    env = lucene.getVMEnv()\n",
      "    env.attachCurrentThread()\n",
      "    d = indexer.doc(i.doc)\n",
      "    return d['text']\n",
      "    # Get the arguments\n",
      "\n",
      "\n",
      "# In[5]:\n",
      "\n",
      "import pathos.multiprocessing as mp\n",
      "\n",
      "pool = mp.ThreadingPool(nodes=8)\n",
      "\n",
      "\n",
      "# In[6]:\n",
      "\n",
      "sentences = []\n",
      "for query in queries:\n",
      "    hits = pool.map(getText,list(indexer.search(query ,scores=False).scoredocs))\n",
      "    sentences.extend(hits)\n",
      "\n",
      "\n",
      "# In[11]:\n",
      "\n",
      "len(sentences)\n",
      "\n",
      "\n",
      "# In[8]:\n",
      "\n",
      "sents = set(sentences)\n",
      "\n",
      "\n",
      "# In[12]:\n",
      "\n",
      "len(sents)\n",
      "\n",
      "\n",
      "# In[148]:\n",
      "\n",
      "import codecs\n",
      "with codecs.open('sheinman.sents','w','utf-8') as f:\n",
      "    for sentence in sents:\n",
      "        f.write(\"%s\\n\"%sentence)\n",
      "\n",
      "\n",
      "# In[98]:\n",
      "\n",
      "def pattern_extractor(regex, sentence):\n",
      "    match = regex.search(sentence)\n",
      "    if match:\n",
      "        doc = nlp(unicode(sentence))\n",
      "        good = True\n",
      "        for tok in doc:\n",
      "            if tok.idx == match.start(1) and tok.pos_ != 'ADJ':\n",
      "                good = False\n",
      "            if tok.idx == match.start(2) and tok.pos_ != 'ADJ':\n",
      "                good = False            \n",
      "        if good:\n",
      "            return match.groups()\n",
      "\n",
      "\n",
      "# In[ ]:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# In[99]:\n",
      "\n",
      "pattern_extractor(re.compile(r\"(\\w+)[\\s,()\\-']*but not[\\s,()\\-']*(\\w+)\"), \"cool but not hot\")\n",
      "\n",
      "\n",
      "# In[95]:\n",
      "\n",
      "reg = re.compile(\"but not\")\n",
      "for sent in sents:\n",
      "    if \"cool but not hot\" in sent:\n",
      "        print sent, reg.search(sent)\n",
      "\n",
      "\n",
      "# In[13]:\n",
      "\n",
      "import re\n",
      "res = [re.compile(r\"(\\w+)[\\s,()\\-']*if not[\\s,()\\-']*(\\w+)\"),\n",
      "re.compile(r\"(\\w+)[\\s,()\\-']*and perhaps[\\s,()\\-']*(\\w+)\"), \n",
      "re.compile(r\"(\\w+)[\\s,()\\-']*but not[\\s,()\\-']*(\\w+)\"),\n",
      "re.compile(r\"between[\\s)(,\\-']*(\\w+)[\\s,()\\-']*and[\\s,()\\-']*(\\w+)\"),\n",
      "re.compile(r\"from[\\s)(,\\-']*(\\w+)[\\s,()\\-']*to[\\s,()\\-']*(\\w+)\"),\n",
      "re.compile(r\"(\\w+)[\\s,()\\-']*at least[\\s,()\\-']*(\\w+)\")]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs = []\n",
      "for regx in res:\n",
      "    pairs.extend(set(filter(lambda x: x is not None, pool.map(lambda x: pattern_extractor(regx,x), sents))))\n",
      "#for sentence in sent:\n",
      "#    if 'if not' in sentence:\n",
      "        \n",
      "\n",
      "\n",
      "# In[101]:\n",
      "\n",
      "adjscale_res = [re.compile(r\"(\\w+)[\\s,()\\-']*but not[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*if not[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*although not[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*though not[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*and even[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*or even[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*and almost[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*or almost[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"not only[\\s)(,\\-']*(\\w+)[\\s,()\\-']*but[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"not just[\\s)(,\\-']*(\\w+)[\\s,()\\-']*but[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"not[\\s)(,\\-']*(\\w+)[\\s,()\\-']*just[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"not[\\s)(,\\-']*(\\w+)[\\s,()\\-']*but just[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"not[\\s)(,\\-']*(\\w+)[\\s,()\\-']*still[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"not[\\s)(,\\-']*(\\w+)[\\s,()\\-']*but still[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"not[\\s)(,\\-']*(\\w+)[\\s,()\\-']*although still[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"not[\\s)(,\\-']*(\\w+)[\\s,()\\-']*though still[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*or very[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*very[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"not[\\s)(,\\-']*(\\w+)[\\s,()\\-']*but[\\s,()\\-']*(\\w+)[\\s,()\\-']*enough\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*unbelievably[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*not even[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*even[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*no[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*perhaps[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*sometimes[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"(\\w+)[\\s,()\\-']*extremely[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"are very[\\s)(,\\-']*(\\w+)[\\s,()\\-']*(\\w+)\"),\n",
      "                re.compile(r\"is very[\\s)(,\\-']*(\\w+)[\\s,()\\-']*(\\w+)\")]\n",
      "pairs = []\n",
      "for regx in adjscale_res:\n",
      "   pairs.extend(set(filter(lambda x: x is not None, pool.map(lambda x: pattern_extractor(regx,x), sents))))\n",
      "\n",
      "\n",
      "# In[102]:\n",
      "\n",
      "len(set(pairs))\n",
      "\n",
      "\n",
      "# In[103]:\n",
      "\n",
      "import pandas as pd\n",
      "flatList = pd.read_csv(\"commonAdjs.txt\",header=None,squeeze=True).str.strip().tolist()\n",
      "\n",
      "\n",
      "# In[104]:\n",
      "\n",
      "adj_pairs = filter(lambda x: x[0] in flatList and x[1] in flatList,set(pairs))\n",
      "\n",
      "\n",
      "# In[105]:\n",
      "\n",
      "len(adj_pairs)\n",
      "\n",
      "\n",
      "# In[20]:\n",
      "\n",
      "adj_pairs\n",
      "\n",
      "\n",
      "# In[116]:\n",
      "\n",
      "def make_scale(word1,word2,adj_set):\n",
      "    if True:#(word1,word2) in adj_set or (word2,word1) in adj_set:\n",
      "        df = pd.DataFrame(adj_set)\n",
      "        word1_partners = df[df[0] == word1][1].tolist()\n",
      "        word1_partners.extend(df[df[1] == word1][0].tolist())\n",
      "        \n",
      "        word2_partners = df[df[0] == word2][1].tolist()\n",
      "        word2_partners.extend(df[df[1] == word2][0].tolist())\n",
      "   \n",
      "        return set(word1_partners) & set(word2_partners)\n",
      "    \n",
      "def p(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(pred)\n",
      "\n",
      "def r(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(true)\n",
      "\n",
      "\n",
      "# In[126]:\n",
      "\n",
      "import itertools\n",
      "master_scales = [['dark', 'dim', 'light', 'bright'],\n",
      " ['few', 'some', 'several', 'many'],\n",
      " ['simple', 'easy', 'hard', 'difficult'],\n",
      " ['freezing', 'cold', 'warm', 'hot'],\n",
      " ['minuscule',\n",
      "  'tiny',\n",
      "  'small',\n",
      "  'big',\n",
      "  'large',\n",
      "  'huge',\n",
      "  'enormous',\n",
      "  'gigantic'],\n",
      " ['slow', 'quick', 'fast', 'speedy'],\n",
      " ['horrible',\n",
      "  'terrible',\n",
      "  'awful',\n",
      "  'bad',\n",
      "  'good',\n",
      "  'great',\n",
      "  'wonderful',\n",
      "  'awesome'],\n",
      " ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      " ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent']]\n",
      "#master_scales = [['hideous', 'ugly', 'pretty', 'beautiful', 'gorgeous'],\n",
      "#                ['same', 'alike', 'similar', 'different'],\n",
      "#                ['ancient', 'old', 'fresh', 'new']]\n",
      "rows = []\n",
      "for scale in master_scales:\n",
      "    for combo in itertools.combinations(scale,2):\n",
      "        pred = make_scale(combo[0],combo[1],adj_pairs)\n",
      "        if pred is not None and len(pred) > 0:\n",
      "            rows.append([p(pred,scale), r(pred,scale), str(combo),str(scale)])\n",
      "        else:\n",
      "            rows.append([0, 0, str(combo),str(scale)])\n",
      "print pd.DataFrame(rows,columns=['Precision','Recall','Seed','Scale']).groupby('Scale').mean().to_latex()\n",
      "\n",
      "\n",
      "# In[95]:\n",
      "\n",
      "import rpy2\n",
      "import rpy2.robjects as ro\n",
      "\n",
      "\n",
      "# In[99]:\n",
      "\n",
      "ro.r('library(LSAfun)')\n",
      "ro.r(\"load('TASA.rda')\")\n",
      "LSA_scales = filter(lambda x: ro.r(\"Cosine('%s','%s',tvector=TASA)\" % x) > 0, adj_pairs )\n",
      "\n",
      "\n",
      "# In[33]:\n",
      "\n",
      "moby = open('mthes/mobythes.aur').read()\n",
      "\n",
      "\n",
      "# In[34]:\n",
      "\n",
      "moby = moby.split('\\r')\n",
      "\n",
      "\n",
      "# In[36]:\n",
      "\n",
      "moby = map(lambda x: x.split(','),moby)\n",
      "\n",
      "\n",
      "# In[59]:\n",
      "\n",
      "from collections import defaultdict\n",
      "entry_lookup = defaultdict(list)\n",
      "i = 0\n",
      "for entry in moby:\n",
      "    for word in entry:\n",
      "        entry_lookup[word].append(i)\n",
      "    i += 1\n",
      "\n",
      "\n",
      "# In[52]:\n",
      "\n",
      "entry_lookup['cold']\n",
      "\n",
      "\n",
      "# In[106]:\n",
      "\n",
      "moby_pairs = []\n",
      "bad_pairs = []\n",
      "for pair in adj_pairs:\n",
      "    if set(entry_lookup[pair[0]]) & set(entry_lookup[pair[1]]):\n",
      "    #f pair[1] in entry_lookup[pair[0]] and pair[0] in entry_lookup[pair[1]]:\n",
      "        moby_pairs.append(pair)\n",
      "    else:\n",
      "        bad_pairs.append(pair)\n",
      "    #for entry in moby:\n",
      "    #    if pair[0] in entry and pair[1] in entry:\n",
      "    #        moby_pairs.append(pair)\n",
      "    #        break\n",
      "\n",
      "\n",
      "# In[61]:\n",
      "\n",
      "len(bad_pairs)\n",
      "\n",
      "\n",
      "# In[62]:\n",
      "\n",
      "moby_pairs\n",
      "\n",
      "\n",
      "# In[107]:\n",
      "\n",
      "pairs_frame = pd.DataFrame(moby_pairs)\n",
      "pairs_frame = pairs_frame[pairs_frame[0] != pairs_frame[1]]\n",
      "pairs_frame.to_csv('vanMiltenburg_pairs.csv',header=False,index=False)\n",
      "\n",
      "\n",
      "# In[108]:\n",
      "\n",
      "len(moby_pairs)\n",
      "\n",
      "\n",
      "# In[127]:\n",
      "\n",
      "rows = []\n",
      "for scale in master_scales:\n",
      "    for combo in itertools.combinations(scale,2):\n",
      "        pred = make_scale(combo[0],combo[1],moby_pairs)\n",
      "        if pred is not None and len(pred) > 0:\n",
      "            rows.append([p(pred,scale), r(pred,scale), str(combo),str(scale)])\n",
      "        else:\n",
      "            rows.append([0, 0, str(combo),str(scale)])\n",
      "\n",
      "\n",
      "# In[128]:\n",
      "\n",
      "print pd.DataFrame(rows,columns=['Precision','Recall','Seed','Scale']).mean()\n",
      "\n",
      "\n",
      "# In[129]:\n",
      "\n",
      "print pd.DataFrame(rows,columns=['Precision','Recall','Seed','Scale']).std()\n",
      "\n",
      "\n",
      "# In[130]:\n",
      "\n",
      "stats = pd.DataFrame(rows,columns=['Precision','Recall','Seed','Scale'])\n",
      "\n",
      "\n",
      "# In[131]:\n",
      "\n",
      "stats['F'] = 2.0 * ((stats[\"Precision\"] * stats[\"Recall\"])/(stats[\"Precision\"] + stats[\"Recall\"]))\n",
      "\n",
      "\n",
      "# In[132]:\n",
      "\n",
      "stats['F'] = stats['F'].fillna(0)\n",
      "\n",
      "\n",
      "# In[133]:\n",
      "\n",
      "stats['F'].mean()\n",
      "\n",
      "\n",
      "# In[134]:\n",
      "\n",
      "stats['F'].std()\n",
      "\n",
      "\n",
      "# In[ ]:\n",
      "\n",
      "\n",
      "\n",
      "from gensim import models\n",
      "import itertools\n",
      "import pandas as pd\n",
      "from gensim import matutils\n",
      "import csv\n",
      "import sys\n",
      "#w2v = models.word2vec.Word2Vec.load_word2vec_format('/home/bryan/GoogleNews-vectors-negative300.bin',binary=True)\n",
      "\n",
      "vectors = pd.read_csv(sys.argv[1],header=None,index_col=0,delim_whitespace=True,quoting=csv.QUOTE_NONE)\n",
      "\n",
      "def p(pred,true):\n",
      "    try:\n",
      "\t    return len(set(pred).intersection(set(true))) * 1.0/len(pred)\n",
      "    except:\n",
      "\t    return 0\n",
      "\n",
      "def r(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(true)\n",
      "\n",
      "\n",
      "master_scales = [['dark', 'dim', 'light', 'bright'],\n",
      " ['few', 'some', 'several', 'many'],\n",
      " ['simple', 'easy', 'hard', 'difficult'],\n",
      " ['freezing', 'cold', 'warm', 'hot'],\n",
      " ['minuscule',\n",
      "  'tiny',\n",
      "  'small',\n",
      "  'big',\n",
      "  'large',\n",
      "  'huge',\n",
      "  'enormous',\n",
      "  'gigantic'],\n",
      " ['slow', 'quick', 'fast', 'speedy'],\n",
      " ['horrible',\n",
      "  'terrible',\n",
      "  'awful',\n",
      "  'bad',\n",
      "  'good',\n",
      "  'great',\n",
      "  'wonderful',\n",
      "  'awesome'],\n",
      " ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      " ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent']]\n",
      "\n",
      "\n",
      "def add_similar(word1,word2):\n",
      "\ty  = (vectors.ix[word1] + vectors.ix[word2])/2.0\n",
      "\tres = vectors.dot(matutils.unitvec(y.values))\n",
      "#\treturn list(set(res.sort_values(ascending=False).index.tolist()) - set([word1,word2]))\n",
      "\treturn res.sort_values(ascending=False).index.tolist()\n",
      "\n",
      "#Code from Gensim; originally from levy et al\n",
      "def mul_similar(word1,word2):\n",
      "\ty = [((1 + vectors.dot(vectors.ix[term])) / 2) for term in [word1,word2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tres = y[0] * y[1]\n",
      "\treturn res.sort_values(ascending=False).index.tolist()\n",
      "#        return list(set(res.sort_values(ascending=False).index.tolist()) - set([word1,word2]))\n",
      "\n",
      "\n",
      "rows = []\n",
      "mul_rows = []\n",
      "for scale in master_scales:\n",
      "    for combo in itertools.combinations(scale,2):\n",
      "        l_scale = list(set(scale) - set([combo[0],combo[1]]))\n",
      "        pred = list(set(add_similar(*combo)[:len(scale)])-set([combo[0],combo[1]]))\n",
      "\tmul_pred = list(set(mul_similar(*combo)[:len(scale)]) - set([combo[0],combo[1]]))\n",
      "        rows.append([p(pred,l_scale), r(pred,l_scale), str(combo),str(scale)])\n",
      "\tmul_rows.append([p(mul_pred,l_scale), r(mul_pred,l_scale), str(combo),str(scale)])\n",
      "\n",
      "stats = pd.DataFrame(rows,columns=['Precision','Recall','Seed','Scale'])\n",
      "stats['F'] = 2.0 * ((stats[\"Precision\"] * stats[\"Recall\"])/(stats[\"Precision\"] + stats[\"Recall\"]))\n",
      "stats['F'] = stats['F'].fillna(0)\n",
      "\n",
      "mul_stats = pd.DataFrame(mul_rows,columns=['Precision','Recall','Seed','Scale'])\n",
      "mul_stats['F'] = 2.0 * ((mul_stats[\"Precision\"] * mul_stats[\"Recall\"])/(mul_stats[\"Precision\"] + mul_stats[\"Recall\"]))\n",
      "mul_stats['F'] = mul_stats['F'].fillna(0)\n",
      "\n",
      "print stats.groupby('Scale').mean()\n",
      "print \"F: \", stats['F'].mean()\n",
      "print \"Recall: \", stats['Recall'].mean()\n",
      "print \"Precision: \", stats['Precision'].mean()\n",
      "\n",
      "print mul_stats.groupby('Scale').mean()\n",
      "print \"F: \", mul_stats['F'].mean()\n",
      "print \"Recall: \", mul_stats['Recall'].mean()\n",
      "print \"Precision: \", mul_stats['Precision'].mean()\n",
      "from gensim import models\n",
      "import itertools\n",
      "import pandas as pd\n",
      "from gensim import matutils\n",
      "import csv\n",
      "import sys\n",
      "#w2v = models.word2vec.Word2Vec.load_word2vec_format('/home/bryan/GoogleNews-vectors-negative300.bin',binary=True)\n",
      "\n",
      "vectors = pd.read_csv(sys.argv[1],header=None,index_col=0,delim_whitespace=True,quoting=csv.QUOTE_NONE)\n",
      "\n",
      "def p(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(pred)\n",
      "\n",
      "def r(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(true)\n",
      "\n",
      "\n",
      "master_scales = [['dark', 'dim', 'light', 'bright'],\n",
      " ['few', 'some', 'several', 'many'],\n",
      " ['simple', 'easy', 'hard', 'difficult'],\n",
      " ['freezing', 'cold', 'warm', 'hot'],\n",
      " ['minuscule',\n",
      "  'tiny',\n",
      "  'small',\n",
      "  'big',\n",
      "  'large',\n",
      "  'huge',\n",
      "  'enormous',\n",
      "  'gigantic'],\n",
      " ['slow', 'quick', 'fast', 'speedy'],\n",
      " ['horrible',\n",
      "  'terrible',\n",
      "  'awful',\n",
      "  'bad',\n",
      "  'good',\n",
      "  'great',\n",
      "  'wonderful',\n",
      "  'awesome'],\n",
      " ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      " ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent']]\n",
      "\n",
      "\n",
      "def add_similar(word1,word2):\n",
      "\ty  = (vectors.ix[word1] + vectors.ix[word2])/2.0\n",
      "\tres = vectors.dot(matutils.unitvec(y.values))\n",
      "\treturn res.sort_values(ascending=False).index.tolist()\n",
      "\n",
      "#Code from Gensim; originally from levy et al\n",
      "def mul_similar(word1,word2):\n",
      "\ty = [((1 + vectors.dot(vectors.ix[term])) / 2) for term in [word1,word2]]\n",
      "\tres = y[0] * y[1]\n",
      "\treturn res.sort_values(ascending=False).index.tolist()\n",
      "\n",
      "\n",
      "rows = []\n",
      "mul_rows = []\n",
      "for scale in master_scales:\n",
      "    for combo in itertools.combinations(scale,2):\n",
      "        pred = add_similar(*combo)[:len(scale)]\n",
      "\tmul_pred = mul_similar(*combo)[:len(scale)]\n",
      "\n",
      "        rows.append([p(pred,scale), r(pred,scale), str(combo),str(scale)])\n",
      "\tmul_rows.append([p(mul_pred,scale), r(mul_pred,scale), str(combo),str(scale)])\n",
      "\n",
      "stats = pd.DataFrame(rows,columns=['Precision','Recall','Seed','Scale'])\n",
      "stats['F'] = 2.0 * ((stats[\"Precision\"] * stats[\"Recall\"])/(stats[\"Precision\"] + stats[\"Recall\"]))\n",
      "stats['F'] = stats['F'].fillna(0)\n",
      "\n",
      "mul_stats = pd.DataFrame(mul_rows,columns=['Precision','Recall','Seed','Scale'])\n",
      "mul_stats['F'] = 2.0 * ((mul_stats[\"Precision\"] * mul_stats[\"Recall\"])/(mul_stats[\"Precision\"] + mul_stats[\"Recall\"]))\n",
      "mul_stats['F'] = mul_stats['F'].fillna(0)\n",
      "\n",
      "print stats.groupby('Scale').mean()\n",
      "print \"F: \", stats['F'].mean()\n",
      "print \"Recall: \", stats['Recall'].mean()\n",
      "print \"Precision: \", stats['Precision'].mean()\n",
      "\n",
      "print mul_stats.groupby('Scale').mean()\n",
      "print \"F: \", mul_stats['F'].mean()\n",
      "print \"Recall: \", mul_stats['Recall'].mean()\n",
      "print \"Precision: \", mul_stats['Precision'].mean()\n",
      "from gensim import models\n",
      "import itertools\n",
      "import pandas as pd\n",
      "\n",
      "w2v = models.KeyedVectors.load_word2vec_format('/home/bryan/GoogleNews-vectors-negative300.bin',binary=True)\n",
      "\n",
      "def p(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(pred)\n",
      "\n",
      "def r(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(true)\n",
      "\n",
      "\n",
      "master_scales = [['dark', 'dim', 'light', 'bright'],\n",
      " ['few', 'some', 'several', 'many'],\n",
      " ['simple', 'easy', 'hard', 'difficult'],\n",
      " ['freezing', 'cold', 'warm', 'hot'],\n",
      " ['minuscule',\n",
      "  'tiny',\n",
      "  'small',\n",
      "  'big',\n",
      "  'large',\n",
      "  'huge',\n",
      "  'enormous',\n",
      "  'gigantic'],\n",
      " ['slow', 'quick', 'fast', 'speedy'],\n",
      " ['horrible',\n",
      "  'terrible',\n",
      "  'awful',\n",
      "  'bad',\n",
      "  'good',\n",
      "  'great',\n",
      "  'wonderful',\n",
      "  'awesome'],\n",
      " ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      " ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent']]\n",
      "\n",
      "rows = []\n",
      "for scale in master_scales:\n",
      "    for combo in itertools.combinations(scale,2):\n",
      " \tl_scale = list(set(scale) - set([combo[0],combo[1]]))\n",
      "#        pred = list(set(add_similar(*combo)[:len(scale)])-set([combo[0],combo[1]]))\n",
      "        pred = map(lambda x: x[0], w2v.most_similar(list(combo)))[:len(l_scale)]\n",
      "        rows.append([p(pred,l_scale), r(pred,l_scale), str(combo),str(scale)])\n",
      "\n",
      "\n",
      "stats = pd.DataFrame(rows,columns=['Precision','Recall','Seed','Scale'])\n",
      "stats['F'] = 2.0 * ((stats[\"Precision\"] * stats[\"Recall\"])/(stats[\"Precision\"] + stats[\"Recall\"]))\n",
      "stats['F'] = stats['F'].fillna(0)\n",
      "print stats.groupby('Scale').mean()\n",
      "print \"F: \", stats['F'].mean()\n",
      "print \"Recall: \", stats['Recall'].mean()\n",
      "print \"Precision: \", stats['Precision'].mean()\n",
      "from gensim import models\n",
      "import itertools\n",
      "import pandas as pd\n",
      "\n",
      "w2v = models.KeyedVectors.load_word2vec_format('/home/bryan/GoogleNews-vectors-negative300.bin',binary=True)\n",
      "\n",
      "def p(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(pred)\n",
      "\n",
      "def r(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(true)\n",
      "\n",
      "\n",
      "master_scales = [['dark', 'dim', 'light', 'bright'],\n",
      " ['few', 'some', 'several', 'many'],\n",
      " ['simple', 'easy', 'hard', 'difficult'],\n",
      " ['freezing', 'cold', 'warm', 'hot'],\n",
      " ['minuscule',\n",
      "  'tiny',\n",
      "  'small',\n",
      "  'big',\n",
      "  'large',\n",
      "  'huge',\n",
      "  'enormous',\n",
      "  'gigantic'],\n",
      " ['slow', 'quick', 'fast', 'speedy'],\n",
      " ['horrible',\n",
      "  'terrible',\n",
      "  'awful',\n",
      "  'bad',\n",
      "  'good',\n",
      "  'great',\n",
      "  'wonderful',\n",
      "  'awesome'],\n",
      " ['parched', 'arid', 'dry', 'damp', 'moist', 'wet'],\n",
      " ['idiotic', 'stupid', 'dumb', 'smart', 'intelligent']]\n",
      "\n",
      "rows = []\n",
      "for scale in master_scales:\n",
      "    for combo in itertools.combinations(scale,2):\n",
      "        pred = map(lambda x: x[0], w2v.most_similar(list(combo)))[:len(scale)]\n",
      "        rows.append([p(pred,scale), r(pred,scale), str(combo),str(scale)])\n",
      "\n",
      "\n",
      "stats = pd.DataFrame(rows,columns=['Precision','Recall','Seed','Scale'])\n",
      "stats['F'] = 2.0 * ((stats[\"Precision\"] * stats[\"Recall\"])/(stats[\"Precision\"] + stats[\"Recall\"]))\n",
      "stats['F'] = stats['F'].fillna(0)\n",
      "print stats.groupby('Scale').mean()\n",
      "print \"F: \", stats['F'].mean()\n",
      "print \"Recall: \", stats['Recall'].mean()\n",
      "print \"Precision: \", stats['Precision'].mean()\n",
      "from gensim import models\n",
      "import itertools\n",
      "import pandas as pd\n",
      "\n",
      "w2v = models.KeyedVectors.load_word2vec_format('/home/bryan/GoogleNews-vectors-negative300.bin',binary=True)\n",
      "\n",
      "def p(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(pred)\n",
      "\n",
      "def r(pred,true):\n",
      "    return len(set(pred).intersection(set(true))) * 1.0/len(true)\n",
      "\n",
      "\n",
      "master_scales = [['hideous', 'ugly', 'pretty', 'beautiful', 'gorgeous'],\n",
      "                ['same', 'alike', 'similar', 'different'],\n",
      "                ['ancient', 'old', 'fresh', 'new']]\n",
      "\n",
      "\n",
      "rows = []\n",
      "for scale in master_scales:\n",
      "    for combo in itertools.combinations(scale,2):\n",
      " \tl_scale = list(set(scale) - set([combo[0],combo[1]]))\n",
      "#        pred = list(set(add_similar(*combo)[:len(scale)])-set([combo[0],combo[1]]))\n",
      "        pred = map(lambda x: x[0], w2v.most_similar(list(combo)))[:len(l_scale)]\n",
      "        rows.append([p(pred,l_scale), r(pred,l_scale), str(combo),str(scale)])\n",
      "\n",
      "\n",
      "stats = pd.DataFrame(rows,columns=['Precision','Recall','Seed','Scale'])\n",
      "stats['F'] = 2.0 * ((stats[\"Precision\"] * stats[\"Recall\"])/(stats[\"Precision\"] + stats[\"Recall\"]))\n",
      "stats['F'] = stats['F'].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print stats.groupby('Scale').mean()\n",
      "print \"F: \", stats['F'].mean()\n",
      "print \"Recall: \", stats['Recall'].mean()\n",
      "print \"Precision: \", stats['Precision'].mean()\n"
     ]
    }
   ],
   "source": [
    "awk '{ print $0; }' ~/MyMethod/*.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `awk` Variables\n",
    "- `awk` was designed for space seperated data files\n",
    "    - The delimiter can be changes by using the `-F` flag\n",
    "- The entire line is in the variable `$0`\n",
    "    - Each field is placed in `$1`, `$2`, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Special `awk` Variables\n",
    "- `awk` has many built in variables that contain useful information\n",
    "- Two commons ones are:\n",
    "    - NF - The number of fields in the current record (on the current line)\n",
    "    - NR - The number of records processed so far\n",
    "- All variables can be used in both patterns and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args = docopt(\"\"\"Aggregates lexical subsitution scores from sentences to form a scale membership list. The statistics are printed to std out.\n",
      "    ratio1 = ((D + g * a) * ( 1 - g * a))/(a ** 2 * g * (1 - g))\n",
      "args = docopt(\"\"\"Aggregates lexical subsitution scores from sentences to form a scale membership list. The statistics are printed to std out.\n",
      "    ratio1 = ((D + g * a) * ( 1 - g * a))/(a ** 2 * g * (1 - g))\n",
      "      --generic  Don't search the corpus, just use the patterns as is, adding \"it was\" to them to make a complete sentence.\n",
      "                            \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "                  \t    \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "  \t                    \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "          \t            \t\t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "      --generic  Don't search the corpus, just use the patterns as is, adding \"it was\" to them to make a complete sentence.\n",
      "                            \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "                  \t    \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "  \t                    \t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n",
      "          \t            \t\t\t#If only using one word, the sentences are a lot messier, so go through and make sure they match a regex\n"
     ]
    }
   ],
   "source": [
    "awk 'NF > 20 { print $0; }'  ~/MyMethod/*.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data['unfiltered'] = data.apply(lambda x: x['0_x'].split(',') + [x['word1']] + [x['word2']],axis=1).tolist()\n",
      "                if i != j and set(row['unfiltered']) == set(row2['unfiltered']) and j not in to_remove:\n",
      "                if set(eval(row['final_output'])) < set(eval(row2['final_output'])):\n",
      "                if i != j and set(eval(row['final_output'])) == set(eval(row2['final_output'])) and j not in to_remove:\n",
      "                if filter(lambda x: x['name'] == (word + '.a'), fn.lus(r'%s.a' % word)):\n",
      "        return len(list(combinations(members,2))) + len(list(product(members,[row['word1'],row['word2']])))\n",
      "\t\t\tnum_atts[synset] = sum(map(lambda y: len(y.attributes()) ,synset.similar_tos())) + len(synset.attributes())\n",
      "\t\t\t\t\tattributes.append(reduce(set.intersection,map(lambda x:set(x.attributes()),group)))\n",
      "\t\t\t\t\tsynsets_for_words = filter(lambda x: not any(map(lambda y: y in x, list(group))),synsets_for_words)\n",
      "\t\tlus_for_words.append(filter(lambda x: x['name'] == '%s.a'%word, fn.lus(r'%s.a' % word)))\n",
      "awk: write failure (Broken pipe)\n",
      "awk: close failed on file /dev/stdout (Broken pipe)\n"
     ]
    }
   ],
   "source": [
    "awk 'length($0) > 80 { print $0; }'  ~/MyMethod/*.py | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4673\n"
     ]
    }
   ],
   "source": [
    "awk 'END { print NR; }' ~/MyMethod/*.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas\n",
      "sys\n",
      "os\n",
      "re\n",
      "dill\n",
      "shelve\n",
      "pandas\n",
      "pathos.multiprocessing\n",
      "numpy\n",
      "numexpr\n"
     ]
    }
   ],
   "source": [
    "awk '/^import/ {print $2;}'  ~/MyMethod/*.py | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argparse\n",
      "codecs\n",
      "copy\n",
      "csv\n",
      "ctxpredict.models\n",
      "dask\n",
      "dask.multiprocessing\n",
      "dask.threaded\n",
      "depextract\n",
      "dill\n",
      "gzip\n",
      "itertools\n",
      "lucene\n",
      "matplotlib\n",
      "matplotlib.pyplot\n",
      "networkx\n",
      "numexpr\n",
      "numpy\n",
      "numpy.ma\n",
      "os\n",
      "os.path\n",
      "pandas\n",
      "pathos.multiprocessing\n",
      "random\n",
      "re\n",
      "rpy2\n",
      "rpy2.robjects\n",
      "rpy2.robjects.numpy2ri\n",
      "rpy2.robjects.packages\n",
      "rpy2.robjects.pandas2ri\n",
      "shelve\n",
      "shutil\n",
      "spacy\n",
      "sys\n",
      "time\n",
      "unicodedata\n",
      "utdeftvs\n"
     ]
    }
   ],
   "source": [
    "awk '/^import/ {print $2;}'  ~/MyMethod/*.py | sort -u "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `sed`\n",
    "- By default, `sed` reads in a file, applies a command to each line in a file, and prints the result to STDOUT\n",
    "    - The edits can be changed to occur in-place by using the `-i` flag. Be careful with this!\n",
    "    - To only print lines explicitly using a command, use the `-n` flag\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting Lines\n",
    "- Similarly to `awk`, `sed` allows us to control which lines we apply the commands to\n",
    "- The lines can be selected by using\n",
    "    - The line number\n",
    "    - A range of line numbers\n",
    "    - A regex\n",
    "    - The inverse of any of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t91\n",
      "2\t92\n",
      "3\t93\n",
      "4\t94\n",
      "5\t95\n",
      "6\t96\n",
      "7\t97\n",
      "8\t98\n",
      "9\t99\n",
      "10\t100\n"
     ]
    }
   ],
   "source": [
    "paste <(head hundred.txt) <(tail hundred.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 hundred.txt\n"
     ]
    }
   ],
   "source": [
    "wc -l hundred.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "sed -n '20p' hundred.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "sed -n '20,25p' hundred.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "sed -n '/2[0-5]/p' hundred.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "sed -n '1,95!p' hundred.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "sed -n '/[[:digit:]][[:digit:]]/!p' hundred.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `sed` Commands\n",
    "- There are many different commands, but the most common ones are \n",
    "    - d: Delete a line\n",
    "    - s/REGEX/SUBSTITUTION/: performs a substitution\n",
    "    - a: Appends after a line\n",
    "    - i: Inserts before a line\n",
    "    - p: Prints a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "read -sp \"Enter the secret word: \" secret\n",
      "\n",
      "echo\n",
      "echo \"Was I supposed to keep $secret a secret?\"\n"
     ]
    }
   ],
   "source": [
    "sed '/^\\s*#[^!]/d' read_ps_example.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#Must be -sp, -ps means \"s\" is the argument of -p\n",
      "read -sp \"Enter the secret word: \" secret\n"
     ]
    }
   ],
   "source": [
    "sed '5,$d' read_ps_example.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sed '/^\\s*#!/a #There is a shebang line in this file' read_ps_example.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#Must be -sp, -ps means \"s\" is the argument of -p\n",
      "read -sp \"Enter the secret word: \" secret\n",
      "\n",
      "#Not printing characters means that we need to \n",
      "#explicitly move to the next line\n",
      "echo\n",
      "echo \"Was I supposed to keep $secret a secret?\"\n",
      "#This is the end of the file\n"
     ]
    }
   ],
   "source": [
    "sed '$a #This is the end of the file' read_ps_example.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#Must be -sp, -ps means \"s\" is the argument of -p\n",
      "read -sp \"Enter the secret word: \" secret\n",
      "\n",
      "#Not printing characters means that we need to \n",
      "#explicitly move to the next line\n",
      "echo\n",
      "\n",
      "#Only One Line Left\n",
      "echo \"Was I supposed to keep $secret a secret?\"\n"
     ]
    }
   ],
   "source": [
    "sed '$i \\\\n#Only One Line Left' read_ps_example.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#Mist be -sp, -ps means \"s\" is the argument of -p\n",
      "read -sp \"Enter the secret word: \" secret\n",
      "\n",
      "#Not printing characters means that we need to \n",
      "#explicitly move to the next line\n",
      "echo\n",
      "echo \"Was I sipposed to keep $secret a secret?\"\n"
     ]
    }
   ],
   "source": [
    "sed 's/u/i/' read_ps_example.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#Mist be -sp, -ps means \"s\" is the argiment of -p\n",
      "read -sp \"Enter the secret word: \" secret\n",
      "\n",
      "#Not printing characters means that we need to \n",
      "#explicitly move to the next line\n",
      "echo\n",
      "echo \"Was I sipposed to keep $secret a secret?\"\n"
     ]
    }
   ],
   "source": [
    "sed 's/u/i/g' read_ps_example.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#Mist be -sp, -ps means \"s\" is the argiment of -p\n",
      "read -sp \"Enter the secret word: \" secret\n",
      "\n",
      "#Not printing characters means that we need to \n",
      "#explicitly move to the next line\n",
      "echo\n",
      "echo \"Was I supposed to keep $secret a secret?\"\n"
     ]
    }
   ],
   "source": [
    "sed '/^#[^!]/s/u/i/g' read_ps_example.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Functions\n",
    "- Functions in `bash` behave like functions in most other languages, with a few notable differences\n",
    "    - No keyword used to define the function\n",
    "    - No return type\n",
    "    - No parameter list in definition\n",
    "    - Called like a `bash` command rather than a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Function Syntax\n",
    "- To define a function:\n",
    "```bash\n",
    "FUNCTION_NAME()\n",
    "{\n",
    "    #CODE HERE\n",
    "}\n",
    "```\n",
    "- To call a function\n",
    "```bash\n",
    "FUNCTION_NAME ARG1 ARG2 ARG3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "hello(){\n",
    "    echo \"Hello World\";\n",
    "}\n",
    "\n",
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is Tuesday, September 26, 2017\n"
     ]
    }
   ],
   "source": [
    "today() {\n",
    "    date +\"%A, %B %-d, %Y\"\n",
    "}\n",
    "\n",
    "echo -n \"Today is \"\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: syntax error near unexpected token `('\n",
      "total 7.5M\n",
      "drwxrwxr-x 3 bryan bryan 4.0K Sep 12 12:11 433Fall17\n",
      "-rw-rw-r-- 1 bryan bryan 2.0M Sep  7 10:17 airline_tweets.tsv\n",
      "-rw-rw-r-- 1 bryan bryan   17 Sep 19 12:33 a_missing_file\n",
      "-rw-rw-r-- 1 bryan bryan  125 Sep 12 09:05 anchored.pl\n",
      "-rw-rw-r-- 1 bryan bryan    0 Sep 19 12:33 an_empty_file\n",
      "-rwxrwxr-x 1 bryan bryan  24K Sep 19 08:46 a.out\n",
      "-rw-rw-r-- 1 bryan bryan  132 Sep 12 09:13 bad_alt.pl\n",
      "-rw-rw-r-- 1 bryan bryan  127 Sep 12 09:14 capture.pl\n",
      "-rwxrwxr-x 1 bryan bryan  176 Sep 14 09:48 cla_examples.sh\n",
      "-rwxrwxr-x 1 bryan bryan  106 Sep 21 00:02 definitions.sh\n",
      "-rw-rw-r-- 1 bryan bryan   15 Sep 19 11:58 err\n",
      "-rw-rw-r-- 1 bryan bryan  137 Sep 12 08:46 fast.pl\n",
      "-rw-r--r-- 1 bryan bryan  37K Sep  4 21:07 fb_messenger.png\n",
      "-rw-rw-r-- 1 bryan bryan  41K Sep 19 12:20 fb_messenger.png.jpg\n",
      "-rw-r--r-- 1 bryan bryan  24K Sep  4 20:15 fb_verify.png\n",
      "-rw-rw-r-- 1 bryan bryan  24K Sep 19 12:20 fb_verify.png.jpg\n",
      "-rw-rw-r-- 1 bryan bryan  106 Sep 19 11:58 gcc_errors.txt\n",
      "-rw-rw-r-- 1 bryan bryan  132 Sep 12 09:12 good_alt.pl\n",
      "-rw-rw-r-- 1 bryan bryan  139 Sep 12 09:09 greedy.pl\n",
      "-rwxrwxr-x 1 bryan bryan   31 Sep 14 10:38 hello.sh\n",
      "-rw-rw-r-- 1 bryan bryan   19 Sep 14 10:37 hello_simple.sh\n",
      "-rw-rw-r-- 1 bryan bryan   12 Sep 19 11:57 hello.txt\n",
      "-rw-rw-r-- 1 bryan bryan  292 Sep 25 19:37 hundred.txt\n",
      "-rw-rw-r-- 1 bryan bryan 918K Sep 21 12:12 Lecture00.html\n",
      "-rw-rw-r-- 1 bryan bryan 990K Aug 31 15:15 Lecture00.ipynb\n",
      "-rw-rw-r-- 1 bryan bryan 366K Sep 21 12:12 Lecture01.html\n",
      "-rw-rw-r-- 1 bryan bryan  85K Sep  5 15:31 Lecture01.ipynb\n",
      "-rw-rw-r-- 1 bryan bryan 314K Sep 21 12:12 Lecture02.html\n",
      "-rw-rw-r-- 1 bryan bryan  61K Sep  7 12:22 Lecture02.ipynb\n",
      "-rw-rw-r-- 1 bryan bryan 287K Sep 21 12:12 Lecture03.html\n",
      "-rw-rw-r-- 1 bryan bryan  24K Sep 12 12:13 Lecture03.ipynb\n",
      "-rw-rw-r-- 1 bryan bryan 302K Sep 21 12:12 Lecture04.html\n",
      "-rw-rw-r-- 1 bryan bryan  38K Sep 14 14:27 Lecture04.ipynb\n",
      "-rw-rw-r-- 1 bryan bryan 313K Sep 21 12:12 Lecture05.html\n",
      "-rw-rw-r-- 1 bryan bryan  47K Sep 19 12:48 Lecture05.ipynb\n",
      "-rw-rw-r-- 1 bryan bryan 294K Sep 26 09:55 Lecture06.html\n",
      "-rw-rw-r-- 1 bryan bryan  34K Sep 21 14:25 Lecture06.ipynb\n",
      "-rw-rw-r-- 1 bryan bryan 277K Sep 26 09:57 Lecture07.html\n",
      "-rw-rw-r-- 1 bryan bryan 324K Sep 26 12:14 Lecture07.ipynb\n",
      "-rw-rw-r-- 1 bryan bryan 244K Sep 21 12:12 Lecture08.html\n",
      "-rw-rw-r-- 1 bryan bryan  565 Sep 15 17:38 Lecture08.ipynb\n",
      "-rw-rw-r-- 1 bryan bryan  129 Sep 12 09:15 noncapture.pl\n",
      "-rw-rw-r-- 1 bryan bryan  140 Sep 12 09:09 nongreedy.pl\n",
      "-rw-rw-r-- 1 bryan bryan    3 Sep 21 12:20 numbersaa\n",
      "-rw-rw-r-- 1 bryan bryan    3 Sep 21 12:20 numbers_aa\n",
      "-rw-rw-r-- 1 bryan bryan    3 Sep 21 12:21 numbers_aaaa\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:21 numbers_aaab\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:21 numbers_aaac\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:21 numbers_aaad\n",
      "-rw-rw-r-- 1 bryan bryan    1 Sep 21 12:21 numbers_aaae\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:20 numbersab\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:20 numbers_ab\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:20 numbersac\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:20 numbers_ac\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:20 numbersad\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:20 numbers_ad\n",
      "-rw-rw-r-- 1 bryan bryan    1 Sep 21 12:20 numbersae\n",
      "-rw-rw-r-- 1 bryan bryan    1 Sep 21 12:20 numbers_ae\n",
      "-rw-rw-r-- 1 bryan bryan   10 Sep 19 09:02 numbers.txt\n",
      "-rw-rw-r-- 1 bryan bryan   15 Sep 19 11:58 out\n",
      "-rwxrwxr-x 1 bryan bryan  191 Sep 19 08:51 out_and_err.py\n",
      "-rw-rw-r-- 1 bryan bryan  176 Sep 19 09:23 part1.tsv\n",
      "-rw-rw-r-- 1 bryan bryan  106 Sep 19 09:25 part2.csv\n",
      "-rw-rw-r-- 1 bryan bryan   45 Sep 19 12:20 pngs\n",
      "-rwxrwxr-x 1 bryan bryan   72 Sep 20 22:58 read_example.sh\n",
      "-rwxrwxr-x 1 bryan bryan   95 Sep 20 23:05 read_p_example.sh\n",
      "-rwxrwxr-x 1 bryan bryan  241 Sep 20 23:09 read_ps_example.sh\n",
      "-rwxrwxr-x 1 bryan bryan  192 Sep 20 23:18 read_t_example.sh\n",
      "-rw-rw-r-- 1 bryan bryan   68 Sep  4 22:57 re_example.pl\n",
      "drwxrwxr-x 3 bryan bryan 4.0K Sep 12 10:40 regex_starter_code\n",
      "-rw-r--r-- 1 bryan bryan 9.2K Sep  5 10:04 registers.png\n",
      "-rw-rw-r-- 1 bryan bryan  12K Sep 19 12:20 registers.png.jpg\n",
      "-rw-rw-r-- 1 bryan bryan  17K Sep  4 23:15 rolling_stone_500_greatest_2010.txt\n",
      "-rw-rw-r-- 1 bryan bryan 1.3K Sep 19 12:06 scipy.log\n",
      "-rwxrwxr-x 1 bryan bryan  128 Sep 19 09:02 simple.py\n",
      "-rw-rw-r-- 1 bryan bryan  127 Sep 12 08:46 slow.pl\n",
      "-rw-rw-r-- 1 bryan bryan   19 Sep 14 10:09 to_sort1.txt\n",
      "-rw-rw-r-- 1 bryan bryan   23 Sep 14 10:09 to_sort2.txt\n",
      "-rw-rw-r-- 1 bryan bryan   15 Sep 14 10:10 to_sort3.txt\n",
      "-rw-rw-r-- 1 bryan bryan   90 Sep 14 10:15 to_sort4.txt\n",
      "-rw-rw-r-- 1 bryan bryan  124 Sep 12 09:05 unanchored.pl\n",
      "-rw-rw-r-- 1 bryan bryan 244K Sep 21 12:12 Untitled.html\n",
      "-rw-rw-r-- 1 bryan bryan  313 Sep  4 16:14 Untitled.ipynb\n",
      "-rw-rw-r-- 1 bryan bryan    3 Sep 21 12:21 xaa\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:21 xab\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:21 xac\n",
      "-rw-rw-r-- 1 bryan bryan    2 Sep 21 12:21 xad\n",
      "-rw-rw-r-- 1 bryan bryan    1 Sep 21 12:21 xae\n",
      "bash: syntax error near unexpected token `}'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "2",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "ls(){\n",
    " command ls -lh\n",
    "}\n",
    "#ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accessing Function Arguments\n",
    "- Inside of a function definition, the special variables `$1`, `$2`, etc refer to the arguments passed to the function\n",
    "    - This means there is no way to access the arguments passed to the script as a whole using the same variables, they must have been saved prior to defining the function\n",
    "    - Just like with a script, the special variable `$#` holds the number of arguments passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "Hello Class!\n",
      "Hello !\n",
      "Hello The!\n"
     ]
    }
   ],
   "source": [
    "hello(){\n",
    "    echo \"Hello $1!\"\n",
    "}\n",
    "hello World\n",
    "hello Class\n",
    "hello\n",
    "hello The Whole World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Many Args Are Printed?\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print_all_args(){\n",
    "    echo $@\n",
    "    echo $#\n",
    "}\n",
    "print_all_args How Many Args Are Printed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "10\n",
      "11\n",
      "/bin/ls: cannot access *.: No such file or directory\n",
      "0\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "count_files_of_type() {\n",
    "    echo $(/bin/ls -1 *.$1 | wc -l)\n",
    "}\n",
    "count_files_of_type sh\n",
    "count_files_of_type ipynb\n",
    "count_files_of_type pl\n",
    "count_files_of_type\n",
    "\n",
    "do_math()\n",
    "{\n",
    "    a=$1\n",
    "    echo $((a + 1))\n",
    "}\n",
    "do_math 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Returning Values\n",
    "- `bash` does support the `return` keyword, but this can only return integer values\n",
    "    - It is an exit code, similar to what a script would produce\n",
    "    - To access it, look in the `$?` variable\n",
    "- The other option is to capture the output using command substitution\n",
    "```bash\n",
    "$(FUNCTION_NAME ARG1 ARG2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum is 15\n"
     ]
    }
   ],
   "source": [
    "sum(){\n",
    "    sum=0\n",
    "    for num in $@; do\n",
    "        sum=$((sum + num))\n",
    "    done\n",
    "    return $sum\n",
    "}\n",
    "\n",
    "sum 1 2 3 4 5\n",
    "echo \"The sum is $?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "The sum is 15\n"
     ]
    }
   ],
   "source": [
    "sum(){\n",
    "    sum=0\n",
    "    for num in $@; do\n",
    "        sum=$((sum + num))\n",
    "    done\n",
    "    echo $sum\n",
    "}\n",
    "\n",
    "total=$(sum 1 2 3 4 5)\n",
    "echo $((total + 1))\n",
    "echo \"The sum is $total\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 .sh files\n"
     ]
    }
   ],
   "source": [
    "count_files_of_type2() {\n",
    "    /bin/ls -1 *.$1 | wc -l\n",
    "}\n",
    "echo \"There are $(count_files_of_type2 sh) .sh files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scope in `bash` Functions\n",
    "- `bash` doesn't have a default concept of variable scope except for the special variables `$1`, `$2`, etc.\n",
    "    - Everything is global by default\n",
    "- The keyword `local` before the assignment operator will ensure that the scope of the variable is only the function itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x is 1\n",
      "The value of x is 2\n"
     ]
    }
   ],
   "source": [
    "x=1\n",
    "mess_up_the_scope(){\n",
    "    x=2\n",
    "}\n",
    "echo \"The value of x is $x\"\n",
    "mess_up_the_scope\n",
    "echo \"The value of x is $x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x is 1\n",
      "The value of x is 1\n"
     ]
    }
   ],
   "source": [
    "x=1\n",
    "cant_mess_up_the_scope(){\n",
    "    local x=2\n",
    "}\n",
    "echo \"The value of x is $x\"\n",
    "cant_mess_up_the_scope\n",
    "echo \"The value of x is $x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
